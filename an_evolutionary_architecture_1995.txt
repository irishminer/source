JOHN FRAZER - AN EVOLUTIONARY ARCHITECTURE (1995)
CONTENTS
3 Preface John Frazer
6 Foreword Gordon Pask
9 Introduction A Natural Model for Architecture
23 Section 1 New Tools
65 Section 2 The Nature of the Evolutionary Model
65 Part 1: The General Proposition
83 Part 11: The Specific Model
106 Postscript Tim Jachna
118 Glossary of Termininology
121 Selected Bibliography
126 Biographical Note
127 Acknowledgements

GLOSSARY
Nanotechnology: Technology based on the manipulation of individual atoms and molecules to build complex atomic structures. 

Natural selection: The evolutionary process described by Darwin which occurs by favouring the genetic material best adapted to increase chances of survival in an environment. Term applied in artificial genetics to automatic selection criteria for an environment (such as performance optimization). 

Neo-Darwinism: Modern evolutionary theory combining Darwinian natural selection with Mendelian genetic theory. 
Negentropy: Negative entropy (see above) or a measure of order. 
Neural network: An artificial network in which the nodes simulate the action of neurones in the brain and learn to transmit a signal in response to a stimulus by adjusting the threshold level of incoming signals. 
Nucleotide: An organic compound, four of which- adenine, cytosine, guanine and thymine (ACGTl- form the DNA code. 
Ontogeny: The process of individual development; the history of that development. 
Parallel processor: A computer processor consisting of an array of processors capable of communicating arid processing several parts of a problem simultaneously. 
Phenotype: The expressed attributes of an organism as a product of the genes and the environment. 
Phenotypic expression: A gene has a phenotypic expression in, say, the colour of a flower. Term extended by Richard Dawkins (1983) to include functional consequences of genotypes outside the body in which the genes are located, e.g. snail shells and, by extension, beaver dams. 
Phylogeny: The development of a species; the history of that development. 
Plastic modelling: A flexible approach to solid modelling which permits easy change and manipulation of the forms being modelled. 
Preformationalism: The theory that the adult body can be mapped from the single cell. 
Relational operator: A technique developed to establish logical connections between objects in a database. 
Replicator: Any system which can make a copy of itself. 
Reptile: A flexible modular enclosure system developed by Frazer (1966). 
Seed: A term used in cellular automata theory for the starting configuration of a system. Used to denote the compact coded form of a generic system. 
Self-organising system: A system whose structure and behaviour determine its action. 
Self-replicating automata: A special class of cellular automata whose pattern of development repeats itself. 
Shape processing: The operation of a computer graphics system which is analogous to word-processing. 
Shape processing language (SPL): A computer graphics language developed by Coates et al (1981) to describe complex graphics operations in an extensible, user-defined manner. 
Singularity: A point ln a mathematical function where the function ceases to be analytic. 
Soft modelling: Another term for 'plastic modelling'. 
State: In cellular automata theory the state of a cell is a description of its properties, such as alive or dead, black or white, 1 or 0. A cell can have any number of states, and models described in this book often have 256 or 4096 possible states. 
Symmetry-breaking: The process by which a system loses its symmetry. In particular the process by which a single spherically symmetrical cell progressively divides to become a complex organism. 
System: A set of things considered as a connected whole. 
Transition rules: In cellular automata theory transition rules determine the state of a cell according to its state and the state of its neighbours at the previous instant. 
Transputer: A single-chip processor with in-built memory and four channels of communication to adjacent processors for constructing parallel processor computers. 
Universal Constructor: A self-reproducing computer devised by Von Neumann. Used by Architectural Association Diploma Unit 11 (1989/90) as the name for their first interactive modelling system. 
Universal Interactor: Name given by Architectural Association Diploma Unit 11 (1991/92) for the rack through which their sensory input and output devices (antennae) communicated. 
Universal State Space Model: Name given by Architectural Association Diploma Unit 11 (1993/94) for the isospatial computer model for evolving a genetic architecture. 
Virus: A chain of genetic material which is dependent on a parasitic relation with another cell for action and reproduction. The term is also applied to self-reproducing code in a computer. 
PREFACE 
This book is the first attempt to publish an overall description of the objectives and achievements of work which began more than thirty years ago, when I arrived at the Architectural Association as a student, and continues still in the AA:s Diploma Unit 11. During this period I have also made use of the computer resources and skills of the University of Cambridge, the inventive initiatives of Autographics Software Ltd., and the research resources and environment of the University of Ulster. I have thanked these institutions and the dozens of research assistants who have been associated with this project in the Acknowledgements: I would like here to thank those particularly responsible for the existence of this book. I am grateful to Alan Balfour, Chairman of the Architectural Association, who supported the work from his arrival at the AA in 1991, and to the late Alvin Boyarsky, the Chairman before him, who had the generosity and insight to initiate and encourage the Diploma programme which has centered on this work. The support of the Arts Council of England and other sponsors is also much appreciated. This publication would not have gone to press at all without the ruthless editing of Pamela Johnston, who showed heroic perseverance and courage in forcing me to cut and substantially rewrite the text. Dennis Crompton put the pages together. Thank you Pamela and Dennis. And, finally, thanks to all those students, teachers, colleagues and friends who over the years have discussed and contributed to these ideas and encouraged me to continue. John Frazer

FOREWORD - Gordon Pask
This is a brilliant, coherent and rational book. lt records not only the present state but also the progress of research extending over a period of thirty years. In this endeavor, the author has had the fortune to be assisted by Julia Frazer and many generations of outstandingly innovative and devoted students. The work is essential reading for architects, artists, cybernetists, scientists and philosophers (much as I dislike the arbitrary classification scheme). As the statement of a thesis, it is strongly supported by practical methodology, and is placed in context with tangible entities and systems such as astronomy, close packing spheres, fractals and chaos theory. The work's originality, however, lends it another dimension. The fundamental thesis is that of architecture as a living, evolving thing. In a way this is evident. Our culture's striving towards civilization is manifested in the places, houses and cities that it creates. As well as providing a protective carapace, these structures also carry symbolic value. and can be seen as being continuous with and emerging from the life of those who inhabit the built environment. This is not to suggest that Frazer is simply recording the often frenetic practice of copying the works of nature in architectural forms. The author is careful, in fact, to use models which are tangible and rational but also very clearly alive and evolutionary. These models are the subject of many projects which relate in varying ways to the substance of architecture and to life itself. In many cases a given process is emulated by machines of one kind or another. In general these are cellular automata, evolving according to a genetic algorithm. However, contrary to what the extreme A.l. community might suppose the machine is not likened to life in a sense that a 'virus' program is 'alive' within the 'software' of a single machine, or even a global network of machines. In fact no device capable of adequately simulating the requirements of the author's research was available prior to its commencement, and great advantage has been gained from the opportunity to construct one specific to its needs. Whilst computer-aided architectural design is useful if repetition or standard transformations are required, it is inadequate to the task of producing new forms. lt would be redundant to comment on the contents of the book: these are ably summarized by the author in the Introduction . However it is appropriate to stress an important cybernetic feature of the work; namely, that unity is not uniformity, but is coherence and diversity admixed in collusion. In the context of a study unit at the Architectural Association, for example, the very liberal project brief allows individuals or coalitions of individuals to pursue apparently divergent interests of their own: coherence is provided by commonly accessible resources such as computerized modelling devices and their alternatives. Probably the simplest example of a device being a general resource is the Universal Constructor developed by AA Diploma Unit 11 in 1990. More elaborate mechanisms of maintaining unity without uniformity have also been used as part of the research. In such cases, the result is the encouragement of originality by even more subtle means, albeit to achieve similar ends. The role of the architect here, I think, is not so much to design a building or city as to catalyse them; to act that they may evolve. That is the secret of the great architect. To a large extent this principle has been uncovered or at least assisted by the work reported in this book. The principle exhibited has particular contemporary relevance as society and the environment, a fortiori the built environment, become ever more dependent upon meaningful information transfer. If you accept that this information environment is becoming of increasing significance, then you must admire this work.

Gordon Pask's contribution to architecture is described in J.H. Frazer, 'The Architectural Relevance of Cybernetics', Systems Research, Vol. 10. No. 3, 1993, pp. 43-4.

Gordon Pask with the Universal Constructor, Diploma Unit 11 installation, AA end-of-year exhibition, June 1990.


INTRODUCTION - A NATURAL MODEL FOR ARCHITECTURE

"a typical seed with two cotyledons. The cotyledons are specialised rudimentary leaves containing a supply of nourishment sufficient for the initial stage in the development of the germ. The germ is the real thing; the seat of identity. Within its delicate mechanism lies the will to power: the function of which is to seek and eventually to find its full expression in form. The seat of power and the will to Jive constitute the simple working idea upon which all that follows is based …" Louis H. Sullivan. A System of Architectural Ornament. 1924 

An Evolutionary Architecture investigates fundamental form-generating processes in architecture, paralleling a wider scientific search for a theory of morphogenesis in the natural world. lt proposes the model of nature as the generating force for architectural form. The profligate prototyping and awesome creative power of natural evolution are emulated by creating virtual architectural models which respond to changing environments. Successful developments are encouraged and evolved. Architecture is considered as a form of artificial life, subject, like the natural world, to principles of morphogenesis, genetic coding, replication and selection. The aim of an evolutionary architecture is to achieve in the built environment the symbiotic behaviour and metabolic balance that are characteristic of the natural environment. Space, structure and form are the traditional outward expressions of an architectural concept which has developed in the mind of the architect. 

This idea is taken further in our work. Architectural concepts are expressed as generative rules so that their evolution may be accelerated and tested. The rules are described in a genetic language which produces a code-script of instructions for form-generation. Computer models are used to simulate the development of prototypical forms which are then evaluated on the basis of their performance in a simulated environment. Very large numbers of evolutionary steps can be generated in a short space of time, and the emergent forms are often unexpected.

Underside of Universal Constructor during assembly: AA Diploma Unit 11, 1990

These techniques have previously been limited to easily quantified engineering problems. Only now is it becoming feasible to apply them to the complex problems associated with our built environment. To achieve this, we have to consider how structural form can be coded for a technique known as a 'genetic algorithm', how ill-defined and conflicting criteria can be described, how these criteria operate for selection, and how the morphological and metabolic processes are adapted for the interaction of built form and its environment. Once these issues are resolved, the computer can be used not as an aid to design in the usual sense, but as an evolutionary accelerator and a generative force.

THE PROPOSITION
This book introduces the concept of an evolutionary architecture, indicates the nature of the biological and scientific analogies, and relates the idea to the wider context of present scientific discourse. In the process, it relies heavily on natural science and the newer sciences of cybernetics, complexity and chaos. This Introduction advances the general proposition and describes the background to our work. Section 1 describes the creation and implementation of tools to assist in developing the model. Section 2 establishes the theoretical proposition and describes one possible way of achieving these principles. Overall, this book will describe the emerging field of architectural genetics. lt will explore at least one possible future based on artificial design life, suggesting a new form of designed artefact interacting and evolving in harmony with natural forces, including those of society.
lt will explain a new computer-based technique for design which models inner logic rather than external form, and it will afford a brief glimpse of a future as yet evolving only in the imagination of a computer.
THE NATURE OF THE ANALOGY
Architecture has frequently drawn inspiration from nature - from its forms and structures, and, most recently, from the inner logic of its morphological processes.lt is therefore necessary to be clear as to where architecture is literally considered as part of nature, where there are analogies or metaphors, and where nature is a source of inspiration. We can say that architecture is literally part of nature in the sense that the man-made environment is now a major part of the global eco-system, and man and nature share the same resources for building. 

As early as 1969 Charles Jencks was predicting that biology would become the major metaphor for the 1990s and the source of the most significant architectural movement this century - the Biomorphic School. See C. Jencks. Architecture 2000: Predictions and Methods (Studio Vista, 1971 ). 

A critical overview of biological analogies is given in P. Steadman, The Evolution of Designs: Biological Analogy in Architecture and the Applied Ms (Cambridge University Press 1979). 

D. Barthelemy et at, 'Architectural Concepts for Tropical Trees' in Holm-Nielson et at (eds.l. Tropical Forests (Academic Press 1989). 

W. Paley, Natural Theology, or Evidences of the Existence and Attributes of the Deity Collected from the Appearance of Nature (Oxford 1802). 

A. Dawkins, The Blind Watchmaker (Longman 1986) and The Selfish Gene (Oxford University Press 1976). 

In turn, our description of an architectural concept in coded form is analogous to the genetic code-script of nature. Analogies, particularly biological, bedevil architectural writing. Sullivan, Wright and Le Corbusier all employed biological analogies, and the concept of the organic is central to the twentieth century. In our case, the primary inspiration comes from the fundamental formative processes and information systems of nature.

THE PROBLEM OF THE BLUEPRINT
Architecture's use of biological analogies has a counterpart in the use of architectural analogies in science. There is an abundance of books with titles like The Cosmic Blueprint (by Paul Davies) or Blueprint for a Cell (by the Nobel Laureate Christian de Duve). These, however, are quick to point out that an architect's blueprint is a specific one-off set of plans whereas the 'blueprint' in nature is a set of instructions which are dependent on a particular environmental context for their interpretation. Our present search to go beyond the 'blueprint' in architecture and to formulate a coded set of responsive instructions (what we call a 'genetic language of architecture') .may yield a more appropriate metaphor. lt may provide at least a model of how such a form-generating process might work, even if it is not a direct parallel of the way in which nature itself generates form. Examples of the architectural analogy turn up in every field. For botanists, too, the concept of the architectural model has provided a powerful tool for studying plant form and structure: 'The architectural model is an inherent growth strategy which defines both the manner in which the plant elaborates its form and the resulting architecture.' lt is ironic that architectural theory actually lacks the methodological incisiveness which these reverse analogies imply. 
INTENTIONALITY 
Although evolution operates without preknowledge of what is to come that is, without design- the seemingly purposeful construction of living things makes it tempting to apply the idea of 'design' to natural forms. On this basis, Bishop William Paley argued that since the existence of something as complex as a watch implied the existence of a watchmaker, the infinitely greater complexity of nature had necessarily to imply the existence of a creator. Richard Dawkins countered with the 'Blind Watchmaker' argument, contending that the blind forces of Darwinian natural selection alone were sufficient explanation for the complexity of natural forms. 

Universallnteractor, AA Diploma Unit 11 end of year exhibition, July 1992 

L. March (ed .), The Architecture of Form (Cambridge University Press 1976).

But Dawkins in turn has resorted to anthropomorphism, attributing 'selfish' tendencies to genes. 
The tendency to invest nature with vitalist forces is common in both science and poetry. 
In the framework of our analogy, we will sometimes apply the word 'design' to nature for convenience of expression: we will also apply it to our new model with purposeful intent. To us the connotations of the term 'design' are very different from the norm: when we 'design', we are clear in our intentions, but perhaps 'blind' to the eventual outcome of the process that we are creating. This 'blindness' can cause concern to those with traditional design values who relish total control. 
lt dm alarm those who feel that what we are proposing might get out of control like a computer virus. However we remain convinced that the harnessing of some of the qualities of the natural design process could bring about a real improvement in the built environment. 
SOURCES OF INSPIRATION
A clear distinction is intended between sources of inspiration and sources of explanation. When natural science is used for explanation or illustration, it is essential that the science is correct and that the analogy is valid. But when it is used for inspiration and as a take-off point for thought experiments, it matters less, and misunderstood or even heretical ideas can provide much imaginative stimulus. 
lt is important to differentiate between the nature of different kinds of theories. Lionel March has written: 'Logic has interests in abstract forms. Science investigates extant forms. Design initiates novel forms. A scientific hypothesis is not the same thing as a design hypothesis.
A logical proposition is not to be mistaken for a design proposal.' In this context, ours is not a theory of explanation, but a theory of generation. 
THE INSPIRATION OF NATURE
The perfection and variety of natural forms is the result of the relentless experimentation of evolution. By means of profligate prototyping and the ruthless rejection of flawed experiments, nature has evolved a rich biodiversity of interdependent species of plants and animals that are in metabolic balance with their environment. Whilst vernacular architecture might occasionally share some of these characteristics, the vast majority of buildings in our contemporary environment most certainly do not. 
Our analogy of evolutionary architecture should not be taken just to imply a form of development through natural selection. 

G. Nicolis and I. Prigogine, Exploring Complexity (Freeman 1993).

J. Holland, Adaptation in Natural and Artificial Systems (University of Michigan Press 1975). 

Other aspects of evolution, such as the tendency to self-organization, are equally or even more significant. Natural processes such as metabolism and the operation of the laws of thermodynamics are central to our enquiry, as are the general principles of morphology, morphogenetics and symmetry-breaking, which are introduced in the next section. Charles Darwin established a new world which broke away from the Newtonian paradigm of stability - a world in a continuous process of evolution and change. Modern physics now describes a world of instability.

llya Prigogine has discovered new properties of matter in conditions that are far from equilibrium, revealing the prevalence of instability which is expressed by the phenomenon of small changes in initial conditions leading to large amplifications of the effects of the changes. 
NATURAL AND ARTIFICIAL MODELS
The modelling of these complex natural processes requires computers, and it is no coincidence that the development of computing has been significantly shaped by the building of computer models for simulating natural processes. Alan Turing, who played a key role in the development of the concept of the computer (the Turing Machine), was interested in morphology and the simulation of morphological processes by computer-based mathematical models. The Church-Turing hypothesis stated that the Turing Machine could duplicate not only the functions of mathematical machines but also the functions of nature. Von Neumann, the other key figure in the development of computing, set out explicitly to create a theory which would encompass both natural and artificial biologies, starting from the premise that the basis of life was information. A significant example of this dual approach in terms of our genetic model is John Holland's Adaptation in Natural and Artificial Systems. Holland starts by looking for commonality between different problems of optimization involving complexity and uncertainty. His examples of natural and artificial systems range from 'How does evolution produce increasingly fit organisms in highly unstable environments_?' to 'What kind of economic plan can upgrade an economy's performance in spite of the fact that relevant economic data and utility measures must be obtained as the economy develops?' Although Holland suggests that such problems have no collective name, they seem to share a common concern with questions of adaptation.

W.J. Mitchell, The Logic of ArchitectureDesign, Computation, and Cognition (MIT Press 1990). 

They occur at critical points in fields as diverse as evolution, ecology, psychology, economic planning, control, artificial intelligence, computational mathematics, sampling and inference. To this list we must now add architecture. 

ARTIFICIAL LIFE
In nature it is only the genetically coded information of form which evolves, but selection is based on the expression of this coded information in the outward form of an organism. The codes are manufacturing instructions, but their precise expression is environmentally dependent. Our architectural model, considered as a form of artificial life, also contains coded manufacturing instructions which are environmentally dependent, but as in the real world model it is only the codescript which evolves. 

GENERATIVE SYSTEMS
An essential part of this evolutionary model is some form of generative technique. Again this is an area charged with problems and controversy. The history of generative systems is summarised by William Mitchell, who maps out a line from Aristotle to Lull through the parodies of Swift and Barges. After tracing back the use of generative systems in architectural design to Leonardo's study of centrally planned churches and Durand's Precis des Lecons d'Architecture, he outlines the concept of 'shape grammars', or elemental combinatorial systems. From our point of view, there are several problems with this approach. All of these generative systems are essentially combinatorial or configurational, a problem which seems to stem from Aristotle's description of nature in terms of a kit of parts that can be combined to furnish as many varieties of animals as there are combinations of parts. Fortunately, nature is not actually constrained by the limitations implied by Aristotle. Mitchell regards architectural design as a special kind of problem-solving process. 
+ 
This approach has limitations which he recognizes. in principle. First, it assumes we can construct some kind of a representation which can achieve different states that can be searched through for permutations corresponding to some specified criterion (the criterion of the problem). 
+ 
Unfortunately for this goal-directed approach, it is notoriously difficult to describe architecture in these terms, except in the very limited sense of an architectural brief to which there are endless potential solutions. The other problem is that any serious system will generate an almost unmanageable quantity of permutations. 

These difficulties stem largely from the views of Herben Simon as expressed in The Sciences of the Artificial MIT Press (1969), an exploration of the problem of complexity with particular reference to artificial (man-made) systems. While Simon's views on the inevitability of a hierarchical strategy in nature have been influential in the formulation of our own theories, his scientific method does not recognize the need for a generating concept when approaching design, and as a consequence design has come to be misunderstood as a problem-solving activity.
+ 

M. Pawley, Theory and Design in the Second Machine Age (Blackwell 1990). 

C.T. Mitchell. Redefining Designing (Van Nostrand 1993). 

J. Thackara. Design after Modernism (Thames and Hudson 1988). 

PROBLEMS WITH INDUSTRIALIZATION
A problem arose in the sixties when architecture started toying with new design processes, taking up a particular form of component-based rationalisation and methodology which embraced a generic approach, modular coordination and a perception of construction as a kit of parts. For despite rhetoric to the contrary, the architectural profession - and the construction industry as a whole - have failed to learn from developments in the aircraft, automotive and shipbuilding industries. Construction remains labour-intensive: it has never made the transition to a capital-intensive industry with adequate research and development capabilities. lt has been left largely to individual architects to take the risk of performing experimental and innovative prototyping in an uncoordinated and romantic or heroic manner. The ensuing (inevitable) failures have been catastrophic for both society and the architectural profession. 

The nostalgic inclinations of some to return to a previous era are an equally inadequate response, at every level, to the current predicament. The archetypes of the past do not reflect the changing demands of society, the realities of the construction industry, or the pressing need for environmentally responsible buildings. 
POST-INDUSTRIALIZATION 
Architecture is not the only field to be concerned with these problems. In industrial design the all-embracing concept of mass production for a homogeneous international market has given way to a search for a new flexibility in design and manufacture. The distinguishing characteristic of this approach is that it focuses on the dynamic processes of user experience rather than on physical form.
+ 

Or, in John Thackara's words, design is now 'beyond the object'. Industrial production used to be associated with high tooling costs and very large production runs. This is now changing because the computer has paved the way for what I have called 'the electronic craftsman'. The direct relationship between the designer at the computer console and the computer-controlled means of production potentially means not just a dramatic reduction in the production costs of the tools for mass production, and thus shorter economic runs, but a one to-one control of production and assembly equipment. This is effectively a return to one-off craft technology, but with all the capability of the precision machine tool. 
+ sanat ve zanaata çağdaş araçlarla dönüş

C. Jencks, The Language of Post-Modern Architecture (Academy Editions 1997). 

In 1972 Alex Pike and I made a presentation to the Royal Institute of British Architects Annual Conference, 'Designing for Survival', which emphasised the need for a responsible approach to energy and materials. Although there was some genuine interest in resource problems it was clear that it was the survival of the profession, not of the planet, which preoccupied most delegates. It is ironic that if architects had seized the initiative at this time and formulated a comprehensive energy policy, they might have ensured a future role for the profession. Instead the chance was wasted with a 'Long life. loose fit, low energy' campaign motivated more by politics than by a serious attempt to address the issue. See J.H. Frazer and A. Pike. 'Simple Societies and Complex Technologies' in Designing for SuNival- R/BA Annual Conference (Lancaster 19721 and RIBA Journal, September 1972, pp. 376-7. 

As Charles Jencks describes it: 'The new technologies stemming from the computer have made possible a new facility for production. This emergent type is much more geared to change and individuality than the relatively stereotyped productive processes of the First Industrial Revolution.And mass production, mass repetition, was of course one of the unshakable foundations of Modern Architecture. This has cracked apart, if not crumbled. For computer modelling, automated production, and the sophisticated techniques of market research and prediction now allow us to mass produce a variety of styles and almost personalised products.
The results are closer to nineteenth-century handicraft than the regimented superblocks of 1965.

THE ENVIRONMENTAL CASE
Natural ecosystems have complex biological structures: they recycle their materials, permit change and adaptation, and make efficient use of ambient energy. By contrast, most man-made and built environments have incomplete and simple structures: they do not recycle their materials, are not adaptable, and they waste energy. An ecological approach to architecture does not necessarily imply replicating natural ecosystems, but the general principles of interaction with the environment are directly applicable.
The construction industry is a significant consumer of raw materials and of the energy required to process, transport and assemble them. Buildings in use are even more significant consumers of energy for heating and cooling. An ecological approach would drastically reduce construction energy and materials costs and allow most buildings in use to export energy rather than consume it. While there is a growing awareness of the importance of environmentally and ecologically sound design amongst both architects and enlightened clients, there is still no comprehensive design theory and few built examples of an ecological architecture. The solution to our environmental problems may lie in relating architecture to the new holistic understanding of the structure of nature. 

Breeding illusions: Juanita Cheung, 1993. This program creates two-dimensional forms that give ambiguous and sometimes conflicting readings when interpreted as three-dimensional forms. Forms develop characteristics which exist in natural systems such as regeneration. cell division and replication (mitosis and meiosis) and memory transfer between generations. 

W.M . Brodey, 'The Design of lntelligelit Environments: Soft Architecture. Landscape, Autumn 1967. pp. 8-12 . 

N. Negroponte. The Architecture Machine (MIT Press 1970). 

RESPONSIVE ENVIRONMENTS ... 
Another important issue for our model of an evolutionary architecture is that it should be responsive to evolving in not just a virtual but a real environment. In his article, 'The Design of Intelligent Environments', Warren Brodey proposed an evolutionary, self-organising, complex, predictive, purposeful, active environment. He asked: can we teach our environments first complex, then self-organising intelligence which we can ultimately refine into being evolutionary? These issues preoccupy us too . 

... AND SOFT ARCHITECTURE
Brodey went on to describe in enthusiastic terms some of the hypothetical implications of intelligent environments, and to introduce the concept of 'soft architecture'. The idea of a soft, responsive architecture also preoccupied Nicholas Negroponte, author of The Architecture Machine. He suggested that the design process, considered as evolutionary, could be presented to a machine, also considered as evolutionary, to give a mutual training resilience and growth. Negroponte placed high expectations first on computer hardware, then on software through artificial intelligence. Neither delivered any answers. 

THE ROLE OF THE COMPUTER
Christopher Alexander dismissed the use of computers as a design aid: "A digital computer is, essentially, the same as a huge army of clerks, equipped with rule books, pencil and paper, all stupid and entirely without initiative, but able to follow exactly millions of precisely defined operations... 
In asking how the computer might be applied to architectural design, we must, therefore, ask ourselves what problems we know of in design that could be solved by such an army of clerks ... At the moment, there are very few such problems." 

C. Alexander, 'The Question of Computers in Design', Landscape, Autumn 1967, pp. 8-12. 

See J.H. Frazer, 'The Use of Computer-Aided Design to Extend Creativity' in H. Freeman and 8. Allison (eds.), National Conference Art and Design in Education, electronic proceedings (NSEAD Brighton 1993). 

Our evolutionary approach is exactly the sort of problem that could be given to an army of clerks - the difficulty lies in handing over the rule book. Much of this text concerns the nature of these rules and the possibilities of developing them in such a way that they do not prescribe the result or the process by which they evolve. 

THE ELECTRONIC MUSE
I see computers not as an army of tedious clerks who will (not) thwart all creativity with their demands for precise information, but as slaves of infinite power and patience. However computers are not without their dangers. If used unimaginatively, they have a tendency to: dull critical faculties, induce a false sense of having optimized a design which may be fundamentally ill conceived, produce an atmosphere where any utterance from the computer is regarded as having divine significance, distort the design process to fit the limitations of the most easily available program, distort criticism to the end-product rather than to an examination of process, and concentrate criticism and feedback on aspects of a problem which can be easily quantified.
'Imaginative use' in our case means using the computer -like the genii in the bottle- to compress evolutionary space and time so that complexity and emergent architectural form are able to develop. The computers of our imagination are also a source of inspiration- an electronic muse. 

I. Aleksander and H. Morton, An Introduction to Neural Computing (Chapman and Hall 1990). 

This discussion is expanded in J.H Frazer and J.M. Frazer, 'Design Thinking: Creativity in Three Dimensions' in Creative Thinking: A Multifaceted Approach. conference proceedings (Malta University Press 1994). and J.H. Frazer. 'The Architectural Relevance of Cybernetics'. Systems Research, Vol. 10, No. 3, pp. 43-4. 

S.A. Kauffman, The Origins of Order: Self Organization and Selection in Evolution (Oxford University Press 1993). 

J. Miller, Living Systems (McGraw-Hill 1978). 

THE ROLE OF HUMAN CREATIVITY 
One of the world's leading exponents of neural networks, lgor Aleksander, is also very conscious of the unique capabilities of the human brain. He reminds us that it is extraordinarily good at making guesses based on experience, at retrieving knowledge from memory without the need for exhaustive searches, at perceiving analogies and forming associations between seemingly unrelated items.
These aspects of intuition, perception and imagination are the traditional creative engines for architectural ideas. While the model of architectural creativity proposed by this text departs in many ways from the traditional model, it still relies on human skills for the essential first step of forming the concept.
The prototyping, modeling, testing, evaluation and evolution all use the formidable power of the computer, but the initial spark comes from human creativity. Problems of Complexity
The 'sheer imponderable complexity of organisms' overwhelms us now as surely as it did Darwin in his time. The developmental processes of nature inevitably lead to complexity, but they work with simple building blocks and an economy of means to achieve complexity in a hierarchical manner.
The coding of all natural forms in DNA is achieved with just four nucleotides, which in turn use just twenty triplets to specify the amino acids that manufacture protein. The hierarchical structure of living systems is analyzed by James Miller on seven levels: cell, organ, organism, group, organization, society and supranational system. These are broken down into nineteen critical subsystems which appear at all levels. This hierarchical and self-similar description applies also to organisms and social systems, which are seen as part of the same continuum. 

Examples of the evolutionary view can be found in chemistry: S.F. Mason, Chemical Evolution: Origin of the Elements, Molecules. and Living Systems (Clarendon Press 1991 ); in physical constants: A. Sheldrake, The Presence of the Past ICollins 1988): in information: S. Goonatilake, The Evolution of Information: Lineages in Gene, Culture and Artefact (Pint er Press 1991 ): and in culture: R. Dawkins, The Selfish Gene (Oxford University Press 1976). 

In the next section I shall examine poly automata in order to show that even very simple local rules can generate emergent properties and behavior in a way apparently unpredicated by the rules. Collections of small actions ripple upwards, combining with other small actions, until a recognizable pattern of global behavior emerges. When a certain critical mass of complexity is reached, objects can self-organize and self reproduce in an open-ended fashion, not only creating their equals but also parenting more complicated objects than themselves. Von Neumann recognized that life depends upon reaching this critical level of complexity. Life indeed exists on the edge of chaos, and this is the point of departure for our new model of architecture.

THE NEW MODEL OF ARCHITECTURE 
There is so far no general developed science of morphology, although the generation of form is fundamental to the creation of all natural and all designed artefacts. Science is still searching for a theory of explanation, architecture for a theory of generation- and it is just possible that the latter will be advanced before the former. In other words, form generating models developed for architectural purposes (or based on unorthodox or incorrect scientific views) may be valuable if they model a phenomenon that scientists are seeking to explain. 
In a modest way some contribution has already been made. Many papers relating to this work (particularly on computing and computer graphics) have been enthusiastically received at international conferences and cited in further publications. Many of the resulting graphics techniques are now an integral part of the computer programs used in architectural offices. If taken much further, this could contribute to the understanding of more fundamental form-generating processes, thus repaying some of the debt that architecture owes the scientific field. 
Perhaps before the turn of the century there will be a new branch of science concerned with creative morphology and intentionality. 

THE URGE FOR A UNIFIED THEORY
In the meantime the approach of the end of the century is signaled by a frantic scramble in all fields to formulate a holistic view of the universe, the great unification theory, or GUT. In the natural sciences this takes the form of two juxtaposed tendencies. One is to embrace everything under the umbrella of evolution (or at least evolution in the form of neoDarwinism). 

On complexity, chaos and catastrophe see (respectively) G. Nicolis and I. Prigogine, Exploring Complexity lW. H. Freeman 19891; I. Prigogine and I. Stengers, Order Out of Chaos: Man 's New Dialogue with Nature (Bantam 19841; and R. Thom, Struct raJ Stability and Morphogenesis (Benja_m n 1972). 

William Lethaby, Architecture. An Introduction to the History and Theory of the Art of Building, 1911. I am grateful to Robert Macleod for drawing this quotation by Lethaby to my attention. 

Evolution of the chemical elements, evolution of physical constants, evolution of information, cultural evolution - evolutionary theory is somehow made to explain all phenomena. The other tendency is to recruit all other developments in science, such as self organizing systems, to expand the theory of evolution to make a new metatheory. Overall there is a tendency to deal with complexity, chaos and catastrophe in the same way; to treat natural and artificial systems equally. 

The optimistic view is that the current debate about the possibility of a new holistic understanding of nature and science will make a significant contribution to current environmental and social problems. This book hopes to underline the importance of these issues. The pessimistic view is that the urge for this holistic understanding is offset by man's desperately myopic determination to maintain his superiority as a species. As each criterion for uniqueness turns out not to be unique after all, a new twist is applied. First language was considered to be unique, then it was the (more difficult to prove) capacity for abstract thought. The ability to use tools was considered unique, until it was demonstrated that other species used tools. and then the criterion became the making of tools ... and so on ad absurdum.What is the problem? Perhaps man's only unique attribute is a self conscious obsession with uniqueness. 
And for what is this supposed uniqueness and superiority used. For the mass murder and torture of his own species, for the keeping of other species under atrocious conditions as a living larder, for the deliberate and conscious destruction of the non-renewable resources and fragile ecosystem of our planet. Homo sapiens is arguably the only species to carry out these acts conscious of the fact that it is possible to behave differently. 
'Modern builders need a classification of architectural factors irrespective of time and country, a classification by essential variation. Some day we shall get a morphology of the art by some architectural Linnaeus or Darwin, who will start from the simple cell and relate to it the most complex structures.' 21

Reptile structural system: John Frazer, 1966 onwards. Design of gymnasium generated by semi-automatic process from a single seed. Computer drawing, Cambridge University mathematical laboratory, 1970. 


SECTION 1 NEW TOOLS 
Throughout this project it has been necessary to design and develop our own tools: our own computer software, our own computer languages and, in some cases, our own prototype computer hardware. When we started there was a shortage of software, computer power, and even books on computer graphic techniques. Since then, the sophistication and range of commercially available software has increased dramatically, and computing power has expanded beyond the most extravagant predictions- while conveniently dropping in cost. Books on computer graphics techniques are readily available, and there are now also disks which provide an accessible introduction to this area. Valuable lessons were learned from having to start from first principles. The initial shortage of computer power meant that a particular economy of computer code was required. 

In turn this need for economy led to a fundamental rethink of the coding and data structures, resulting in a solution peculiarly appropriate to the nature of our analogy. In addition, although commercial software is now easier to find, it never quite seems to do what one wants in the way that one wants. By starting from first principles, we were able to lay down working procedures which seemed natural to the processes that we were considering. We also gained a fundamental understanding of computer processes and languages. The development of our own languages and machine codes for their interpretation required a careful understanding of the structure of language and the coding of information. By designing and building our own logic circuits, we learned to think clearly and economically at the micro-level of individual logical operations. All computation can be reduced to simple logical statements such as 'and' or 'nor'. 

These in turn can be represented by logic gates which are made in silicon out of transistor-equivalent electronics. By building up the logic gates into computational elements it is possible to form any logical function. Again parallels can be seen between the generative methods of nature and the way in which the apparent complexity of computing, in both hardware and software, is built up hierarchically from the simplest functions. 23






This problem was posed by the mathematician David Hilben. It is known by its German name of Enrscheidungs problem (or, more colloquially, as 'the little shitty problem'). 
THE GENERATIVE TOOLBOX 
It is the conceptual model of generative tools which is important, not the hardware or software. This section starts with a brief mention of Alan Turing and John von Neumann, not in this instance to acknowledge their awesome historical role, but to emphasize that they were both principally interested in conceptual computers; in generative processes and the nature of living processes. 
In this tradition the computer is a device with the power and speed to meet the requirements of the limits of our imaginations. We need this power to compress evolutionary time and pace so that results can be achieved more realistically in our life-times. The emphasis, however, rests in the techniques, in the demonstration of the theoretical model and technical feasibility, and in the workings of the thought experiment. 
Perhaps this computing without computers is the most important lesson to be learned from designing these tools. The real benefits are found in having to rethink explicitly and clearly the way in which we habitually do things. There is an adage in the computer business that the main benefits of proposing the installation of a computer system (and particularly CAD) are gained before the system arrives. There is a lot to be said for imagining vast computer power and then merely proceeding with the thought experiment, even though this means that ideas are never put to the test. In our case, we have tried never to use hardware or software constraints as an excuse not to test an idea: if an appropriate tool doesn't exist, we design and construct one ourselves. 
It could further be argued that we do not need to build these tools, but could simulate their behaviour in the computer. The point is that by externalizing and materializing the inner processes of the computer, our physical models act like any architectural model by assisting understanding and visualization. Our models are not just tools which assist with the formative process, but tools of explanation. 
THE TURING MACHINE 
In 1935 Alan Turing was preoccupied with a problem concerning provability. In one of his thought experiments, he conceived of the idea of a universal computing machine. This worked (conceptually) with an endless paper tape and a head which could move backwards and forwards along the tape, reading, writing or erasing symbols. The idea was that the machine would be capable of performing any computable process by following a set of logical instructions on the tape. Turing published this thought experiment in 1936: his paper clearly describes the concept of a reprogrammable digital computer, but it gives no indication that the machine might actually be constructible. Turing's designs for buildable machines were first developed during the Second World War while he was working on breaking the Enigma code used by German U-boats. By the time the world's first computer ran the first stored computer program, he had already moved on and was proposing the notion of artificial intelligence. He used the computer mainly as a device for modelling the morphogenetic processes which were to occupy him for the rest of his life. 

A.M. Turing, 'On Computable Numbers, with an Application to the Entscheidungsproblem', Proceedings of the London Mathematical Society, (2). 42. 1937. 

A.M. Turing, 'The Chemical Basis of Morphogenesis'. Philosopical Transactions of the Royal Society of London. Series B, No. 641, Vol. 237, 1952. 

The Connection Machine was designed by Daniel Hillis of Thinking Machines Corporation. 

The Transputer is a single-chip processor designed by lnmos specifically for parallel applications. 

For an introduction to the use of Transputers in modelling applications see two papers by M .E.C. Hull, J.H. Frazer, A.J. Millar: 'The Transputer for Geometric Modelling Systems', Parallel Processing for Computer Vision and Display, conference proceedings (Addison-Wesley 1988) and 'The Transputer - An Effective Tool for Geometric Modelling Systems', International Journal of Computer Applications in Technology, Vol. 1, No. 1/2 1988, pp. 67-73. 
THE VON NEUMANN MACHINE 
Starting from Turing's proposition for a universal computing machine, John von Neumann developed the logical basis of the serial computer, defining the three basic elements of central processor, memory, and control unit. Serial computers are now generally referred to as Von Neumann machines. Von Neumann went on to build the first American computers, but possibly of more significance still was his work on self replicating automata, from which he developed a theoretical framework for a self-replicating computer, described later in this section. 
PARALLEL PROCESSORS 
In contrast to serial computers, parallel processors can perform many computations at the same time. In one sense a parallel processor is nothing more than a large array of cooperating processors (65,536 processors in the case of the Connection Machine), but the technical problems with such large arrays mean that they have to be specially designed, particularly in terms of their communication capabilities. 
In the case of the Transputer, there are four channels of communication which allow each processor in the array to communicate with four neighbours. These forms of array processor are ideal for our purposes, and it is no coincidence that Daniel Hillis initially designed the Connection Machine to further his own interest in artificial life. 
The first Transputer measured less than 10mm square yet contained the equivalent of over one million transistors. It was too complicated to design manually, so a CAD system was developed for the purpose. This was built up from elements which automatically designed individual logic gates on a rule-based system, making it easier to simulate the operation of the complete Transputer before undertaking the expense of manufacturing a prototype. An assembly of Transputers was i-n turn designed and tested in a similar hierarchical manner. This model of a design process is identical to certain aspects of our evolutionary model. 


It is no accident that one of the key figures in the design team for the Transputer was an architect, David Wilson, who studied first at Cambridge University and then at the AA. 

WISARD is an acronym for 'Wilkie. Stoneham and Aleksander's Recognition Device'. 

Opposite: Experimental neural network computer: Miles Dobson. 1991. 
Neural network computers are intended to be more closely analogous to the human brain than conventional serial computers. The function of neurones is simulated by components which are arranged in a network: several inputs represent synaptic connections which cause the neurone to learn to fire when a particular threshold signal/eve/ is reached. This model was constructed so that the logical structure, the switching of the network and the adjustment of the threshold levels could all be seen and understood. 
NEURAL NETWORKS 
Neural network computers simulate the learning and pattern-recognition capabilities of the human brain. Again they can be thought of as large numbers of serial computers, but in this case linked in a hierarchical pattern of communication modelled on the connection patterns of the synapses in the brain. They are usually simulated with modified conventional processors, although their mode of operation is quite different. Most applications to date stem from the pioneering work of Igor Aleksander and the WISARD, a pattern-recognition device. 
COMPUTER MODELLING 
Computer modelling is a method of virtual representation which provides a form of electronic prototyping. It allows ideas to be developed, described, visualized and evaluated in terms of environmental performance without the expense of actual construction or the time-consuming task of producing drawings. In our case it also provides the framework within which we can generate and evolve our new concepts. There are many computer modelling programs now available, but they all make certain implicit or explicit assumptions about form and its manipulation. It is essential to understand these limitations and to be able to adapt the programs' functionality. The next few pages introduce some of these problems and indicate why we have designed our own modelling and manipulation programs. 
DATA STRUCTURES 
All computer modelling depends upon datastructures, all graphic representation upon transformations. The nature of the datastructure affects the ease with which data can be entered or changed; the amount of information affects the level of sophistication in the rl)odelling and simulation. While the nature of the datastructures in most CAD systems leaves a great deal to be desired, the transformation of the geometric data is now well established. I will return to the datastructure problem later: it is first worth describing certain aspects of the transformation process, which determines the full potential of a computer model. 
Experiments with neural networks: Miles Dobson, 1991. 
This is one of a series of experimental neural network computers with switchable neurone connections. Components simulating the function of neurones are arranged in a network with several inputs representing synaptic connections. These cause the neurone to fire when a particular threshold signal/eve/ is reached. The experiments show that even simple networks can learn about pattern and symmetry, and can cope with incomplete or ambiguous information. 

First described by Robin Forrest in a Technical Memorandum of the Cambridge Mathematical Laboratory and reprinted as 'Transformations and Matrices in Modern Descriptive Geometry' in The Architecture of Form. L. March {ed.), {Cambridge Univeristy Press 1976). 

For an explanation see Robin Evans. 'When the Vanishing-Point Disappears'. AA Files, No. 23. Summer 1992. 

L. March and P. Steadman, Tha Geometry of Environment IRIBA Publications 1971 ). 
 
TRANSFORMATIONS 
The most common two-dimensional transformations are scaling, trans lating, reflecting, differential scaling, rotation and shearing (and identity - keeping it the same). All the two-dimensional transformations can be extended to three axes to give three-dimensional transformations. Additional transformations correspond to our familiar geometrical projections for orthographic (including plan and elevation), isometric, axonometric and perspective (plus the less common projections of cabinet, cavalier, etc.). Some architects are surprised to find that a perspective is as simple as any other computer projection. A three-point perspective is the norm (usually experienced as looking up or down a tall building with the verticals converging). A further transformation is required for a two-point perspective (usually projected from nearer eye level with the verticals shown parallel): the technique here is geometrically similar to that employed for perspective 'correction' in a rising front plate camera with bellows. 
The datastructure is by this stage in the form o(a matrix (a long list of three-dimensional coordinates), and the transformation is simply achieved with a multiplication of the data matrix by the transformation of the matrix. Furthermore, several transformations can be applied simultaneously by multiplying the matrices together to form one complex transformation. The matrix method of transforming a point into a perspective projection is geometrically identical to the technique favoured by Piero della Francesca in De Prospectiva Pigendi. 
SYMMETRY 
Symmetry operations are extensively used in design and architecture, yet only the simplest procedures such as reflection and rotation are generally available as part of CAD programs. The architectural applications of the seventeen symmetry groups in the plane have been described by Lionel March, and are available as single commands in the Shape Processor Language described below. Although March's examples are architectural, and thus essentially three dimensional, they are treated as extrusions of two-dimensional symmetry ope~ations. This is perhaps because architecture is more two-dimensional than we like to think, with the plan or section being the dominant profile- Le plan est le generateur, in the words of Le Corbusier. Three-dimensional symmetry operations can also seem alarmingly complex and are rarely encountered outside crystallographic reference books. Although the basic moves are simple enough, the seventeen possibilities for plane symmetry leap to 230 in three dimensions. We attempted to make the tool of these 230 symmetry operations available. Working with symbolic diagrams in crystallographic reference books and an algorithm for converting the crystallographic code into matrix operations, we built up a three-dimensional reference library in which each of the 230 symmetry groups is represented by three-dimensional computer models. The spatial transformations for each group are ready coded and can be clearly seen in three-dimensional space with examples based on a simple three dimensional arrow. 

Compute r-based symmetry database and model: John Frazer with Heather Rea, research assistant. 1990. 

This pro,ect established a computer database and model of the 230 three dimensional spatial symmetry groups. 

T Hahn (ed ), fnrernatronal Tables for Crystallograplw. Vol A (Reidel Publishin Co. 19871. 
SHAPE PROCESSING 
The first microprocessor-based graphics systems became available around 1977. Although initially very expensive, they illustrated the potential for stand alone graphics workstations which provided a predictable response without the delays typical of time-sharing mainframe machines. Since there was an almost complete lack of graphics and design software for these new machines, we set about producing our ideal working environment. Although our long-term goal was to produce generative softvyare, we first had to produce two- and three-dimensional graphics manipulation and datastructures. We produced our first graphics programs in 1978: the GPPP (General Purpose Pattern Program) and GPPP3 (a three dimensional version). Even these first packages had generative, transformational, combinatorial, random and animation functions. Curve-fitting and splin-ing were soon added. The programs featured a very simple function key user-interface which allowed functions to be combined into complex cumulative transformations. 
The next step, achieved in the same year, was to develop the Shape Processor Language (SPL). The key feature of this graphics language was the capacity to rename any complex group of commands with a user-defined title. This title then became a command in the system and provided an infinitely extensible user-defined graphics command lan-guage. There were no limits to the complexity that this feature could provide in two and later three dimensions. A drawing from the Reptile system containing over 2,000 structural units and 24,000 vectors was re-input and described with just twelve three-dimensional data definition statements and twelve user-defined commands. We overcame the 8K Commodore Pet's lack of suitable graphics by driving a vector planer directly, so that in effect we worked on a mechanized drawing board. 


P.S. Coates, J.H. Frazer. J.M . Frazer and A.E. Scott, 'Commercial and Educational Impact of Shape Processors, Computer Graphics 81, conference proceedings (Online Publications 1981 ), pp. 383-95. 

The Reptile system is described later. 

P.S. Coates. J.H. Frazer, J.M Frazer and AE. Scott, 'Low-Cost Microprocessor-Based Draughting Systems', Computer Aided Design 82, conference proceedings (Butterworth Scientific 1982), pp. 525-35. 

J.H. Frazer, 'An Intelligent Approach', CADCAM International. Vol. 5, May 1986, pp. 34-6. 

This award was made jointly to John Frazer and Julian Fowler. 

T. Stevens. 'Low-Cost CAD', AlBA Journal, December 1984. pp. 50-1 . 
DRAFTING AND MODELLING 
To help finance this very expensive software development we created a commercial version, initially as a two-dimensional drafting system and later as a wireline three-dimensional system. These were launched as Power Assisted Drafting (PAD, 1979) and Autoplan (1980) respectively. Facilities included automatic dimensioning and redimensioning as a component was modified, automatic hatching accommodating holes and curves, and curve-fitting and splining. It was several years before other products with. similar but now better-known names became available. 
From 1983 we developed links with computer-controlled machine tools such as lathes and milling machines. In 1985 we embarked on an educational solid modelling program with an easily operated user interface, which was marketed by British Thornton in 1987 as the 'Design Modeller'. This allowed 'plastic modelling' by means of the interactive manipulation of three-dimensional forms with a three-dimensional cursor. The user-interface was a 'virtual workshop' with a menu of tools which could be used to modify the form . Drafting was fully automatic. Now standard in many modellers, these features were highly innovative when introduced, and were acknowledged with a series o{ -prizes and awards, including a Design Council Design Award in 1988, a Shorts Award for technical innovation, a British Computer Society Award for technical achievement and a Toshiba award for invention. These programs, widely exhibited and widely copied, are still in use in education and training throughout the world. 
LOGICAL DATASTRUCTURES 
The emphasis of most computer-aided design systems has been on the detailed description and modelling of single design ideas rather than on the exploration of alternatives and ranges of related ideas. In one alternative approach, known as parameterization, the description of a family of parts allows the specification of a range of parameters, usually dimensions. While this approach is useful for certain engineering applications, it is really only a more flexible extension of existing datastructures. Another approach is that of 'shape grammars', where the combinatorial rules for elements are specified in order to explore their permutational possibilities. This approach may be better suited to architectural problems, but it requires the syntax and grammar of a particular formal language to be specified explicitly in advance. 


J.H. Frazer. 'Plastic Modelling- The Flexible Modelling of the Logic of Structure and Space' in T. Maver and H. Wagter (eds.), CAAD Futures 87, conference proceedings (Elsevier 1988). pp. 199-208. See T. Maver's essay in the same publication for a description of building evaluation techniques. 

This has been taken up and developed by a University of Ulster research student, Paul Hanna: 'Preserving Relationship Information in Computer Modelling', thesis. 1994. 

'The Environmentally Interactive Building', a report by Architectural Association Diploma Unit 11 with T.R. Hamzah and Yeang Sdn. Bhd .. 1993. 

Both the datastructures and the user-interfaces of CAD systems are designed to develop the geometry of form rather than the geometry of relationships. 

At the 'Computer-Aided Architectural Design Futures Conference' in Eindhoven in 1987 we introduced the concept of 'plastic' or 'soft' modelling in contrast to the rigid implications of solid modelling. The idea was to solicit from the user information of a higher order about the relationships between the elements. Ultimately, we proposed a set of powerful 'relational operators' to encourage the user to specify logical relationships between elements rather than specific geometric coordinates. 
ENVIRONMENTAL MODELLING 
Another element of our task was to establish and model an environment which could simulate and, more importantly, influence the performance of our evolving model. The problem was that programs for evaluating environmental performance all required a detailed design to evaluate. To put that in a simpler context: while there were programs to show solar penetration, they were of limited help in designing a solar shading device, as they could only check in an iterative manner. We therefore embarked on the development of a series of environmental design tools appropriate to our needs in order to gain a better understanding of the processes involved. 
SOLAR GEOMETRY 
Our interest in the influence of the sun on plants and termite mounds led us to concentrate on solar geometry. This interest was reinforced when we were commissioned by Ken Yeang to write a report on environmentally interactive buildings with particular reference to tall buildings in the tropics. We realized that architects who claim to be using the geometry of the sun as a generating factor all too frequently either show misleading diagrams of the path of the sun, or fail to interpret the geometry of the sun correctly in related designs. The extent of the problem can be appreciated if one looks at photographs of buildings which have some sort of solar shading, for a high proportion show the sun hitting the very part of the building that is supposed to be shaded. I am not suggesting that the photographer mischievously waits until the sun is in the wrong place in order to show up the incompetence of the architect. On the contrary, he probably waits for that moment to show the building to best effect. The fact that the architect should subsequently select that photograph for publication, and that neither the publisher nor presumably the reader notice, may tell us something of their priorities. 

The late Anthony Wade drew attention to this problem in a lecture given at the Cambridge University School of Architecture in 1970. He pointed out many photographs of buildings (by Le Corbusier and Jim Stirling in particular) which showed sunlight falling directly on an area which the conceptual diagram indicated should be shaded, or deep shadows where sun was to be expected. 

Architectural Association Diploma Unit 11. 1992/93. 

Part of the problem is the lack of a clear mental model capable of relating the earth's rotation around the sun to the apparent path of the sun when viewed from the earth. The problem is compounded by the tendency for textbooks to use a stereographic projection to show the path of the sun. Architects frequently interpret this to be an orthographic plan view and falsely assume that it can be related to the plan of a building. Furthermore, unless the building is drawn almost to point size. even a plan view may give the impression that certain parts are shaded when in fact they are in direct solar contact. Worse still is the tendency in section to show the sun at summer and winter angles as a single ray. The rays of the sun are of course parallel and, if shown as such across the whole face of the building, they may produce a totally different result to that intended by the architect. 
TOOLS FOR UNDERSTANDING 
In response to these problems the unit developed a whole family of solar tools, starting with tools for understanding and explanation and progressing to tools for design and evaluation. Physical models helped to bridge celestial and terrestrial understanding: students made a dome, which could be viewed either from the outside, to gain a 'sun's eye' understanding of the geometry of daily and seasonal movement, or from the inside, to learn how this geometry was experienced. An associated computer program showed related shadow and sunpatch movement. The next step was a three-dimensional solar protractor which showed solar geometry for every latitude and again facilitated understanding as well as the provision of detailed information. A two-dimensional protractor represented a further level of abstraction. 
Three-dimensional protractor: Samantha Hardingham. Sophie Hicks and Sichin Lin, 1993.
This is a three-dimensional solar tool which displays the relationship between the sun, the earth, the celestial dome and the observer - a model for the comprehension, explanation and visualisation of solar angles. It is used to track the path of the sun and its hourly position relative to a specific position on earth at any latitude at any orientation and on any date

Two-dimensional solar protractor: Sichin Lin, 1993. 
The two-dimensional solar protractor is a scientific instrument designed to show the posi-tion of the sun in terms of azimuth and altitude at any time during any day of the year, and at any latitude on earth It also shows the time of sunset and sunrise on a typical day m any given month The protractor is an abstraction of the three-dimensional solar tool in that it no longer displays a spatial relationship but rather contains concise information for use as a design cool. 

Orthographic sun paths: Guy Westbrook, 1993. 
This computer program was developed to generate orthographic projections of the path of the sun for any latitude and any building orientation. True orthographic projections can be used in conjunction with conventional architectural drawings (or incorporated into conventional computer models of buildings) to give an exact projection of the path of the sun for any proposed building. It provides a design tool that is useful during concept development. rather than a technique for retrospective analysis. 

Solar shading design tools: Manit Rastogi, 1993. 
This technique takes a proposed window opening and then calculates and graphically displays the geometrically limits of the zone of exclusion that will ach1eve complete solar shading. In effect, it projects a negative shadow of the path of the sun patch which would have fallen through the opening. Shading devices can then be confidently designed within this limiting zone. Alternatively, the program can automatically generate the minimum profile of a shading device.
COMPUTER MODELS 
Next followed a series of computer models to assist with design rather than analysis. There were programs that could draw the true orthographic projections for any building location and orientation; programs specifically to design the geometry of solar-shading devices, which operated by projecting a reverse solar shadow, the ghost of the required shading device; and, finally, programs for examining solar penetration. Of special interest to Ken Yeang were active and interactive solar devices, and for these we developed a system of categorization and computer simulation programs. These tools are currently being evaluated in architects' offices worldwide. 

Orthographic sun paths: Guy Westbrook, 1993. 

The 'we' throughout this description refers to John, Julia and Peter Frazer. Some of these developments were carried out in association with the University of Ulster. 

The concept and first working models are described in J.H .. J.M. and P.A. Frazer. 'Intelligent Physical Three-Dimensional Modelling System', Computer Graphics 80. conference proceedings (Online Publications 1980), pp. 359-70. 

The detailed workings of the mats, cubes and versatile systems are described in J.H .. J.M ., P.A. Frazer, 'New Developments in Intelligent Modelling', Computer Graphics 81. 
conference proceedings (Online Publications 1981), pp. 139-54. 
MACHINE-READABLE MODELS 
Our attempts to improve the design of the software of the user-interface were paralleled by attempts to improve the hardware. The keyboard and mouse have never seemed to me well suited to manipulating models or graphics: a digitizing tablet might be closer to a drawing board, but it is rarely used in that way. In any case, we were eager to get away from conventional, drawing board dependent design approaches. 
INTELLIGENT PHYSICAL MODELLING 
We embarked on a series of experiments in using physical models as input devices. These were assembled from kits of parts for building models which had electronic components embedded in them and plugs and sockets to connect them both physically and electronically. We developed a variety of techniques for interrogating an assembly of these components so that a controlling processor could deduce the location, orientation and type of every component and then construct a virtual model of the physical assembly. One such technique was a one-to-one correspondence between the physical model and the virtual model. Alternatively, the type (identifying code) of a component could be used to access a series of mapping instructions so that either the virtual model showed more detail than the physical model, or the physical model was no more than a model of spatial relationships mapped to quite different virtual forms. 
In 1979 we produced the first working model of a self-replicating computer. This was followed by a machine-readable grid-board system which was demonstrated live in the main conference session at the 'Computer Graphics 80' conference. 1980 also saw our development of 'intelligent mats'. This involved mapping the two-dimensional relation 
ships between standard square mats in order to plan relationships of different proportions. Polling for an identifying code caused a mat to wake up and pass back a message by prompting neighbours. A cube version demonstrated that it was possible to operate in three dimensions employing a different electronic technique. A means of self-inspection was introduced by transferring control to each cube as it was encountered. Each cube in turn explored each face to see if it had a neighbour: if it had, it sent back a message to say what it had found and where, and control was then transferred to that cube, and so on, until the search was complete. Light-emitting diodes were used to indicate the search path so that the process of self-inspection could be understood in slow motion. Later that year we constructed a more complex version of the model which featured blocks in a variety of shapes and sizes, including some oblique angled connections. Provision was made for each component to have its own eight-bit identifying code which could be extended to sixteen or more if necessary. This allowed for an infinite number of different components (256 with just eight bits) and provided a means for additional data to be trans mitted. For example, one component contained six mercury tilt-switches which allowed the gravitational orientation of the model to be established. Another had reed-switches sensitive to magnets embedded in external 'cladding' panels, which allowed the fenestration to be changed when mapped to the virtual model. 


Machine-readable models as input devices, working prototypes: 
John, Julia and Peter Frazer, 1980. 
Intelligent beermats. 
Embedded electromcs make 1t possible to Identify each mat un1quely and to pass back messages to a controflrng processor which then deduces the spatial configuration ol the mats. As the mats can be made to represent spaces. they can be used as a quick wav of permutating spatiaf arrangements 
Three-dimension a I intelligent modelling system. 
The technique used for the mats is extended into three d1mensions and a concept of self inspecuon IS rntroduced. Each cube in turn exam1nes each of 1ts faces. and if another cube 1s found a message is sent back and control is transferred from cube to adJacent cube untif an exhaustive search 1s completed. Light-emitting diodes allow the search route to be followed m slow motion. 

Flexible intelligent modelling system. 
Flexibility 15 improved with variable geometry, orienrati011 sensors and the option of cladding sensor The plotter 1s drawing tile conftguratilon of the blocks which the controlling processor has deduced by interrogation 

Miniaturized system. 
Very small blocks can only be stacked vertically, but can be of 256 types thus providing a versatile but mmrature machine-readable modellmg /111 . 
This model contained many elements which proved crucial to the development of our conceptual model, including self-inspection and searching for neigh bours, transfer of control, transmission of messages, coding and mapping, sense of gravitational orientation, etc. The technique of polling for a unit and passing back messages has also been incorporated into our most recent model. 
The last step was to miniaturize the system to the level of bricks smaller than two sugar cubes. To achieve this we streamlined the electronics to vertically stacked eight-bit coded units combining the grid-board technique with the message-passing. This meant that it was possible to build an architectural model with toy bricks and to interrogate it by means of a controlling processor: the result was a virtual model from which complete drawings, perspectives and calculations could be produced. Although still severely limited to a kit-of-parts approach, it provided a viable alternative to keyboard, mouse or digitizer input. All of these were reviewed in the context of alternative three dimensional data systems at the 'Computers Graphics in the Building Process' conference. Washington, 1982. 

Working electronic model of the Generator project: John and Julia Frazer, computer consultants to architect Cedric Price, 1980. Cedric Price 's proposal for the Gilman Corporation was a series of relocatab/e structures on a permanent grid of foundation pads on a site in Florida. We produced a computer program to organize the layout of the site in response to changing requirements, and in addition suggested that a single-chip microprocessor should be embedded in every component of the building, to make it the controlling processor. This would have created an 'intelligent' building which controlled its own organisation in response to use. If not changed. the bUilding would have become 'bored' and proposed alternative arrangements for evaluation. learning how to improve its own organization on the basis of this experience. 
THE GENERATOR PROJECT 
These modelling techniques underwent further development in projects for Cedric Price and Walter Segal. In 1978 Cedric Price asked us to work as computer consultants on the Generator project. This consisted of a kit of parts which enabled enclosures, gangways, screens and services to be arranged and re-arranged to meet the changing require-ments of the client, the Gilman Paper Corporation. It was proposed to grid the site (a clearing in a forest in Florida) with foundation pads and to provide a permanent mobile crane for moving components, allowing the users of the building to become involved in its organization. 
The computer program was developed to suggest new arrangements of the site in response to newly defined needs. By embedding electronics in every component and making connections to the foundation pads, we effectively turned the site into a vast working model - a gigantic reconfigurable array processor, where the configuration of the processor was directly related to the configuration it was modelling. At this stage, the controlling processor could be dispensed with, as there was more than adequate processing power distributed throughout the structure of the building. The building thus became 'intelligent', as we explained in a letter to Cedric Price in 1979. This led to some amusing headlines: 'Birth of the Intelligent Building', 'Thinking for Fun' and 'The Building that Moves in the Night'. Unfortunately the term 'intelligent building' has now been devalued to mean any building with provision for information technology! The term 'intelligent structures', on the other hand, has acquired considerable meaning and is now the subject of serious books and a journal. 

These projects were widely reviewed in the technical press. for example: P. Purcell, 'Technical State-of-the-Art'. CAD Journal, Vol. 13, January 1981; W.S. Elliott. 'Critical Technologies Contributing to CAD/CAM', Symposium on Trends in CAD/CAM. Nice, September 1981; B. Reffin-Smith, 'Designing tor the Animated Standard Person', Computing, 17 December 1981 ; G. Pask. Microman (Century Publishing Co. 1982). pp. 54. 128-30; R. Sarson, 'It's Coming: Intelligent Lego', Computer Talk, 14 January 1985; B. Evans, 'Intelligent Building Blocks', Architect's Journal, January 1985, pp. 47-54. 
For a general description of the project see 'Cedric Price's Generator'. Building Design, 23 February 1979. 

'World's First Intelligent Building', RIBA Journal, January 1982; D. Sudjic. 'Birth of the Intelligent Building'. Design, January 1981; 'Thinking for Fun', Building Design, 18 August 1980; 'A Building that Moves in the Night', New Scientist. 19 March 1981. 

An excellent source is K.P. Chong, S.C. Liu and J.C. Li. Intelligent Structures 
(Elsevier 1992). 

Jonathan Greig, 'Celebrating the Cerebral', Building Design, May 1988, Supplement. 
We were concerned that the building would not be changed enough by its users because they would not see the potential to do so, and consequently suggested that a characteristic of intelligence, and therefore of the Generator, was that it would register its own boredom and make suggestions for its own reorganization. This is not as facetious as it may sound, as we intended the Generator to learn from the alterations it made to its own organization, and coach itself to make better suggestions. Ultimately, the building itself might be better able to determine its arrangement for the users' benefit than the users themselves. This principle is now employed in environmental control systems with a learning capability. 

Important new ideas emerged from the Generator project. These included embedded intelligence and learning from experience during use, the isomorphism of processor configuration and model structure, and the question of consciousness. I am not interested- in the argument about whether computers are actually intelligent, alive or conscious, but as a mental exercise it is interesting to consider a building to be conscious at least in the sense of being able to anticipate the implications of its actions, as any good environmental control system should be able to do. My son, John-Paul, once remarked that if a building was conscious, then it could get depressed and choose to terminate its existence by demolishing itself. 

Self-builder design kit, working electronic system: John and Julia Frazer, consultants, with John Potter, research assistant, for architect Walter Segal, 1982. 

Calbuild kit: Stephen Brown and John Frazer with David McMahon, research assistant, 1985 . 
THE WALTER SEGAL MODEL 
Walter Segal developed a timber-framed building technique suitable for self-builders in a scheme promoted by Lewisham Council. It was not a building system in the sense of a kit of parts, but rather a discipline in the efficient and economic use of materials. It used a two-foot grid with a two-inch structural zone which allowed the economic cutting of panels from standard eight-foot by four-foot sheets. Segal was anxious that his self-builders should also become designers, and he encouraged them to visualize the plan using first matchsticks to represent the two-foot module and ttien a slotted base-board and panel model. He found, however, that the self-designers required a great deal of help in interpreting their models. 

Walter Segal developed a popular and successful self-build timber-framed housing system but also wanted to encourage self builders to be self-designers. To this end, we developed an interactive machine-readable modelling kit. The self-designers had simply to arrange the components, and a computer would scan the model electronically, identify all the parts and their locations and automatically assemble them into a plan and, finally, interpret the plan and produce costs, calculations, schedules and drawings. Thus a sell-builder with no knowledge of architecture could quickly produce bUildable designs according to Segal's method. 
This model was later modified to become a teaching aid to encourage "what if' computer dialogues. The different colours of panels represent different construction materials and insulation values. and can be quickly exchanged to evaluate design alternatives. 

For a description of Segal's method see J. McKean, Learning from Segal 
{Birkhauser 1989). 

The 'we' refers initially to John Frazer, who designed and constructed the electronics, and later includes John Potter. the research assistant at the University of Ulster who developed the software. 

There is a complete description of this project in J.H. Frazer, 'Use of Simplified Three Dimensional Computer Input Devices to Encourage Public Participation in Design', Computer Aided Design 82, conference proceedings (Butterwonh Scientific 1982). pp. 143-51 .. 

For comment see H. Pearman, 'Computer Aided Living', Design, September 1985, pp. 48-9, and 'Call up a Fine Design', ObseNer, 3 November 1985. 

The 'we' is now John Frazer and Stephen Brown at the University of Ulster. with David McMahon as research assistant. 
We built an electronic version of the panel model with an eight-bit code on every panel and fitting, which allowed 128 different panels, door and window combinations, taking into account door swings, etc. A controlling processor scanned the board and interpreted the plans (two storeys were built side by side). Plans and three-dimensional views could then be displayed, areas calculated, costs determined and structuraI frame drawings produced. In effect the program incorporated not only Walter Segal's design rules but much more of his expertise. As a result, people without any knowledge of architecture or computers could design a house by building a simple model. The working system was successfully demonstrated to Walter Segal in his house in 1982. He was delighted with it, but his untimely death prevented him from experimenting further with the system. 
We had some of Sega~'s ideas literally preserved in silicon and could have continued with the project after his death. On this occasion we decided not to, but it is an interesting notion that architects' ideas could be made permanently available in this way. Quite unlike a design 'in the style of', it would allow others to create projects using the actual generating programs used by deceased architects- a bizarre prospect given the strangeness of using these programs when the ~architect is alive. The Segal project showed us that there was real potential for client and user-involvement in design and a new potential for immortality for the architect. 
Having decided not to continue with the Segal experiment, we _converted the model into a teaching aid. Coloured panels were used to represent different building construction and insulation, allowing students to experiment with 'what if?' scenarios to learn the implications of trading heat loss against solar gain, and so forth. 
THE UNIVERSAL CONSTRUCTOR 
In 1990 the unit designed and constructed the most ambitious of these intelligent models. It was a three-dimensional array of identical cubes that we called the 'Universal Constructor', in reference to Von Neumann: we wanted also to emphasize the intended generality and universality of its function. Cubes, rather than a more realistic representation of an element, were chosen for their universality, which meant they could represent anything, and for their self-similar geometry, which meant they could model at any scale. 

Each cube could have any one of 256 states which were displayed by means of eight light-emitting diodes (LEOs). Thus the eight-bit code could be used to map the state of the cell to any form or structure; to environmental conditions such as wind; to sound, or even to dance. There was a 12 x 12 cell base-board with the same electronics as the cube, and vertically stacking cubes to a height of 12. This created a 12 x 12 x 12 x 256 logic space. Each cell had an identifying state and was capable of displaying any other state. Messages could be passed between any two units by streaming data in serial form down one stack of cells and up another. The flow of logic in the model was thus made visible by slowing down the system. The array could be used as an input or output device. For input, the exact configuration, location and identifying code of every cell could be deduced by a controlling processor interrogating each location and prompting for a possible neighbour above. As an output device, each cell could display 256 messages with the eight LEDs. Interaction with the observer was made possible by two red LEDs- one flashing light meant 'take me away'; two flashing lights meant 'add a cube on top'. 
The Universal Constructor was by far our most elaborate model, with nearly 500 integrated circuits, each with 400transistor-equivalent complexity, over 3,200 LEOs, over 3,200 Zener diodes, 12 pin-connectors between the cubes, and a 40-amp power supply. It was designed and constructed by the unit as a whole and had a common computer program for interrogation, message-passing and screen display. Each member of the unit had his or her own individual application, program and mapping. In a typical application, the different codes in the physically flat base were translated in the computer model as heights in a virtual contoured landscape. 

Opposite and above: Computer-controlled spline curve-generator, working prototype: Pete Silver, 1990. 
This splining machine provides both a visualisation technique and a design tool for drafting accurately splined curves at drawing board scale. Four high-precision linear actuators control a flexible spline in direct emulation of naval architecture's traditional analogue technique of bending a spline and holding it with weights. The linear actuators are canto/led in 0. 05mm steps by a computer using data initially derived from the Universal Constructor. 

The 'unit' in this case is Diploma Unit 11 (originally numbered 14) at the Architectural Association. 1989/90. 

J.H. Frazer. "Universal Constructor' in Wim van der Plas (ed.). Second lntemational Symposium on Electronic Art. conference proceedings ISIS EA 1991). 

There is a full description in J .H. Frazer, 'Datastructures for Rule-Based and Genetic Design', Visual Computing - lntegratmg Computer Graphics with Computer Vision (Springer-Verlag 1992). 
An observer was encouraged to set a problem by adding some environmental features with the coded cubes. The application program would then respond by soliciting the addition of more cubes representing its coded and diagrammatic response. Finally, the display screen interpreted and mapped these coded representations to a virtual but less abstract model. The range of different applications was diverse and included three-dimensional cellular automata responding to a site problem, a curve-fitting program controlled by a Fibonacci series, and an encoding of the Laban dance notation. 
One project had a rule-based system for generating proportionally-based curves. The 256 states were used to drive stepper motors which had splines attached to generate a complex curved surface. Input mapping was also possible and in one example a bicycle was wired with sensors which controlled1he state of the cells, making it possible to take a virtual bicycle ride round the model via the display screen. In the dance example, the interactor established a stage 
set or landscape for the system to 'perform' in. The system then requested units to be added at the points where dance positions or leaps were to occur. This caused the lights in the model to 'dance' a unique dance choreographed by the rules in response to the environment; the 256 possible combinations represented positions in the Laban notation. 
A number of other experimental design applications were attempted, including an evolutionary model. This first developed using the 256 available cell states and evolved as a series of loops in a chaotic string where each step in the evolutionary development was continuously read and tested against the rule, defining a pattern or path-structure in space. Later, the system was extended to 16-bit state patterns, which allowed evolution to a very high level of complexity and produced forms of a highly unexpected, although sequentially logical, form. 
The Universal Constructor, first shown at the Architectural Association in May 1990, represented a tool for the explanation and demonstration of a radically new design process and attracted considerable attention and press comment when presented in Groningen and Tokyo. It was understood in architectural terms as an expression of logic in space. 

Evolving sequence from the Universal Constructon Stefan Seemuller, 1991.
From an tmllal conftguratton of cubes, each wllh an 1denttfymg state denved from the Unwersal Constructor, the sequence evolves bv usmg th state numbers to control parameters m a graphic mappmg program. The process 1s a senes of loops m a chaotic 


The Universal Constructor, working model of a self-organizing interactive environment: group project, 1990. 
The model consists of a base-board which is termed the 'landscape', and a senes of cells wh1ch can be stacked vertically at specific locations on the landscape. The cells can represent either structure or environment: each contains an integrated circuit which can communicate with the units above and below it. Each landscape location can also communicate with a controlling processor. 
Each unit has an identifying code w h1ch it can display w ith eight llght-emllmg diodes and commumcate to adjacent units. The system knows what each part IS and where it is. In additiOn ro having an Identifying code. each unit can also display the state m wh1ch 1t exists and pass messages to anyone mteractmg wit/1 the model. This direct interaction encourages experimentation and helps to explam the concept to others: cells can iJe added or 1emoved by simply plugging o unplugging the units. The whole system tS machme readable. allowmg any change to be detected immediately in terms of the idenrifymg code and 1ts locatton. The stare of each cell can be mapped to a graphics output device where it is represented by colour or a geometrical transformation. 
In a typical expenmental appf1catton the sys tem w ill reques t an interactor co conftgure an enwonment conslstlllg of cells m different states. Usmg fights, the mode/then ind1cares 1ts proposed response by askmg the mteractor's assistance m adding or removing units. The participator can m turn moddy the env1ronment. 

Alvy Ray Smith, 'Introduction to and Survey of Polyautomata Theory' in A. Lindenmayer and G. Rozenberg (eds .). Automata, Languages. and Development 
(Amsterdam 1976), pp. 405-22. 
Opposite: 
Three-dimensional self-organizing constructor: lchiro Nagasaka, 1991. This experiment in self-organization works like a three-dimensional cellular automaton except that both positive and negative feed back are used to modify the way in which adjacent cells interact. The interaction between cells relies on their comparison of the features of their internal growth and their attempt to copy successful behaviour. 
POLY AUTOMATA 
Polyautomata theory is a branch of computational theory concerned with a multitude of interconnected automata acting in parallel to form a larger automaton. We usually simulate polyautomata using an ordinary serial computer, but in some instances we built special-purpose machines to study their properties. Our interest in these systems stems from their potential as generative devices, and from their simplicity, which makes them particularly appropriate for exploring rule-based systems. 
CELLULAR AUTOMATA 
Cellular automata are a special class of polyautomata which lend interesting insights into how complex behaviour can emerge from simple rules. They are disarmingly simple devices for demonstrating the behaviour of rule-based systems. 
Acellular automaton consists of a regular array of _cells in one, two, three or more dimensions. Each cell can have at least two states: in the simplest automata these might be 0 or 1, true or false, black or white, alive or dead, but in more complex automata more states are possible. Each cell is said to have neighbours which are cells with some specified spatial relationship. In the simplest two-dimensional grid of squares, a cell might be defined as having four edge-neighbours, or eight, including those on the diagonal. In three dimensions, a cubic cell might be considered to have six face-neighbours, or up to twenty-six if those adjacent to the edges and vertices are also counte~. Additionally more remote neighbours can be considered. For example, in a one 
dimensional automaton the neighbourhood zone may be extended from the two adjacent cells to include the next two or even those beyond. 
The behaviour of the cellular automaton is controlled by transition rules which determine the state of each cell at each moment in time in relation to the previous state of the cell and of its neighbours. In other words, the state of the whole of an automaton at one instant is determined by its state at the previous instant and by the operation of transition rules which usually affect every cell in the same way, synchronously (the system responds to some form of 'clock' pulse). The rules for the individual cells are referred to as 'local' and the overall behaviour as 'global'. For example, the transition rules in a one-dimensional two-state system might be simply of the form: 'if a cell is in state 0 and both its neighbours are in state 1, then on the next clock pulse the state of the cell should change to 1' (the cells have to be examined individually although they all change together). Successive changes are often referred to as 'generations' and 'neighbours'; affecting behaviour may be referred to as 'parents' and 'grandparents', with the adjacency of the 'offspring' being referred to as on a 'hand'. A starting configuration, referred to as the 'seed', is usually required. This may be just one cell, but more often it is a pattern of cells which has either been established at random or carefully designed (see the 'Life Game' below). 


The behaviour of a cellular automaton is defined thus by E.F. Codd (restating Von Neumann): 'The state of a cell at timet+ 1 is uniquely determined by its neighbourhood state at time t, together with the transition function f of the finite automaton which is associated with very cell. (The neighbourhood of any cell includes the cell itself.)', Cellular Automata (Academic Press 1968). 
S. Wolfram, 'Universality and Complexity in Cellular Automata', Physica D, No. 100. 1984, pp. 1--35. 
Von Neumann, 'Theory of Automata: 
Construction, Reproduction, Homogeneity' in A.W. Burks (ed.), The Theory of Self Reproducing Automata, Part II (University of Illinois Press 19661. It should be noted that Stanislaw Ulam is credited with suggesting to Von Neumann that he could build a theoretical model using the principle of a cellular automaton. 
R.G. Schrandt and S.M. Ulam, 'On 
Recursively Defined Geometrical Objects and Patterns of Growth' in A.W. Burks (ed.l. Essays on Cellular Automata (University of Illinois Press 1968). and E.F. Codd. Cellular Automata (Academic Press 1968). 
Surprisingly complex behaviour can result from simple local rules. While the rules control the overall behaviour of the emergent 'pattern', the precise nature of an actual configuration may be significantly affected by the 'seed'. A variety of behavioural patterns can emerge. Possibilities can include cells becoming identical (the pattern 'dies'), permanent chaos, and the development of complicated local structures or of a periodic pattern. The behaviour of one dimensional automata has been explored and analysed by Stephen Wolfram. 

Cellular automata are, technically speaking, finite state machines which can be regarded as a class of computer. They are at least capable of simulating computational operations. This was the purpose of the Turing Machine and the Life Game. Other cellular automata have been developed to model and simulate a variety of biological and physical phenomena. They can be seen, therefore, as an example of a technique which can be applied to both natural and artificial phenomena. 
Self-Replicating Automata 
John von Neumann also indulged in thought experiments and began to theorize about automata, in particular self-replicating automata. He considered the Turing Machine to represent a class of universal automata which could solve all finite logical problems and began to- investigate the possibility of one automaton taking raw materials and building another automaton. Also examined was the feasibility of such an automaton physically replicating itself and proceeding to develop even more complex forms. In other words, Von Neumann set about designing a self-building, evolving automaton. What he in fact developed was immensely complicated and essentially unbuildable, requiring some 200,000 cells in any one of twenty~nine states. It remained a paper exercise which was part cellular automaton, part robot, having only a conceptual robot arm. 
It is more than coincidence that during the period when John Frazer was developing the Reptile system (described in the next section) in the mathematical laboratory at Cambridge, he shared the use of the computer-graphics system most nights with John Conway while he was developing the Life Game. 
THE LIFE GAME 
During the sixties John Horton Conway developed what he called the 'Life Game' while working in the mathematical laboratory at Cambridge University. As the name suggests, the Life Game appears to simulate life-like behaviour, but the intention was to demonstrate a universal computer of a very much simpler form than that proposed by Von Neumann. What Conway achieved was a two-dimensional, two-state cellular automaton on a square grid with simple transition rules but the most surprising properties. The rules and behaviour are described throughout in anthropomorphic language: cells are 'alive' or 'dead'. Eight neighbours are considered: a cell will die of loneliness in the next generation if less than two of its neighbours are alive, and die of overcrowding if more than three are present. The optimum number of neighbours for survival is two or three. A dead cell can come to life if it has three live neighbours. 
This simple set of rules produced some extraor,dinary properties and behaviours. A 'gun', for example, generates a continuous stream of pa~erns, demonstrating the potential to grow indefinitely. A 'glider' is a pattern which moves continuously by displacing, in exact balance, a short cycle of birth and death events. An 'eater' engulfs and destroys any pattern that it encounters. There is a thriving industry in related publications and any reader unfamiliar with the behaviour of the Life Game is encouraged to try one of the many programs now readily available. 
Conway's Life Game is a universal computer in the sense that it can demonstrate all computational processes and also simulate and model many life-like processes. This has led to a great deal of speculation on the relationship between computation and vitalistic forces. Nevertheless the Life Game is not a model of nature and evolution, since, ame-ngst other things, it is strictly deterministic in nature. 
Yet again it clearly shows that simple rules can produce complex emergent behaviour. 
ARTIFICIAL LIFE 
In turn the Life Game has spawned a vast amount of speculation and research in the field of 'Artificial Life'. This book shares the same body of theory. Our model of architecture exhibits the characteristics of metabolism, epigenesis, self-reproduction and mutability, which are generally agreed to be requirements of life. However we are considering the implications of viewing our model of architecture as if it were a form of Artificial Life, whereas most enthusiasts in the field are searching for artificial forms of behaviour which they can claim as alive.

C.G. Langton (ed.). 'Artificial Life. 
Proceedings of Interdisciplinary Workshop on the Synthesis and Simulation of Living Systems. September 1987' in Santa Fe Institute Studies in the Science of Complexiry, Vol. VI, 1989. 
C. G. Langton at al (eds.) 'Artificial Life II: Proceedings of the Workshop on Artificial Life, February 1990'. in Santa Fe Institute Studies in the Science of Complexiry, Vol. X, 1991 . An oveNiew is provided by S. Levy in Artificial Life- The Ouest for a New Creation (Jonathan Cape 1992). 
J.H . Holland, Adaptation in Natural and Artificial Systems (University of Michigan Press 1975). 
57 
 We are avoiding being drawn into this debate, as it inevitably centres on arguments about the definition of life. A simple computer virus passes most of the traditional tests for being alive, and in many alarming respects it exhibits life-like behaviour, yet only strong 'A.L.' enthusiasts (as the jargon would describe them) would claim that a computer virus is alive in the same sense as you or I are alive. 
The Artificial Life field has generated some remarkable ideas. Of particular interest are the proceedings of the interdisciplinary workshops on the synthesis and simulation of living systems held in Los Alamos in 1987 and 1990. 
EVOLUTIONARY TECHNIQUES 
A variety of new computer techniques enable problems to be solved by successive improvements and developments. The simplest method consists of measuring a given performance and selecting the solutions w~ich represent a small improvement while rejecting the remainder. This simple approach works for optimizing already satisfactory solutions, where only a small variation of quality is required. In cases where an already well-developed solution does not exist, or a possible improvement may be radically different, more sophisticated techniques have to be developed. 
ADAPTIVE MODELS ??
John Holland has developed a theoretical framework for an adaptive model which first requires a restatement of the biological position. He defines an adaptation's salient features as the progressive modification of a given structure by the repeated action of certain; operators. He cites chromosomes as the structure for genetics, and mutation and recombination as the operators. The structures of physiological psychology are seen as cell assemblies, and the operators as synapse modification. 
This forms the basis of a mathematical formalism which defines a set of structures appropriate to the field of interest using all possible combinations of the elements (chromosomes). An environment is then defined for the system undergoing adaptation: this is an adaptive plan which determines successive structural modifications in response to the environment and measures the performance of different structures in the environment. 

Diagrams by Raymond Chui 
R. Dawkins, The Blind Watchmaker (Longman 1986). 
The adaptive plan produces a sequence of structures from a set of operators. Information obtained from the environment influences the selections made. Evaluation then takes place using the specified measures. Holland illustrates applications in genetics, economics, game-playing, pattern-recognition, control and function optimization and the central nervous system. This treatment is then developed into generalized reproductive plans and genetic operators. This approach has come to be described as the application of 'genetic algorithms'. 
GENETIC ALGORITHMS 
The technique of the genetic algorithm was developed primarily for problem-solving and optimization in situations where it was possible to state clearly both the problems and the criteria to be fulfilled for their successful solution. 
The scientific community's excitement about these techniques stemmed from their ability to produce solutions not 'imagined' by the instigator of the program, and to solve problems whose detailed structure was not understood. 
Genetic algorithms are a class of highly parallel, evolutionary, adaptive search procedures. They are characterized by a string-like structure equivalent to the chromosomes of nature. These represent a coded form of parameters which control the problem being investigated. They are described as highly parallel because they search using populations of potential solutions rather than searching randomly or adjusting a single potential solution. Since optimum solutions are obtained by small, gradual changes within the population over several generations, they are defined as adaptive. Selection from the population occurs according to a measure of fitness criteria. 
BIOMORPHS 
Biomorphs are a class of two-dimensional, recursive, tree-like structures developed as a demonstration by Richard Dawkins. Their growth and development is controlled by a simple genetic representation which determines a particular aspect such as the number of branches or the branching angle. A population is generated by mutation and one biomorph is selected for further breeding. When represented in a demonstration this approach can appear dramatic, as development can be rapidly focused in a preferred direction. However it is limited as a search and optimization technique, as all the genetic information from the unselected offspring is lost along with the advantages of parallelism. Nevertheless, as a means of popularization, rather than as a refinement of a technique, the biomorphs have been very successful. 

K. Simms . .' Anificial Evolution for Computer Graphics' in Computer Graphics, Vol. 25, No. 4, July 1991, pp. 319-28 and 'Interactive Evolution of Dynamical Systems' in Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life (MIT Press 1991); S. Todd and W. Latham, Evolutionary An and Computers (Academic Press 1992). 
In this instance the 'unit' is Architectural Association Diploma Unit 11 1988/89. 
And 'we' is John Frazer with Peter Graham as research assistant at the University of Ulster. 


CLASSIFIER SYSTEMS 
With classifier systems the emphasis moves away from the search for one optimum solution to the production of a population that has learned to respond in an appropriate way to particular inputs. They are more appropriate to classes of learning problem than to optimization, but the techniques are usually combined. A classifier system receives information from the environment, the information is checked against conditional rules (classifiers), and the rules are acted upon in order to output to the environment. This linking of detection, internal processing of information, and action is regarded as analogous to an organism perceiving information from the environment, thinking about it, and acting on it. If the action produced is effective, the organism receives some pay-off or reward, and this reinforcement of success is modelled in implementations of classifier systems. 
CONFLICTING CRITERIA 
Genetic algorithms were initially developed for scientific problems requiring search and optimization techniques with precise technical applications. Typical of_ such problems may be the optimization of the profile of a turbine blade. Here, the selection of chromosomes is based on simulating the performance of the blades defined by the chromosome: those with the best performance are chosen (natural selection). The potential of this technique has been broadened - notably by Dawkins' biomorph program - to allow the program-user to make the selection on other grounds, such as appearance (artificial seletion). This approach has been taken up in a number of fields- including fine art, in the work of Simms and Latham. 
We experimented with means of extending and modifying Holland's original techniques to suit our problem. The unit took classifier systems and applied them to matching performance and environmental response. We mixed both convergent and divergent evolution, natural and artificial selection, and applied them to problems with ill-defined or conflicting criteria. In the case of defined criteria, operating under artificial selection and letting the designer use his or her intuition proved remarkably successful, first with the simple example of evolving a Tuscan column, and later with evolving yacht hulls. Natural and artificial selection were combined by a number of means. These included using complex calculations of displacement, wetted hull area and block co-efficients to estimate performance. For criteria for which the calculations proved less effective, we relied on the designer's skilled eye.
Such a technique may work, but it reduces the efficacy of the technique in dealing with complex problems with very large numbers of iterations. This necessitated more automation and, to achieve that end, we introduced hierarchical genetic algorithms - one algorithm learned the criteria for selecting the outcome of the next. Eventually this was refined into the hierarchical classifier system described in the final section of this book. 
J.H. Frazer and P. Graham, 'Genetic 
Algorithms and the Evolution of Form' in Third International Symposium on Electronic Art. conference, Sydney 1992. 
J.H. Frazer and P. Graham. 'The Application of Genetic Algorithms to Design Problems with Ill-Defined or Conflicting Criteria'. Conference on ValuBs and (ln)Variants. conference proceedings. Amsterdam 1993 (publication pending). 
J.H. Frazer. 'Can Computers Be Just a Tool?', Systemica: Mutual Uses of Cybernetics and Science, Vol. B. Amsterdam 1991. pp. 27-36. 
J.H. Frazer and J.M. Connor. 'A Conceptual Seeding Technique for Architectural Design', PArC 791nternational Conference on the Application of Computers in Architectural Design. conference proceedings (Online Conferences with AMK 1979). pp. 425-34 . 

JUST TOOLS? 
Our use of the term 'tool' throughout this section does not imply that we regard these techniques in the dismissive sense implied by the phrase 'a computer is just a tool'- as a means of reinforcing current practice rather than challenging it. At the 'Conference on the Application of Computers in Architecture' in Berlin in 1979 I specifically questioned the prevailing assumption that computer-aided design techniques should reflect traditional design methodologies. I said: 
This paper rejects the notion that a CAD approach should reflect the traditional non-CAD architectural methodology on the grounds that, first, the present architectural design process is fundamentally unsatisfactory in any known form and not worth imitating and, second, imitating the human process is unlikely in any case to represent the most imaginative use of a machine. The idea that CAD programs should be useful to designers in the existing context is also rejected on the grounds that they may be tailored to make them acceptable to the designer's sensibilities instead of addressing themselves to the problem of producing results which might be acceptable to the user's sensibilities.' 
The tools that we have been developing for this project are clearly not intended to reinforce existing practice. 
The following section describes how we used them to generate a new model of the. generative architectural process. 


Development of yacht hull by genetic algorithms: John Frazer with Peter Graham, research assistant, 1993. 
The objective of this research project at the University of Ulster was to use genetic algorithms to optimize the performance of a racing yacht hull by a combination of both natura/and artificial selection. The procedure was the same as in the preceding examples. in that a genetic code was designed to control the fairing of the curves of the hull's profile. A population was generated with very small genetic mutations. Each hull was then plotted out and complex calculations were performed for displacement, trim, waterplane. wetted surface and for block and prismatic coefficients which gave some indication of the likely performance of the hull. Natural selection was applied by breeding funher populations from the hulls with the most promising coefficients. Anificial selection was also be applied by selecting huJ/s for breeding on the basis of experience, intuition or other considerations such as the ergonomics of the deck layout. 

Evolution of Tuscan columns by genetic algorithms: John Frazer wlth Peter Graham, research assistant, 1993. 
This research project at the University of Ulster used genetic algorithms to evolve proponional information. James Gibbs' rules for drawing Tuscan columns specify the ratios between all the pans of the column. The structure and logic of the rules are programmed into the computer but a gene is substituted for each of the carefully specified proponions and a population of random mutanrs is generated. Selection can occur by two different methods. 'Natural selection' is used where it is assumed that there is some natural advantage in Gibbs' proponions. In this case. when the computer automatically selects for breeding the two columns closest to Gibbs' ratios, convergence on a perfectly proponioned Gibbs Tuscan order occurs in very few generations. Alternatively, in 'anificial selection', columns for breeding can be selected by eve. either with the intention of breeding a Gibbs-like column, or to develop a deliberately different set of proponions. 
SECTION 2 THE NATURE OF THE EVOLUTIONARY MODEL 
PART 1: THE GENERAL PROPOSITION 
The evolutionary model requires an architectural concept to be described in a form of 'genetic code'. This code is mutated and developed by computer program into a series of models in response to a simulated environment. The models are then evaluated in that environment and the code of successful models used to reiterate the cycle until a particular stage of development is selected for prototyping in the real world. 
The real-world prototype is expected to be capable of interactive response to the changing environment on a short time-scale, but this is not essential in the theoretical model. In order to achieve the evolutionary model it is necessary to define the following: a genetic code-script, rules for the development of the code, mapping of the code to a virtual model, the nature of the environment for the development of the model and, most importantly, the criteria for selection. It is further recommended that the concept is process-driven; that is, by form-generating rules which consist not of components, but of processes. It is suggested that the system is hierarchical, with one process driving the next. Similarly, complex forms and technologies should be evolved hierarchically from simple forms and technologies. 
GENERATIVE DESCRIPTION 
In order to create a genetic description it is first necessary to develop an architectural concept in a generic and universal form capable of being expressed in a variety of structures and spatial configurations in response to different environments. Many architects already work in this way, using a personal set of strategies which they adapt to particular design circumstances. Such strategies are often pronounced and consistent to the point where projects by individual architects are instantly recognizable. All that is required is that this generic approach is explicit and sufficiently rigorous to be coded. The process that we are describing has evident parallels with the way much conscious design occurs: it is also similar to the way in which many vernacular archetypes and successful prototypes have been developed and adapted for different sites, environments and individual requirements. 


See Brian Hatton's interview with John Frazer in Lotus. No. 79, 1993, pp. 15-25. 

However we do not propose a return to vernacular forms of building evolution, for that tradition can no longer meet the requirements of contemporary urban life. 
Nor can we advance the evolutionary process by constructing and evaluating full-size prototypes, as was the practice in the past, with the construction of the Gothic cathedrals, for example. This would take too long and involve unacceptable costs, in terms of both money and, in the case of structural failure, human life. 
COMPUTER MODELLING AND SIMULATION 
We suggest that the prototyping and feedback expressed in vernacular architecture by actual construction should be replaced by computer modelling and simulation. At present, computer modelling tends to occur after a design is substantially completed, and only minor modifications result. It is unusual to find the 'what if' type of modelling that is common in the field of economics. There are several reasons for this. Inputting the data for a fully designed building is time-consuming and expensive, and the modelling required for environmental evaluation is not yet necessarily compatible with the modelling required for the production of working drawings. Once the model has been loaded into the computer, there are only certain kinds of alteration which can easily be done. Despite the claims of the CAD salesmen, the truth is that it is generally not easy to make changes, at least not of the kind that would help to develop alternative strategies. One possibility would be to have modelling systems which enable some form of evaluation at a very early 'sketch' stage. 
Unfortunately, despite substantial investment, there is still a lack of software capable of doing this, largely because of a misunderstanding about the function of the sketch or design doodle. In addition, the architectural design ethos is unsympathetic to systematic comparative design evaluation and development. Designers tend to trust their intuitive preconception and then modify it until it 'works' -with 'works' often being interpreted in an almost mystical rather than a functional sense. 
It is ironic that the fixed ways of representing and abstracting building form which developed within the limitations of the drawing board and the techniques of projective geometry should have been carried over so directly into the computer. Geometrical forms could have remained plastic and fluid in the computer; instead they have become rigidified. 

For a more flexible alternative approach to computer modelling see J.H. Frazer. 'Plastic Modelling- The Flexible Modelling of the Logic of Structure and Space' in T. Maver and H. Wagter (eds.), CAAD Futures 87. proceedings of the Second International Conference on Computer-Aided Design Futures (Elsevier 1988), pp. 199-208. 
For description of these systems see C.F. Earl. 'Shape Grammars and the Generation of Designs' in J. Rooney and P. Steadman (eds.). Principles of Computer Aided Design (Pitman/Open University 1987). 
W.J. Mitchell. The Logic of Architecture Design, Computation, and Cognition (MIT Press 1990). 
Viollet-le-Duc. Dictionnaire Raisonne de /'Architecture (Paris 18541. 
For a full description of this technique see L March, 'A Boolean Description of a Class of Built Forms' in L. March (ed.). 
The Architecture of Form (Cambridge University Press 1976). 
We need to find an alternative to our drawing board obsession with fixed forms, and it seems that we have to think in terms of language -of a vocabulary or syntax. By this, we do not mean the kind of simplistic approach that is based on the use of large configurable elements and a related shape grammar or spatial syntax. We do not mean the endless permutational exercises much beloved by computer theorists who can produce, for example, every known Palladian plan (plus a few new ones) after devising a so-called Palladian Syntax. 
ITERATIVE ADAPTATION 
We are proposing an alternative methodology whereby the model is adapted iteratively in the computer in response to feedback from the evaluation. In order to make these significant changes to the database, it is essential to have new forms of datastructure with a better understanding of the logical relationships inherent in the building model. I first experimented with a special datastructure with the Reptile system in -1968, but this was specific to the rules and geometry of one structural system. What we are now proposing is a technique applicable to a wide range of architectural concepts and geometries, all conceived as generative systems susceptible to development and evolution, all possessing that quality characterized by Viollet-le-Duc as 'style': 'the manifestation of an ideal established upon a principle'. 
CODING THE DATA 
Once the concept has been described in terms of generative rules, the next step is to code it in genetic terms. The idea of coding can be illustrated by Lionel March's coding of plan schema and overall building massing. This is a more conventional and compact method than ours, but of course March's intentions are also very different. For example, the plan of Le Corbusier's Maison Minimum is coded in hexadecimal representation (that is, counting to the base 16, - 0,1 ,2 ... 9,A,B,C,D,E.F). So the plan is expressed as FF803F71180EFE033F. This can be expanded in binary, where each hex digit is translated to a 4-bit sequence (7 to 0111, E to 1110, etc.). This is then grouped in 9-bit blocks ready to fold into a 9 by 9 array. Each cell of this array then maps to a cell in a Venn diagram (a technique of Boolean algebra for expressing logical operations diagrammatically). The diagram finally undergoes a metric transformation to become the correctly dimensioned plan. In a similar manner, the overall three-dimensional form of Mies van der Rohe's Seagram Building can be coded as 10083EFEOFOO. This may sound complicated, but the coding is very economical and some elements of this system are implicit in the invisible datastructures of any computer modelling system. 

Reptile structural system, structural models: John Frazer, 1968. 
This structural system consists of two unirs based on an octahedral/tetrahedral geometry. It has a folded-plate mechanism which enables it to create large-span enclosures ol complex form. The 'seeds ' are minimal configurations of the units which form a genetic code that can be developed in the computer to create complex structural forms. 
EVOLUTIONARY TECHNIQUES 
In our general model it is necessary to develop this coded version of the concept to address a higher level of complexity. This is done by applying genetic algorithms, classifiers or other evolutionary programming techniques. as described in the previous section. The genetic code is bred into populations which are developed into abstract models suitable for evaluation in a simulated environment. The criteria for selection must be carefully considered and specified. The genetic code of the selected models is then used to breed further populations in a cyclical manner. Abstract models can be externalized for further examination or prototyping at any time. This externalization requires a further step of transformation or mapping. 

'Reptile structural system, interactive computer developments: John Frazer with Richard Parkins, machine-code programmer. and Francisco Guerra. research assistant, 1969 onwards. This research on the seedmg techn1que was camed out m the Cambndge Umverslty mathematical laboratory. The 'seed' of the structural system was descnbed in a genetic computer code. A senes of developmental ins tructions were then provided w hich allowed the seed to be automatically developed and man1pulated in the computer ro produce more complex structural forms. 
See J. Gowan (ed.). Projects: Architectural Assoctat/On 1946-71 (AA Publications 1972), p. 75. 
TRANSFORMING THE OUTPUT 
The model used for evaluation is highly abstract and represents structure, spaces and surfaces in a manner which can only be accessed by the environmental modelling program . Externalizatio~ of the model may include transformations which can affect its dimensions or form in a more complex way than was necessary for the schematic evaluation. A more significant requirement is that of materialization into a buildable medium. This could be achieved by making part of the code simply represent a material and a precise construction technique, as we did with the model for Walter Segal described earlier. However we are inclined to think that this final transformation should be process-driven, and that one should code not the form but rather precise instructions for the formative process. 
CODING: THE EXAMPLE OF THE REPTILE SVSTEM 
In 1966 I started developing a flexible enclosure system which consisted of just two folded-plate structural elements that could be combined to form a range of structures in a large number of shapes and sizes. I called it the Reptile (rep-tile) system because it consisted of repeated units. The dinosaur metaphor also seemed apt for a component-based approach that I anticipated would soon be obsolete. Already I was looking for a more process-oriented and biological approach, and when the project was published in Architectural Design in 1974 included a drawing of the structure being dismantled and replaced with a more biomorphic form. The accompanying caption said: 'The association with an obsolete species is intended to emphasize that such a ;Component approach to architecture is probably only of transient significance.' 

Reptile structural system, computer drawings: John Frazer, 1968 onwards. The coded 'seed' of the structural system is automatically developed and manipulated in the computer to produce complex structural forms. The design of this structural enclosure was developed from the seed with a minimum of intervention. 
Two structural units. Coded spatial orientations of units. 
'Reptiles', Architectural Design, April1974, pp. 231-9. 
The first of many computer programs associated with the project was written at the Architectural Association in 1967/68. This was able to produce perspective output of structures that I had designed- but only after the coordinates of each unit had been painstakingly digitized. 
I started to search for a technique of minimizing the data input, and thus conceived the ancestor of all subsequent developments. In 1969, with access to the then massive computing power of the Atlas Titan in Cambridge University mathematics laboratory, I developed a seeding technique that allowed a densely coded description of a minimal configuration of the units to be developed and manipulated into a complex structural form without inputting any further data. 
The two structural units of folded-plate construction could be orientated in eighteen different ways relative to each other, resulting in over three hundred useful combinations. These combinations allowed the sy-stem to produce enclosures in a wide variety of plan shapes and complex structural forms. And because the units could be combined to form a straight edge allowing simple rectangular openings, the system was entirely compatible with traditional rectangular buildings. There was no need for the kind of special variants or cut units which had limited geodesic structures, for example, to dome-like shapes. 
For a full description see J.H . Frazer and J.M. Connor. 'A Conceptual Seeding 
Technique for Architectural Design', PArC 79. proceedings of the International Conference on the Application of Computers in 
Architectural Design (Online Conferences with AMK, 1979). pp. 425-34 . 
The machine-code programming was done in the Cambridge University mathematical laboratory by Richard Parkins with the encouragement of Charles Lang. 
The FORTRAN programs were written by John Frazer and research student 
Francisco Guerra. 

The units were located by 60° axes using integer coordinates (A,B,C). 'B' coordinates had a negative direction, not positive as one would expect, in order to make 'C' coordinates (which were not stored) easy to find (A+B+C=O), and to provide a check when inputting data manually. 'D' coordinates described depth or level of the unit in the structure. The type and orientation of a unit was described by two digits, the first giving the type and vertical orientation, the second the orientation in the horizontal plane (T'T"). This whole description of the unit in space took the form (A,B,C,D,T'T"), for example -66,32,34,21, . and was originally packed (using machine-code instructions) into one word of the Cambridge Atlas Titan with a pointer to the next unit in the chain. The units were chained clockwise by levels from the top of a structure downwards, with information about the levels being stored at the beginning of dare referring to that level. 
The basic datastructure was set up and manipulated by a series of machine-code sub-routines and functions. These allowed the descriptions of the units to be retrieved, deleted or updated, and additional units to be inserted into the chain. On output the integer location on the 60° grid was translated into normal orthogonal coordinates by a Sl.lb-routine which contained a graphic description of the. unit, its type and orientation. A variety of descriptions were available depending on the detail required in the final drawing. The GINO graphics package was used to produce perspectives and other drawings as requi_r:ed. 
The seed was a minimal closed configuration which included units in all possible orientations but not necessarily all possible combinations. The deve~op ment of a structure was initialized by implanting- the datastructure with a description of the units making up the selected seeds. The units were chained from the top of the seed downwards in a clockwise direction. 
Structure of the program. Automatic cutting and stretching. The two seeds. The two seeds starting their growth. 
For further theoretical background see F. Guerra, 'The Use of Random in Architectural Design', Bulletin of Computer Aided Architectural Design, No. 14, January 1974. and 'Hypothetico-Deductive Model for Design', Architectural Design, April 1974. 
This initial seeded datastructure was then manipulated by a series of FORTRAN sub-routines enabling the seed to be grown, stretched and sheared until the required building form was produced. There were twelve possible routines for developing the datastructure in this way, and these could be further combined to produce complex hybrids. 
For example, to stretch a piece of structure a cut line was established. The datachain was then searched level by level until a unit on the cut line was found. The 60° orientation of the axes meant that this search was exceptionally simple: hence also the requirement for the apparently redundant 'C' coordinate. If traditional orthogonal coordinates had been used, a separate computation would have been required to establish on which side of an inclined cut line each unit in the datachain lay. Instead, it was po~sible to determine the orientation and location of the new units needed to complete the structure just from the location, type and orientation of a starting unit and those units adjacent to it in the chain. These new units were inserted in the chain using the appropriate machine code sub-routine and the subsequent units in the chains were moved to their new position in the structure by using the machine-code updating sub-routine. 
The particular configuration of the units in the seed had a significant effect on the final overall building form. The information about the type and orientation of the starting unit and those adjacent to it "f'aried from seed to seed, affecting the choice of procedure for infilling units in the data chain. Diff~rent seeds could be used to produce identical plan arrangements, b_ut not identical formal and aesthetic effects. 
In this early and restricted version of the program only two seeds were ever used: the knot (containing forty two units), and the star (containing seventy-two units). 
J.H. Frazer and J.M . Connor, 'A Conceptual Seeding Technique for Architectural Design', PArC 79, p. 425-34. (See note on p. 72.) 
GENERALIZING THE APPROACH 
Soon after developing the Reptile system we started to investigate the possibilities of generalizing the process. 
A paper given at the 'Application of Computers in Architecture, Building Design and Urban Planning' conference in Berlin outlined some of the problems with a conventional approach to using computers in the design process, introduced the Reptile program, and described the theoretical framework for a generalized proposal. 
The program for the generalized proposal requires two kinds of information; the conceptual model of the building information in its minimal coded configuration, and a description of the actual components and details for the output stage. 
Here, the concept of cultivating the seed to produce different buildings is extended, as details of the seed are mutated to produce variants which can overcome the great increase in environmental variety encountered , with general-purpose building construction. Individual mutations of the assemblies can be developed interactively and stored as variants which are referred to by an additional set of digits in the item description in the datachain. 
As with the Reptile program, all the necessary information for the creation of new items is derived from the type, location and orientation of the item in the see~ and those adjacent to it at the point where a move or change is to be made. An important feature of the program is that it only takes account of dimensional coordination if this is essential to the concept. It does not rely on modular coordination or a grid but computes the distances between locating points of items in the chain in order to determine modules appropriate to the particular construction. 

In operation the configuration of the minimal construction or seed is compared with the user's requirements for a particular building, and the seed is grown, stretched and pruned until it conforms to these require ments. Many forms of building may equally fulfil a particular s~t of building requirements and the same form of building can be. produced by several different cultivation routines. The seed is compared with the brief for the building and the datastructure is modified in two ways. First, the quantifiable and specific parts of the brief are dealt with in the form of optimization routines. Alternative strategies for cultivating the seed are automatically evaluated and the program adjusts itself on a simple heuristic basis to adopt those tactics which have proved most successful in previous attempts. Second, the user evaluates a series of solutions produced by the first technique in terms of non-quantifiable criteria, including aesthetic judgements. 
The program can either learn to adapt to particular aesthetic prejudices, or it can classify solutions in terms of formal or aesthetic characteristics which can then be requested by the user. 

AA Diploma Unit 11 1991/92, illustrated in Projects Review 1991/92. 
D. E. Goldberg, Genetic Algorithms in Search, Optimisation and Machine Learning (Addison-Wesley 1989). 
ENVIRONMENTAL FEEDBACK: THE UNIVERSAL INTERACTOR 
In 1991/92 we experimented with the other major aspect of the problem: the use of environmental factors to affect the development of the genetic code and the selection of successful mutants. In order to provide this environmental interaction we constructed a series of experimental antennae which were either transmitters or receivers of information.
The receivers responded to movement, sound, colour; they included piezo grass for sensing wind patterns, finger jewellery and suits for sensing touch and body movement, as well aS video systems for detecting cultural patterns. The transmitters sent out sound, light and movement. Moving antennae of various forms gave an indication of the emotional state of the system, and there were a variety of devices which encouraged participation and experimented with conflict and cooperation. 
We built a special-purpose Universal lnteractor with three racks for standard circuit boards. Each transmitter or receiver had one or more controlling cards in the rack. Buffered connections from the rack to a controlling computer enabled one or more of the antennae to be activated at any moment. The data from each antenna was converted to digital form, allowing all the devices to communicate equally. The environmental information was fed to a simplified environmental model which formulated responses based on an evolutionary algorithm. 
THE EVOLUTIONARY ALGORITHM 
The main structure of the evolutionary program was based on a simple classifier defined by Goldberg. It had five elements: an environmental system, a developmental system, an evaluation module,. a genetic algorithm, and a graphic output mapping procedure. The developmental system reacts to signals from the environment and the genetic code of each individual, sends messages to the environment of the proposed. environmental response, and provides the data for the genetic algorithm to make selections. 
78 SECTION 2 
Here the datastructure is based on a direct analogy with DNA, where each amino-acid is represented by three contiguous bases in the DNA sequence, known as a codon. Each codon and each triplet is represented in binary form (with U,C,A & G represented by 00,01,10 and 11 respectively). Thus the codon UUU is represented by 0000000 and GGG as 111111, yielding the necessary sixty-four possible permutations. The environmental datastructure is similar in form and is derived by measuring the environmental properties on a cube. The cube has eight vertices, and for the demonstration, three environmental factors were measured, each in turn represented by six bits like the codon. In this way the environmental messages took the same overall form of an 8 x 3 x 6 array of trits {0, 1 or wildcard%). 
The simple model is thus a cube representing the site of the project: the three outer orbits are the environmental factors, and the three inner orbits are the responses of the system. The environmental factors - sound, wind and sun - are operated by signals from the Universal lnteractor by random, chaotic and periodic functions respectively. The intensity of each factor is calculated at the vertices of the cubes, coded, and then passed to the developmental section. 
In the developmental section the environmental information flows through the detectors and is decoded into appropriate finite length messages. The classifier system then activates string rules controlling output through the effectors. In this way, it combines environmental cues and internal rules to determine the environmental response. Credit for successful responses is apportioned by means of a complicated auction technique derived from John Holland. This auction results in a proportional evaluation which is then passed to the genetic algorithm to select cells for the next stage. 
In order to simulate the phenotypic expression of the genel-lc code, each cell is allowed a development environment consisting of volume and movement. Changes in the cell's local context are represented graphically by depicting the process of cell division (gastrulation) Orl the surface of a segmented sphere. Successful response is represented by cell growth in this segment and manifested by an asymmetrical development from the spherical shape. The results can be fed back to the transmitter antennae for broadcast to the environment. 

Genetic algorithm: tchiro Nagasaka, 1992. 
The classifier system responds to a set of environmental inputs and evaluates the relative success of that response. Environmental signals can be taken from any of the antennae communicating with the Universallnteractor and a response transmitted to the output antennae. The nature of the response is based on feedback from the environment and more successful responses are gradually developed. 
Above and previous page: 
celestial dome: Sichin Lin, 1991. 
The celestial dome shows the movement and interaction of the sun and moon. It consists of an acrylic hemisphere with lines of 
tricolour light-emitting diodes arranged to indicate the seasonal and hourly movement of the planets. It attempts to engage 
simultaneously both hemispheres of the brain by simulating both the direct visual geocentric experience of an observer and the intellectual understanding of that experience. Standing 
inside the dome gives a simulation of the path of the sun (and moon), whilst standing outside gives a sun's eye view or cosmo logist's understanding of the orbits of the eanh (and moon). Ths dams also shows eclipses by the third colour of the diodes. The system could be used practically as a sunlight and shadow prsdictor: an associated computer animation sequence shows a daily and seasonal cycle of light and shadows for a building. 
OUT OF THE DINOSAUR PERIOD 
Many of the key ideas for our present model were already present in the Reptile pro-gram: the idea of a conceptual seed which could be manipulated into an endless variety of forms and sizes; the dense coding in abstract terms and the representation of the code in a word; the manipulative and developmental routines; the mapping at the output stage to a full graphic representation. The selected datastructure is very close to our preferred option for the present system. Even the use of an evolutionary program was hypothesized but not implemented. 
The Reptile example demonstrated the feasibility of the technique using very limited computer resources. The drawback of the system was essentially that it was limited to the geometry of the specific components on which it was based. It was literally a three-dimensional form of repeated configurational program. It was also limited to producing single enclosure forms, albeit of a complex folded kind. 
Similarly, the Universal lnteractor and associated genetic algorithm demonstrated that it was possible in principle to evolve an appropriate response to environmental stimuli. 
We believe that these experiments demonstrate the overall feasibility of our theoretical framework. The next step is a complete coherent model capable of evolving and responding to the necessary degree of complexity. 
Emergent surface: Helene Stub, 1992. A three-dimensional surface emerges as a result of a series of catastrophes on a recursive two-dimensional program. 
Isometric- having equal measure. Isomeric- having identical pans. Isomorphic- having identical forms. Isotropic- equal in all directions. 
PART II: THE SPECIFIC MODEL 
While just one means of trying to achieve an evolutionary architecture is described here, our model leaves room for a great range of experiments -a vast architectural genetic search space. It consists of an endless array of data points which collectively constitute a dataspace. Each point in the dataspace is intelligent in the sense that it knows why it is there and where it is, with a clear awareness also of the spatial relationship of its neighbours. The laws of symmetry and symmetry-breaking are used to control the development of the model from the genetic code. Information flow through the model takes the form of logic fields. 
We have organized our model using a multiple hierarchical approach and a datastructure which is recursively self similar. The simulated environment in which evaluation takes place is modelled in exactly the same terms as the evolving structures. The environment and the structure not only evolve in the same dataspace, but can co-evolve, and competitive structures may also evolve il) the same space. The environment, which in this case includes userresponse, is modelled with virtual societies. The environment has a significant effect on the epigenetic development of the seed. Epigenetic algorithms- a modified form of classifier algorithm - are used to perform the selection: normal crossover and mutation are used to breed the populations. 
Externalization of this datastructure is driven by modelling the process of form-generation rather than the forms themselves. The tendency of certain clusters of solutions to occur in the genetic phase space indicates that various architectural types can be identified with certain strange attractors. Only the whole model knows about the whole. And the whole is a hierarchical organization of nested automata roaming freely in virtual logic space; concepts purely in the imagination of the computer. 
UNIVERSAL STATE SPACE MODELLER 
The first step was to establish a suitable computer environment for our model. We call this architectural genetic search space a 'Universal State Space Modeller' or 'isospatial model': 'universal' because it can model any space or structure or environment equally; 'isospatial' because it is isometric, isomeric, isomorphic and isotropic. The geometry of the space is also recursively selfsimilar and can be termed 'homospatial', as it is a fractal, and therefore scaleless, space. This recursive property allows hierarchies of complexity to be described in the same logical space. 

Datastructure diagrams: Gary Kong. 
J.H. Frazer, 'Datastructures for Rule-Based and Genetic Design', Visual Computing: Integrating Computer Graphics with Computer Vision !Springer-Verlag 19921. 
Other properties are mafenotipde evident by the name of the model. We use the term 'state space' because each logical point in the space can exist in many different states; each point in the state space corresponds to a virtual state of a system being modelled. 
The modelling is in the sense of a virtual computer modeller capable of processing a vast variety of evolutionary systems and strategies. But we have also built a physical model, to assist comprehension and visualization - in this case, by externalizing the computer model. 
THE DATASTRUCTURE 
The datastructure consists of a field of dimensionless logical points in a regular geometrical array. Each point, or mote, carries with it information about its location and its relations to other motes. It also has details on its own properties, including a parameterized geometrical description of any form to which it might map and, most importantly, a history of how it came to be where and what it is. Lastly, it carries a set of rules concerned with why it is where and what it is. The mote itself do not move, but its identity and properties may do so in response to some transformational activity.
The field of motes extends indefinitely and the motes which surround a particular object (a structure) have properties which can affect the environment. This means that the behaviour and performance of such a structure can be modelled in a particular environment. More significantly, the form of the structure may respond to an environment in a manner analogous to a genotype producing a phenotypic reaction. The rules of the mote may also be allowed to evolve in response to many iterative interactions with the environment. 
The mote is a minute logical particle in a regular geometrical configur ation. It is possible to have an orthogonal array: this has the advantage of easy representation and visualization,
as it can be considered as an array of voxels or cells in a spatial enumeration system. This is also its main disadvantage, giving rise to misunderstandings about the nature of the mote and also appearing to have a geometrical bias and non-equivalent neighbours.
(A ceU in a cubic array has six face-neighbours, twelve edge-neighbours and eight vertex-neighbours. They are of three different spatial and geometric relationships and three different centre distances.) Moreover, twenty-six is a lot of neighbours to talk tol This problem can be partly avoided by considering face-neighbours only, and by communicating with edge- neighbours via a face-neighbour, and with vertex neighbours via a face- neighbour and its face-neighbour. But this route of communication is complex and best avoided. 
Symmetry-breaking: Elena Cecchi, 1994. The use of reflections to explore symmetry breaking in cellular automata. 
If the system is considered as hexagonal layers. a coordinate system based on four axes at mutual angles of 109° 2B' is most 
convenient. The location of any point in the spatial array can then be given by the intersection of the four planes perpendicular to the axes. Labelling the positive coordinate directions as those most mutually opposed produces the useful relationship that the sum of the four coordinates of a point is always zero (i.e. if the four axes are labelled p,q,r,s, then P+Q+r+s=O). Thus only the most convenient three axes need be used for a particular point. or all four can be used and the relationship used as a check. In the case of the cubic orientation, a more normal three coordinate cartesian x,y,z system will suffice but is less elegant. 

An alternative (and preferable) geometry uses the centres of close packing spheres (cubic face-centred packing). This gives each mote twelve neighbours of identical geometric conformation and equal centre to-centre distance. The reduction in number, equidistance between centres and ease of identification in the database all provide significant advantages for certain applications. The close-packing configuration can not only be orientated as a whole to allow easy visualization and assimilation of orthogonal structures, but also orientated with closest packing layers horizontal. Furthermore, the entire array is recursively subdividable. A tetrahedron defined by a configuration of points in the array can be repeatedly subdivided by any convenient multiple as a self similar array. This fractal characteristic produces significant data storage economies, but more importantly allows the model to work at several scales at the same time. 
Each mote in the datastructure carries complete copies of the architectural g~netic code and of all instructions needed for the generation of forms of a particular species, although for programming economy these are not actually stored more than once. The important point is that there is no single master mote with all the key instructions, no single master controlling program outside the datastructure. The datastructure of motes is the program in the sense that only the whole knows about the whole. Specific in.stances are manifestations of a response to out side stimuli or environment. All of this is broadly anaiOQOUS to each cell in the human body having a complete copy of the DNA and producing specific forms (phenotypic reactions) in response to a particular en vi ron ment. 
Each mote is aware of the identity of its neighbours and of its geometrical relationship to them. It also knows its own identity, which might include properties. It understands the history, method and reason for its genesis, as mentioned at the beginning of this section. It is thus able to reproduce the development of a specific form. If part of a form changes, it is able to know if it must also change. This occurs by passing messages from neighbour to neighbour, agreeing on a course of action, and then moving synchronously. 
The mote is like a cellular automaton in the sense that its state at time 't' is dependent on transition rules and the state and of it and its neighbours 
Information transmission by fault replication: Nicola Lefever, 1992. 
The exploracion of fault replication is extended co three-dimensional faufcs. which, it is discovered. not only replicate but develop in complexity. It is proposed rhat complex structtJral forms could be developed from information. in the form of faults in the material of the structure Itself. 
at time 't-1 '. In the case of the mote, these neighbours may include motes which are external to the structure (i.e. environment) or very remote. 
Broadly speaking, information travels through the array in the form of logic fields. This means that the form as a whole can evolve in response to the environment.It also means that the system as a whole can learn which rules are successful in developing and modifying form and so evolve new rules for improving its form-making ability.
Usually this occurs in conceptual form, generating a computer model which may then be manufactured as a frozen moment in the evolutionary process. In special cases, the system may go on to exhibit this behaviour after manufacture. In this instance, one could describe not only the conceptual model but also the artefact or structure as 'intelligent'. Furthermore, since the whole system controls its organization in the manner of a three dimensional cellular automaton, it could be thought of as being a computer (at least in the sense of a finite state machine). 
THE HIERARCHY OF THE DATA 
The datastructure is also hierarchical in the isospatial array of logical units. The most elementary logical unit is defined as the 'moron'. A moron can be 1 or 0, true or false, black or white, alive or dead. Morons are grouped together in spatial configurations: their spatial disposition and logical states are interpreted as being the state of a mote- the next logical unit in the hierarchy. Complex spatial assemblies of motes are in turn read as the genetic code-script of a seed. 
Hierarchical cellular automata: 
Manit Rastogi, 1994. 
Two two-dimensional cellular automata are used to dnve two three-dimensional cellular automata. One controls the evolvmg structure. he other the environment. The inceraction between the two produces he surface forms. 
A seed is selected for fertilization and genetic crossover occurs (as part of the genetic code-script is exchanged between two seeds). The seed then develops by successive cellular duplication in accordance with the rules encoded in the genetic script. The spatial configuration of the expanding array of cells is determined by symmetry-breaking, which responds at first strictly to the code-script and then increasingly to the specific environment as interpreted by the code-script. 
Information is transmitted through the array by logic fields which radiate concentrically from an active cell. The manner of broadcasting is not dissimilar to the way in which a mobile phone is woken up if there is a message for it in this case, however, the message may be for a number of cells. As there is no one cell in charge, any emergent global behaviour is solely the result of the application of local rules.
In this case, 'local' means coming from one cell. Although each cell communicates only with its twelve immediate neighbours, it may broadcast a message for a distant cell, even though it has no spatial awareness of its location. 

Individual cells gradually take on specialist functions.
They may adopt the role of receptor, to interpret information from the environment; or process-driven producer, to make virtual material for the construction of the virtual model; or assembler, to build the virtual model. The virtual model is also inevitably built hierarchically. The outcome is a virtual model designed to test and evaluate in the simulated environment. 
Thus the mapping process which leads from the code-script to the virtual model is essentially process-driven. 
It no longer presupposes a set of components (as the Reptile system does) but a generative set of processes capable of producing the emergent forms, structures and spaces of the model as a by-product of the cellular activity. 
What you see of a tree, or of a person for that matter, is the phenotypic by-product, the residue of cellular activity. 
In several senses coded messages can also be considered as models. They can be visualized as physical spatial models in the same way that molecular models can be constructed of the code-script of DNA. As Eigen has pointed out, there is a close etymological relationship between the concept of information and form; furthermore, information can be understood as the interpretation of a form into abstract symbols.
Much of the time we have indeed been modelling at this level of abstraction, but in this context, when we use the term 'virtual model', we mean computer model. Our model has the potential for greater proximity to buildable forms than most current computer models since it also understands the process of manufacture and construction. 
Nicola Lefever, 1992. 
Unpacking the datastructure to read the binary coded instructions. 
M. Eigen and R. Winkler, Laws of rhe Game: How rhe Principles of Narure Govern Chance (English translation, Penguin 1982). Interstitial spaces: Gary Kong, 1994. 
An investigation of the connective, or interstitial. spaces between spheres in different close packing configurations. These spaces are represented as elements of rwo kinds. generated by the tetrahedra/and octahedral configurations of the spheres. The mapping of the spaces is seen as an alternative to mapping directly from the spheres or cells. 
Evolution of form described by equations: John Frazer with Peter Graham, research assistant, 1992. 
A genetic algorithm is used to control the evolution of three-dimensional shapes generated by equations. This example explores small variations in a mathematical expression. 
As part of our experiment a series of such virtual models has been constructed, some from the same seed by varying aspects of the environment such as user-response, some from different seed stock. The performance of each model is carefully evaluated and the seed stock of the most successful models is preserved and the process repeated. 
Selection, crossover, mutation, etc. are controlled by a hierarchy of nested classifier systems, itself controlled by modified form of genetic algorithm (see the section on new tools). The rules of a normal classifier system are logical, matching stimulus to action. However, in our case, what actually occurs is more complex and closer to the transition rules of a hierarchy of nested cellular automata: the effect is one of environment response to stimulus. The response evaluated for the genetic algorithm therefore determines the emergent properties of the system as a whole. 
The environment in which the cellular development takes place is modelled in the same datastructure as the cells. That is to say, the environmental components, including site, climate, users and cultural context, are all viewed as being logical states in isospace and are modelled in the same manner.
Dynamic systems such as turbulent air movement can also be modelled in the same space by dividing the coding of the space state into a state and a vector. Based on a quantum continuum, this model raises the possibility of co-evolving some aspects of the environment, leading to a Gaia-like interdependence. 
THE GENETIC LANGUAGE 
This hierarchical structure provides the syntax of our genetic language. It starts with the binary cod ls and works up to the full code script with all the expected requirements, such as symbols, words and syntax. Much of this is like any other computer language and our in house languages (such as SPL and GLA) are ultimately transcribed to computer machine-code (or more recently into higher level languages) for convenience. Developing this language is our current area of concentration. 
THE COMPUTER 
Depending on the resources available, there are several possible computer implementations. The ideal machine is one with an isomorphism between the structure of the processor and the structure of the data. The elegant way to achieve this is to arrange tetrahedrally disposed parallel processors to represent a sparse version of our data array. For example, Transputers with four channels of communication to the four adjacent nodes would be an ideal configuration. Since the datastructure has a self-similar nature there is no need for every mote to have a processor, but this would be the long-term goal. The Connection Machine described in the previous section on the artificial model has more than 64,000 processors and designs for massively parallel fine grain machines suggest that 16 million processors would be feasible. Any parallel machine allows the possibility of simultaneous logic fields to flow through the system: this again has the elegance of form seen in the data transfer since it is isomorphic with the model. Even in less powerful computer environments, the concept of treating each mote as object-oriented code is very attractive.
as this maintains the close relationship between the model and the operation of the code, combining data and program in the same form and space. 
 
Spatial description of movement 
J u i-Pang Tseng, 1993. 
The articulation of the human bodv is replicated bv a multi-jointed mechanism with variable resistors at each Joint. The position of the '!1mbs of the mechanism can be interpreted bv the linked computer. and spatial envelopes can be described in relation to body movements. 
At the other extreme, the minimum computing requirement is a common program which memorizes the state of every logical unit in the system and allows the whole to behave as a cellular automaton, having genetic algorithms available to control both starting configurations (seeds) and transition rules. The program must also allow for both natural and artificial selection and for coding, mapping, translation and projection to some form of graphic (or audio) output. 
The individual programs require a statement of starting conditions, an interpretation of the states of the model, the transition rules, the basis for selection, and instructions for coding, mapping, translation and projection. 
Emergent artificial life- evolving the rules: John Frazer with Peter Graham, research assistant, 1994. 
This experiment at the University of Ulscer attempted to evolve artificial life out of nothing. with neither rules nor seed being given in the first insrance. We started with a spherical close-packing dataspace, so that each cell had twelve equally spaced 
neighbours, and then used generic a/gomhms to select and develop successful rules and seeds. In this case success was the ability to survive and develop complex structures. 
CRITERIA FOR SUCCESS 
In order that natural selection should work, certain criteria must be satisfied. 
• The genetic information must replicate accurately. 
There must be an opportunity to generate variety and mutation (usually achieved by genetic crossover and very small random errors in the genetic copying). 
• Any variation must also be capable of reproduction, and must occasionally confer potential advantage when expressed as a phenotype. 
• There must be massive overproduction of phenotypes. 
• There must be selective competition in particular environments (before replication of the genetic code). 

To satisfy the laws of natural evolution, there is no need to have a living organism. All the criteria for success present in a natural evolving system are mirrored in our artificial evolutionary model. Genetic information in the form of computer code is reproduced by the equivalents to cause it to crossover and mutate. Phenotypes in the form of virtual models are developed in simulated environments, performances are compared, and a selection of appropriate genetic code is made and then replicated in a cyclical manner. 
Architectural genetic structure: 
Sichin Lin, 1993. 
A materialization of the arc!Jitecrural equivalent of the code-script of the chromosomal fibre of nature. The model displays in a phylogenie manner all the aspects of its ontogenetic development. including cellular division, replication. mutation and environmental response. A complex aperiodic crystal structure evolved from the simplest periodic crystals, it mirrors the possible development of biological life. 
99 
It has been emphasized above that , including instructions for making all the materials, then processing and assembling them. This includes making enzymes for the production of nucleotides. plus instructions for cell division and differentiation. These are all responsive to tt:le environment as it proceeds, capable of modifying in response to conditions such as the availability of foodstuffs, and so on. Genes are not for shapes but for chemistry, and by analogy our model also describes process rather than form.
This procedure is environmentally sensitive. The rules are constant, but the outcome varies according to materials or environmental conditions. Eventually it is our intention that the form-making process will be part of the system, but for the moment our model works by describing the process of processing and assembling the materials. The actual processing and assembly is external to the model. 
Simply stated what we are evolving are the rules for generating form, rather than the forms themselves. 
We are describing processes, not components; ours is the packet-of-seeds as opposed to the bag-of-bricks approach.
Generation 7. 
This first attempt to evolve both the seed and the rules stabilized after seventeen generations . 
Generation 8. Generation 70. 
THE EXTENDED ARCHITECT 
The approach so far described implies some changes in architects' working methods. The generic approach already adopted by many designers has to be made explicit, rigorous, and stated in terms which enable a concept to be expressed in genetic code. Ideally this information could be deduced by the computer from normal work methods without any conscious change being necessary. Architects have to be very clear about the criteria for evaluating an idea and prepared to accept the concept of client- and user-participation in the process.
The design responsibility changes to one of overall concept and embedded detail, but not individual manifestation. Overall the role of the architect is enhanced rather than diminished, as it becomes possible to seed far more generations of new designs than could be individually supervised, and to achieve a level of sophistication and complexity far beyond the economics of normal office practice. The obvious corollary of this is a diminished need for architects in the process of initial generation. While there would still need to be enough architects to guarantee a rich genetic pool of ideas, the role of the mass of imitators would be more efficiently accomplished by the machine. In this new context architects might have a role closer in concept to that of an extended phenotype, and I thus suggest the designation 'extended architect'. 
Generation 12. 
Frorn S. Jones, The Language of the Genes (Harper Collins 1993) 
101 
Generation 7 5. Generation 17. 
THE SORCERER'S APPRENTICE 
But perhaps we could go further. The next logical evolutionary step would be to evolve the seeds as well. Would this be possible? Could one set In motion a process of creation which required massive feedback but only one seed - Darwinian evolutionary theory, after all, implies one common root for all species. Perhaps we do not even need one seed? Could we evolve everything from a form of primeval data-soup, evolving not just the rules but even the starting configuration? Could it be possible to co-evolve the very environment in which the epigenetic development will occur? 
This is what we are attempting to do at the moment: to evolve architectural life from nothing, with no preconceptions, with no design… just blind tactics.' 
Natural selection has superb tactics, but no strategy - but tactics, if pursued without thought for the cost and for long enough, can get to places which no strategist would dream of.' 

THE NEXT STAGE 
The present five-year project followed years of background research dedicated to formulating the general theory, designing tools and testing parts of the idea. In the course of recent work we have produced further tools and experiments, but our main objective has been to develop a coherent theoretical model, which we believe we have achieved. 
Our immediate concern is now to develop this model further; to incorporate into it all the processorientated information generated by our previous work both at the Architectural Association and the Technical Research Division at Cambridge University. This will enable us to externalize the theoretical model in terms of specific building propositions which can be tested, evaluated and criticized.
The environmental model needs to be developed to provide a complete model for evaluation and testing. Most importantly, we need to test the conceptual model itself by inputting seeds and testing the outcome in specific situations. 


R. Dawkins, The Extended Phenotype (Oxford University Press 1982). 
C.G. Langton (ed.l, 'Artificial Life, 
Proceedings of Interdisciplinary Workshop on the Synthesis and Simulation of Living Systems, September 1987' in Santa Fe Institute Studies in the Science of 
Complexity, Vol. VI. 1989. 
W. Katavolos. 'Organics. in U. Conrads (ed.). Programmes and Manifestoes on 20th century Architecture (Lund Humphries 197m 
C. Jencks, Architecture 2000: Predictions and Methods (Studio Vista 19711. 
P. Cook (ed.), Archigram 9, 1970. 
A.G. Cairns-Smith. Genetic Takeover and the Mineral Origins of Life (Cambridge University Press 1982). 
Our longerterm goal lies in trying to incorporate the building process literally into the model, or perhaps the model into the very materials for building, so that the resultant structures are selfconstructing.
This may be achievable by either molecular engineering, by the application of nanotechnology, or perhaps by the genetic engineering of plant forms or living  organisms to produce forms appropriate for human habitation as an extended phenotype. Frei Otto has suggested growing structures, Doernach and Katavolos imagined organic structures erected from chemical reactions. Alvy Ray Smith envisaged buildings growing from single brickeggs. Charles Jencks referred to scenes from 'Barbarella' showing the emergence of human and vegetable forms. The final issue of the Archigram magazine contained a packet of seeds from David Greene. In the short term, the prospect of growing buildings seems unlikely, but selfassembly may be achievable. 
More practically we expect in the meantime to proceed with implementation in a hierarchical and evolutionary manner by developing multi functioning organic wholes from single-function cells.
We envisage a form of genetic takeover much as CairnsSmith envisaged the takeover of mineral replicators by progressively more sophisticated developments.
The dinosaur of the Reptile system is evolving into more complex organisms. 
CHARACTERISTICS OF THE NEW ARCHITECTURE 
In the systems that we have been discussing, global behaviour is an emergent property often unpredicated by local rules. In the same way, the emergent architecture will also be unpredicated.

It is tempting to show examples or simulations of what the new architecture might be like, but the emphasis at this stage must be on process, in order to maintain the universality of the model. The model itself, together with its evolutionary and descriptive processes, will result in a process-driven architecture. 
Our architecture is a property of the process of organizing matter rather than a property of the matter thus organized.
Our model is, at any given time, the expression of an equilibrium between the endogenous development of the architectural concept and the exogenous influences exerted by the environment. 

This is a paraphrase adapted to an architectural concept: S. Goonatilake, The Evolution of Information: Lineages in Gene. Culture and Artefact {Pinter Press 1991). 

These last two lines are based on a description of life by S.A. Kauffman, The Origins of Order: Self-Organization and Selection in Evolution IOxford University Press 1993). 

EVOLUTIONARY ARCHITECTURE 
An evolutionary architecture will exhibit metabolism. It will enjoy a thermodynamically open relationship with the environment in both a metabolic and a socio-economic sense.
It will maintain stability with the environment by negative feedback interactions and promote evolution in its employment of positive feedback. 
It will conserve information while using the processes of autopoiesis, autocatalysis and emergent, behaviour to generate new forms and structures. 
It will be involved with readjusting points of disjuncture in the socio-economic system by the operation of positive feedback.
This will result in significant technological advances in our ability to intervene in the environment. 
Not a static picture of being, but a dynamic picture of becoming and unfolding - a direct analogy with a description of the natural world. 
A FORM OF ARTIFICIAL LIFE 1
Our model will derive order from its environment and be controlled by a symbiotic relationship with its inhabitants and that environment.
It knows the coded instructions for its own development and is thus, in a limited sense, conscious. It can anticipate the outcome of its actions and therefore can be said to have some intelligence. All the parts of the model cooperate and in that sense it can be considered as an organism, but it will only fully exist as such if it is a member of an evolving system of organisms interacting with each other as well as with the environment.
Our new architecture will emerge on the very edge of chaos, where all living things emerge, and it will inevitably share some characteristics of primitive life forms. And from this chaos will emerge order: order not particular, peculiar, odd or contrived, but order generic, typical, natural, fundamental and inevitable- the order of life. 

POSTSCRIPT - TIM JACHNA
One requirement for an Architectural Association Diploma is the completion of a General Studies essay investigating in depth a theoretical or historical architectural problem. In Diploma Unit 7 7 almost all of these studies are closely related to unit work, and describe the theory and philosophy underlying the student's research. The following text is extracted from Tim Jachna's 1993 General Studies essay. 
INTRODUCTION: DESIGN AND THE MODEL 
A broad distinction may be drawn between a 'primitive' culture, in which the act of building is characterized by direct, habitual responses to 'misfits' which arise in an otherwise static cultural milieu, and a culture, such as our own, in which the meteoric pace of change and the immense complexity of contextual forces engender misfits with such frequency, and of such variety, that the habitual adaptive responses of individuals are ineffectual in remedying them. 

As tradition and instinct have become insufficient guarantors of 'fit', form-makers must now formulate systems of explicit rules for action.

This is not to say that the habitual process operates in the absence of rules.
On the contrary, it embodies rules of great complexity, but these fall under the categories of tradition and reflex, and are unformulated and inseparable from the greater cultural context and human instinct. 
In contrast, modern thought is characterized by the belief in a principle of dualism: the Cartesian division of all existence into the two exclusive worlds of material entries (res extensa) and mental concepts (res cogitans). The pace, scale and dualistic predilection of modern culture are severing access to reservoirs of embedded wisdom and confronting the conscious mind with the full complexity of the building process. 

Rules (or general principles) are combined into 'theories'. A theory is formed by subdividing a problem into pieces small enough to be analysed individually and then re-combining the fragments. 
In a 'dualistic world, the mind has no access to the deep structure of the physical world; by the Cartesian model, such access would only be misleading as the true nature of things is to be found solely in abstract logic.
By nature, then, the product of the rigorous application of such a theory in the field of architecture is never a building -which is a thing of the world and therefore different in kind to any theoretical system- but a 'design'. 

I. Prigogine and I. Stengers. Order Out of Chaos: Man's New Dialogue with Nature (New York 19B4). pp. xx-xxi. 

H. Pagels, The Dreams of Reason: The Computer and the Rise of the Sciences of Complexiry, pp. 74~. 

A design is an abstraction. The historical emergence of the architect is concomitant with the birth of 'design' within the building process. A design is a construct conceived completely in the mind.
Implicit in any design is a 'guess' at the nature of the world that will be encountered by the designed object: a set of tentative boundary conditions analogous to a 'culture' in a laboratory experiment. This set of assumptions can be referred to as a 'model' of the context. 
The consistent success of classical physics can be attributed to the axiom that, for all practical purposes, mathematical formulae can stand in for the system they describe. This canon of absolute isomorphism between the model and the world gave a 'handle' by which one could grasp the real world and put it to use. 

But quantum mechanics has shown that the very act of grasping reality changes the nature of that reality. The observer is an integral but unquantifiable variable in the equation.
The Nobel Laureate chemist llya Prigogine has given the most far-reaching account of such influences on physical systems. Such changes, and the systems in which they occur, are called 'irreversible', or 'non-linear', because one cannot derive the state of the system before the event from its state afterwards, nor vice versa.
This characteristic is as apparent in chemical processes as reversibility and determinism are in the physical processes which have framed scientific thought for most of the modern era, and Prigogine proposes that truly 'reversible' systems be seen as the exception, rather than the rule, if they can be said to exist at all. 
A related distinction is made by the physicist Heinz Pagels. Simple dynamic systems which can be described wholly in mathematical terms are called 'simulatable': any amount of output may be obtained by the input of a few pieces of data.
Non-linear systems, on the other hand, defy description by these simple equations. In fact, one finds that a description of an 'unsimulatable' non-linear system requires as much data input as output. Such a system is 'its own briefest description and its own fastest computer'. 
There must always be a barrier of complexity between the world and our models of it. 

It was Kant who first clearly articulated, to the post-Newtonian world, the distinction between theories of the world and the world itself. Yet despite all admonitions against 'mistaking the map for the territory', the physicist David Bohm claimed that: 
D. Bohm, Wholeness and the Implicate Order (London 1980). p. 21. 
Pagels, op. cit. pp. 252-3. 
'Whenever we find a theoretical reason for something, we are exemplifying the notion of ratio, implying that as the various aspects are related in our idea, so are they related in the thing that the idea is about.' 
It can be seen that the method by which a design is formulated in the mind is not the method by which its object is constructed in the world. The realization of a design is not, then, a re-enactment of the process of conception in the mind, but rather an imposition 'from above'.
A design always anticipates environmental forces, and is organized accordingly. The problem is that the context for which the design is 'formatted' is invariably the wrong context, and the disparity between the modelled and the real context grows as the complexity of the designed object and its anticipated environment increases. 
The 'macrostate' of a system refers to its behaviour at the systemic scale, viewed as a whole. The statistically describable aspects of the macrostate are the result of 'averaging-out' the behaviour of the multitude of 'microstates', or sub-systems. The microstates themselves are much less predictable in behaviour. Since any human-designed system is likely to come into contact only with microstates of large, important systems (climatic, cultural, sociological), the dilemma of modelling is compounded. 
The context into which a designed object is introduced consists of microstates of a virtual infinity of unsimulatable systems
, each of which is unpredictable in behaviour and affected by the equally unpredictable behaviour of all the others. Design may be seen as a process, but the moment of the intrusion of its product into the world is one of ill prepared abruptness and violence. 
MATERIAL AND MATTER 
Although the concept of a basic building block of matter - the atom - has existed since the pre-Socratics, it was only in the last century that it became anything more than an object of hypothesis and speculation. The discovery of the physical existence of an entity which fitted the description of the basic unit of matter was an incremental process that began with John Dalton's chemical Jaws of 1809, but only produced direct evidence with Albert Einstein's 1905 paper on Brownian Motion. That atomism flourished for so long in the total absence of empirical' confirmation is indicative of the tenacity of the rationalist doctrine. 
Cyril Stanley Smith, A Search far Structure (Cambridge, Mass. 1981), p. 115. 
ibid. pp. 122-4. 
The pursuit of the atom has led to a conceptual fragmentation of the world into smaller and smaller component parts, driven by the atomistic axiom that the lower the position of such a part in the hierarchy of order, the more 'real' it is. The quest for the level at which all matter is the same, however, did not yield an explanation of why, at the macroscopic scale, all materials are different. As the metallurgist Cyril Stanley Smith put it: 'the chemical explanation of matter is analogous to using an identification of different brick types as an explanation of the Hagia Sofia.' 
Newton's atoms were indivisible and separate, interacting only mechanically, as in the familiar billiard-ball analogy. Joseph Thomson, at the end of the last century, was the first to demonstrate the existence of a subatomic particle: the electron. The electron gave physical meaning to the chemical concept of valence, or combining capacity, in atoms. Quantum mechanics further demonstrated the dependence of the structure of the atom on its context, and vice versa. Properties of matter could now be attributed to different, patterns of interaction between the valence electrons of the constituent atoms. The qualitative aspects of nature could now be quantified, and solid-state physics emerged as a science of materials as opposed to matter. 

Prigogine has proposed that non-equilibrium is the normal state of the world. 
At equilibrium, matter exhibits an inertia that resists change, but at far-from-equilibrium conditions, the smallest fluctuation may achieve macroscopic change. This increased 'sensitivity' also makes the system much more susceptible to external fields. 
Matter can no longer be seen as passive and 'dead'.
It can sense and respond, in unpredictable ways, to its own condition and to external influence. The intimacy of the interaction of material with these fields is forcing a revision of the conception of matter and form as a separable duality.
FIELDS AND FORMS 
The earliest and best-known articulation of a dualist opposition of form and matter is that of Plato who, to paraphrase, stated that material form is the imperfect reflection of transcendent eternal form, existing outside time and space. Such form is accessible to the minds of men, but the creation of form is the exclusive realm of God. Man's attempts to realize worldly form could only ever achieve imperfect imitations. 
R. Sheldrake, The Presence of the Past 
Conversely, Aristotle hypothesized that non-material orgnimzmg principles were inherent in matter. 
To some extent this view has been borne out by the realization that the macroscopic structural characteristics of materials emerge from quantum-level interactions in those materials but while material structure defines a set of formal opportunities and constraints, it cannot be seen as form. 
The biologist Rupert Sheldrake uses an apt example to illustrate this point: 
'As a house is built it takes up a particular structure. Its form is symbolically represented in architect's diagrams, and to start with it originated in someone's mind. But this form cannot be understood by ... analysing the house nor the blueprints nor the architects' brains ... With the same building materials and the same amount of labour, houses of different shape and structure could have been built ... Neither the materials nor the amount of work done in building fully explain the form of the house.' 

Much impetus for advances in the theory of formal causation have been drawn from within Sheldrake's field of biology. 

At the close of the nineteenth century, there were two conflicting schools of thought on the generation of form in living things. August Weismann proposed that living matter could be divided into 'somatoplasm' (or body matter), and 'germ-plasm' - a chemical substance which was the repository for the form-giving agent. Although a physical substance, this 'germ-plasm' could not be affected by the physical body. Hans Driesch countered this assertion by using the phenomena of embryonic regulation, regeneration and reproduction to support his claim that something with an implicit wholeness acted upon the organism but was not a material part of it. He called this principle 'entelechy'. Both of these theories shared the constraint of unidirectional causality; the form-giving agent directed material but could not be affected by it.
In this way, both were analogous to Plato's theory of transcendent form, although Weismann also attempted to accommodate the Aristotelean argument. 
Modern genetics, despite its more advanced understanding of the underlying phenomena, retains this essential duality in its distinction of the genotype (the 'program' for the organism, contained in the genes) from the phenotype (the organism as it actually manifests itself). 
Current wisdom seems to vindicate Weismann, localizing formal causation in a substance contained within every cell of the body yet unaffected by changes in the form of the organism: the DNA. 

ibid. p. 86. 
R. Arnheim, Entropy and Art: An Essay on Disorder and Order (Berkeley, Calif. 1971), p. 33. 

H. Greene, Mind and Image (Lexington, Ky. 1976). p. 193. 
Rupert Sheldrake, however, doubts that this is an exhaustive description of the apparatus by which form is imparted. He argues that if the genetic program were contained entirely in the genes, all of the cells of the body would be programmed identically, since the DNA contained in a given cell is identical to that of any other cell. How, he asks, does a cell 'know' where it is? With acknowledgement to the legacy of Driesch, he proposes that the DNA is just the information used by a 'program' to 'calculate' the form of the organism. 

Sheldrake calls such programs 'morphogenetic fields'. The idea of a field is not foreign to mainstream science, which uses this concept to explain gravitation, electromagnetism and other phenomena that can be perceived by their effects on matter, yet cannot be explained in terms of matter. Field phenomena are exhibited in objects with holistic properties, such as a magnet or a hologram. A field is always whole. If a magnet is broken in two, each half will possess its own magnetic field. If a hologram is shattered, each fragment will depict, not a shard of a three-dimensional image, but a complete two-dimensional image. A field is mutually tied to the material in which it is manifested. The history of the form is the history of the field. Every type of material form in the universe, from subatomic particles to the universe itself, is conjectured by Sheldrake to have an associated field which guides its formation and maintains its structure. 

The art theorist Rudolf Arnheim has perceived a similar function operating in the genesis of a work of art. He names this shaping principle the 'structural theme' of the work, and sees it as working i.n conjunction with a cosmic form-building force called the 'anabolic tendency'. 'The structural theme', Arnheim states, 'must be conceived of dynamically, as a pattern of forces, not an arrangement of static shapes.' - The biological origins of this line of thought are often explicitly acknowledged. Louis Sullivan used the term 'seed germ' to describe the initial impulse which is maintained through all phases of development of a building's design, and Henry James gave the name 'germ' to the equivalent literary impulse that blossoms into a novel. 

D. Zohar, The Quantum Sell 
(London 1991 ), pp. 74-80. 
R. Dawkins. The Blind Watchmaker (london 1986).p.118. 

An important characteristic of all the above-mentioned formative influences is that they are never prescriptive or static. 
They are plans for action, not plans of form, whose formal destinies are entwined with the material in which they are manifested and only revealed in their interaction with unique sets of contingencies in the material world. 
UNITY AND COMPLEMENTARITY 
Newton's basic particles of matter were solid and separate. The particles of quantum mechanics are intrinsically linked to each other but they also exhibit, even at the physically indivisible subatomic scale, properties of both matter and of fields. 
Probably the most revolutionary assertion of quantum theory is that the observed behaviour of entities at the subatomic scale requires that all being can be described with equal validity as either solid particles or immaterial waves. 
Moreover, neither of these descriptions can stand on its own: being must always be considered from both points of view simultaneously. Niels Bohr dubbed this characteristic the 'principle of complementarity'. 
In tler book, Tf"!e Quantum Self, Danah Zohar expands upon the analogy that may be drawn between the wave/particle distinction and the classical opposition between mind and body, and conjectures that the complementarity of existence at the quantum level may contain the seeds for conceiving of the same sort of intimate co-dependence of the 'consciousness' and 'intelligence' aspects of the brain, of individuals and the relationships between individuals, as well as of the physical and cognitive foci of the self (mind and body). Her intentions are not merely metaphorical. She believes that these characteristics find their causal origin in the double-sided nature of their irreducible components. 
Even without descending to the quantum level, it is possible, to discern a co-existence and co-dependence between form (represented by information) and matter (conceived as material) by extending the biological analogy. 
The matter which makes up life-forms is the same as that which makes up all inanimate things. The information represented by DNA is conveyed by RNA in mere linear 'sentences' composed in an alphabet with only four 'letters': the amino-acids adenine, guanine, thymine and uracil. What is special about living organisms is that this material and this information are able to come together in such a way that they can influence each other, and are indeed dependent upon one another. This characteristic goes some way towards a definition of life itself. One may speak separately about waves and particles, form and matter, information and material, yet the separation remains merely cognitive. 

M. Heidegger (David Farrell Krell, ed.), Basic Writings (London 1977), pp. 157-8. 
Bohm, op. cit. pp. 55-6. 
H. Focillon, The Life of Forms in Art 
(New York 1992), p. 96. 
In The Origin of the Work of Art, Heidegger remarks on the elusiveness of the locus of the 'thingness' of a thing. He states that 'what is constant in a thing, its consistency, lies in the fact that matter stands together with a form. The thing is formed matter.' But this is not meant to indicate that these two aspects, in concert, can be taken as a definition or explanation of the thing. Heidegger continues, 'Matter and form are in no case the original determination of the thingness of the thing.' Both seem to be emanations from a more profound root. 
The physicist David Bohm has called this bedrock of existence the 'implicate order', to distinguish it from theories and sensual perceptions of the world, which are 'explicate orders' - points-of-view on the universe that give consistent and true, but riased and constricted, knowledge of certain manifestations of the totality, but not an 'understanding of totality as a process.' 'Form' and 'matter' can be seen as two such 'angles' on the implicate order. Henri Focillon has remarked on the existence of a formal destiny in matter, and a material destiny in forms, but each is only ever drawn towards its destiny in conjunction with the other. In the world, there exists much more unworked matter and many more unrealized ideas than there are forms. One must inquire into the nature of the process of making which unites these separate potentialities into a unified actuality. 
DIALOGUE AND CONFLICT 
At first glance, Bohm's implicate order and Sheldrake's morphogenetic fields might be perceived as nothing more than elaborate restatements of Platonic idealism. A crucial distinction is that, while the timeless forms were thought to exist in a dimension outside of, and discontinuous with, empirically accessible reality, Bohm and Sheldrake propose realities that are dimensional extensions of the world of human experience:; In opposition to classical conceptions of separateness and distinction, the new sciences offer models of integration and interdependence. 

In the course of natural selection, genes are not 'chosen' for any intrinsic qualities which they may have. Selection is not an abstract computation. Simply put, a gene survives (and is thereby 'selected' to be passed on to future generations) if the organism in which it exists survives, and this largely depends on the suitability of its genes to its environment.
The notion of an ideal gene is useless. The merits of genetic information are revealed only in the process of their immersion in the context through the interface of the organism. Their meaning is purely relational. 

Bohm, op. cit. p. 58. Focillon, op. cit. p. 19. 
Amheim, op. cit. p. 33. 
115 
David Bohm has extended this line of argument to its most profound conclusion- mind/matter as a continuum: 
'All man-made features of our general environment are extensions of the process of thought, for their shapes, forms and general orders of movement originate basicaly in thought, and are incorporated within the environment in the activity of human work which is guided by such thought. Vice versa, everything in the general environment has a shape, form and mode of movement, the content of which "flows in" through perception, giving rise to sense impressions which leave memory races and thus contribute to the basis of further thought. The environment and human thought participate in a process in which analysis into two separate parts has no meaning.' 

Henry Focillon also calls for a renunciation of the opposition between mind and matter, affirming that a work of art is 'born of an incessant exchange between matter and form'. 
In all of its literal and metaphorical guises, dialogue has often been defined as a resolution of differences. Rudolf Arnhelm states this unequivocally in his description of the creation of a work of art as a process of tension reduction between the 'structural theme of the work and the entropy principle': the universal tendency for matter and energy to dissipate, for all differences to level out and for all complete structures to break down to their constituent parts. 
Obviously, the existence of any form or complex system is evidence of at least temporary transgression of this trend, and theories of form giving often define another universal tendency, in opposition to entropy, which makes possible all articulate existence.
This can be seen in Arnheim's structural theme, Sheldrake's morphogenetic fields, and indeed in all field phenomena. 
The reduction of tension between the entropic tendency towards dissolution and the creative will towards elaboration is described by Kohler's Law of Dynamic Direction,
 'which reduces tension not by dissipating or degrading energy but by organizing it according to the simplest, most balanced structure available to the system'. 

ibid. p. 35. 
Heidegger, op. cit. p. 158. Dawkins, op. cit. p. 177. 
The key phrase here is 'available to the system'. At any scale above the atom, the idea of describing some states of a system as allowable and others as unsuitable implies a set of criteria above and beyond the laws of physics which strive towards entropy. Human needs fall into this category of added criteria. 
The meeting of the two antagonistic tendencies of entropy and form giving is the act of 'making'. Heidegger defined making as imposing the onus of usefulness upon matter. 
INTERFACE AND THE OBJECT 
A modern understanding of material in a~gregates has fused the previously irreconcilable rational and sensual aspects of matter, while genetic science has shown how matter at a,. scale reducible to digital information (DNA) can exert its influence upward through the hierarchy of scale and complexity to effect macroscopic change on the world. 
The scale at which DNA interfaces with the world may tautologically be called the 'human scale', because the bodies of humans and other organisms are the result -the graph, it could be said - of a dialogue between this information and the environment. The genetic material inherited by an organism can be said to represent an .implicit model of the environment that organism's parents were subject to, proffered as a 'best guess' at the world the offspring will be entering. Each generation of the organism's species is 'edited' by natural selection to produce a revised set of genes specifically suited to the current environment of the species. Each individual organism is an interface in which the model ,';:; 
implicit in the DNA is tested. The growth of an organism and the evolution of a species are at the same time processes of calculation, of becoming and of design. 
Similarly, the making of an object is a real-time, real-world testing of one's ideas against the world, and the world against one's ideas. Just as the body stands as a representation of the genes to the world, and of the world to the genes, a created object is a type of measuring-stick that gives a relational definition to both the generating idea (or field) and the object's material content and context. Human-created form is the measure of man to nature and the measure of nature to man. 

Bohm, op. cit. p. 54. 
As has been mentioned, dialogical interaction is only necessary, or possible, if there is a difference between the two conversants. The created object is a graph of this difference and a means of tracing of the process of resolution. 
DIFFERENCE AND INFORMATION 
The gap of complexity that exists between unsimulatable real world systems and our models of them reflects the fact that the real world often behaves in ways that directly contradict our ideas about it. There is always more to a thing than the contents of our thoughts about it. Indeed, David Bohm asserts that it is through their independence from our thoughts about them that real things distinguish themselves from the contents of our minds. It is only from the difference between our logic and the world that we gain insight into a system, and into our model of that system. 
It must be re-iterated that this exploration and resolution cannot be carried out symbolically, at an abstract level. The forming of material is the process of calculation, mapping and resolution of differences. The information thus gained is not absolute knowledge about either the world or the form. It is 'relational'; not a measure of either participant in the dialogue, but a measure of the dialogue itself. In the creative process, the resultant form is the graph of this dialogue; a trace or memory of the process. 
DECISION AND TRUTH 
A phenomenological model is tentative, malleable and always in flux, never attaining absolute isomorphism with the world. It is expressed, tested and revised in the creation or manipulation of entities in the world. Except in a utopia, which precludes creativity by refusing to differ from its description, this is a definitive aspect of the human condition. 
The abstracted process of design creates forms for a utopia which doesn't exist. A pan-cultural re-emergence of concern for process and wholeness is finding ample confirmation and encouragement from all sectors of human thought. It is becoming increasingly necessary, and increasingly possible, for the form-maker to re-immerse himself in the form-giving dialogue. 


SELECTED BIBLIOGRAPHY 
Books and articles relevant to this theme. 
I. Aleksander, Thinking Machines: The Search for Artificial Intelligence (Oxford University Press 1987). 
- (ed.), Neural Computing Architectures (North Oxford Academic 1989). 
- and H. Morton, An Introduction to Neural Computing (Chapman & Hall1990). 
C. Alexander, Notes on the Synthesis of Form (Harvard 1964). 
J.A. Baglivo and J.E. Graver, Incidence and Symmetry in De sign and Architecture (Cambridge University Press 1983). 
M. Benedikt (ed.), Cyberspace (MIT Press 1992). 
J. Benthall, Science and Technology in Art Today (Thames and Hudson 1972). 
C. Blakemore and S. Greenfield (eds.), Mindwaves (Biackwell1987). 
M. Boden, Artificial Intelligence and Natural Man 
(Harvester 1977). 
J.T. Bonner, Morphogenesis (Princeton 1952) 
A.W. Burks, Essays on Cellular Automata (University of Illinois Press 1968). 
A.G. Cairns-Smith, Genetic Takeover and the Mineral Origins of Life (Cambridge University Press 1982). 
-Seven Clues to the Origin of Life (Cambridge University Press 1985). 
-'The Chemistry of Materials for Artificial Darwinian Systems', International Reviews in Physical Chemistry, Vol. 7, No.3, 1988, pp. 209-50. 
H. Cohen, The First Artificial Intelligence Coloring Book (Kaufmann 1984). 
E.F. Codd, Cellular Automata (Academic Press 1968). Harold Cohen, exhibition catalogue (Tate Gallery 1983). C. Darwin, The Origin of Species by Means of Natural 
Selection (first published 1859, Penguin edit ion 1968). L. Davis, Handbook of Genetic Algorithms (Van N,pstrand Reinhold 1991). 
P. Davis, The Cosmic Blueprint (Heinemann 1987). R. Dawkins, The Selfish Gene (Oxford University Press 1976). - The Extended Phenotype (Oxford University Press 1982). -The Blind Watchmaker (Longman 1986). 
J. Dayhoff, Neural Network Architectures (Van Nostrand Reinhold 1990). 
C. de Duve, Blueprint for a Cell (Neil Patterson 1991). D.C. Dennett, Consciousness Explained (Penguin 1993). 
K.E. Drexler, Engines of Creation (fourth Estate 1990). R. Dulbecco, The Design of Life (Yale 1987). 
F. Dyson, Origins of Life (Cambridge University Press 1985). C. and R. Eames et al, Powers of Ten (Scientific American Library 1982). 
P. Ehn, Work-Oriented Design of Computer Artifacts (Arbetslivscentrum Stockholm 1988). 
M. Eigen and R. Winkler, Laws of the Game: How the Principles of Nature Govern Chance (English trans., Penguin 1982). 
R. Forsyth (ed.), Machine Learning (Chapman & Hall 1989). B. Fuller, Synergetics (Macmillan 1975). 
J. Gleick, Chaos (Cardinal 1987). 
D.E. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning (Addison-Wesley 1989). 
S. Goonatilake, The Evolution of Information: Lineages in Gene, Culture and Artefact (Pinter Press 1991). 
J.E. Gordon, The New Science of Strong Materials (Penguin 1968). 
S.R. Hameroff, Ultimate Computing: Biomolecular Consciousness and Nanotechnology (Elsevier 1987). 
M .H. Hansell, Animal Architecture and Building Behaviour (Longman 1984). 
S. Hildebrandt and A. Tromba, Mathematics and Optimal Form (Scientific American Library 1985). 
B. Hillier and J. Hanson, The Social Logic of Space (Cambridge University Press 1984). 
J.H. Holland, Adaptation in Natural and Artificial Systems (University of Michigan Press 1975}. 
J.C. Jones, Designing Designing (ADT Press 1991 ). S. Jones, The Language of the Genes (Harper Collins 1993). S.A. Kauffman, The Origins of Order: Self-Organization and Selection in Evolution (Oxford University Press 1993). 
S.F.A. Kettle, Symmetry and Structure (John Wiley 1985). J.R. Koza, Genetic Programming: On the Programming of Computers by Means of Natural Selection (MIT Press 1992). 
C.G. Langton (ed.), 'Artificial Life: Proceedings of Inter disciplinary Workshop on the Synthesis and Simulation of Living Systems, September 1987' in Santa Fe Institute Studies in the Science of Complexity, Vol. VI, 1989. 
- et sl (eds.) 'Artificial Life II: Proceedings of the Workshop on Artificial Life, February 1990', in Santa Fe Institute Studies in the Science of Complexity. Vol. X. 1991. 
W. Latham, The Conquest of Form (Arnolfini, Bristol 1988). S. Levy, Artificial Life: The Ouest for a New Creation (Jonathan Cape 1992). 
J.E. Lovelock, Gaia (Oxford University Press 1979). -The Ages of Gaia (Oxford University Press 1988). B.B. Mandelbrot, The Fractal Geometry of Nature (Freeman 1977). 
l. March and P. Steadman, The Geometry of Environment (RIBA Publications 1971). 
L. March (ed.). The Architecture of Form (Cambridge University Press 1976). 
S.F. Mason, Chemical Evolution (Oxford University Press 1991). 
T. Maver and H. Wagter (eds.}, CAAD Futures 87, proceedings of the Second International Conference on Computer Aided Architectural Design Futures (Elsevier 1988). 
T.A. McMahon and J.T. Bonner, On Size and Life (Scientific American library 1983). 
H. Meinhart, Models of Biological Pattern Formation (london Academic Press 1982). 
J. Miller, Living Systems (McGraw-Hill1978). 
J. Milsum, Positive Feedback (Pergamon 1968). 
W.J. Mitchell. Computer-Aided Architectural Design (Van Nostrand Reinhold 1977). 
- The Logic of Architecture: Design, Computation, and Cognition (MIT Press 1990). 
J. Monod, Chance and Necessity (Collins 1972). N. Negroponte, The Architecture Machine (MIT Press 1970). 
G. Nicolis and I. Prigogine, Exploring Complexity (freeman 1993). 
G. Pask, An Approach to Cybernetics (Hutchinson 1961). -'The Architectural Relevance of Cybernetics', Architectural Design, September 1969, pp. 494-6. ._- 
P. Pearce, Structure in Nature is a Strategy for Design (MIT Press 1978). 
H.O. Peitgen and P.H. Richter, The Beauty of Fractals (Springer Verlag 1986). 
-and D. Saupe (eds.), The Science of Fractal/mages (Springer Verlag 1988). 
R. Penrose, The Emperor's New Mind (Oxford University Press 1989). 
F.C. Phillips, Introduction to Crystallography (Longman 1946}. 
I. Prigogine, I. Stangers, Order Out of Chaos (Bantam 1984). J. Reichardt, Cybernetic Serendipity (Studio Vista 1968). - (ed.), Cybernetics, Art and Ideas (Studio Vista 1971). 
D. Sagan, 'Metametazoa: Biology and Multiplicity' in J. Crary and S. Kwinter (eds.), Incorporations (Zone 1992). 
E. Schrodlnger, What is Life?(1944) and And Mind And Matter (1958). (Reprinted Cambridge University Press 1967.) 
R. Sheldrake. The Presence of the Past (Collins 1988). 
A.V. Shubnikov and V.A. Koptslk, Symmetry in Science and Art (English trans., Plenum 1974). 
K. Simms. 'Artificial Evolution for Computer Graphics', Computer Graphics, Vol. 25, No. 4. July 1991, pp. 319-28. 
-'Interactive Evolution of Dynamical Systems'. Toward a Practice of Autonomous Systems, proceedings of the First European Conference on Artificial Life, Paris, December 1991 (MIT Press 1992). 
H.A. Simon, The Sciences of the Artificial (MIT Press 1969). B.R. Smith, Soh Computing (Addison-Wesley 1984). P. Steadman, The Evolution of Designs: Biological Analogy in 
Architecture and the Applied Arts (Cambridge University Press 1979). - 
I. Stewart, Does God Play Dicel: The New Mathematics of Chaos (Penguin 1990). 
-and M. Golubltsky, Fearful Symmetry: Is God a Geometer?, (Penguin 1993). 
T. Stonier, Information and the Internal Structure of the Universe (Springer Verlag 1990). 
R. Thorn, Structural Stability and Morphogenesis: An Outline of a General Theory of Models (Addison-Wesley 1972). 
W. D'Arcy Thompson, On Growth and Form, 1917, abridged by J.T. Bonner (Cambridge University Press 1961 ). 
S. Todd and W. Latham, Evolutionary Art and Computers (Academic 1992). 
A.M. Turing, 'The Chemical Basis of Morphogenesis', Philosophical Transactions of the Royal Society of London, VoL 237, No. 641, 1952. 
Von Neumann, 'Theory of Automata: Construction, Reproduction, Homogeneity' in A.W. Burks (ed.), The Theory of Self-Reproducing Automata. Part II (University of Illinois Press 1966). 
J. Weizenbaum, Computer Power and Human Reason (Freeman 1976, Pelican 1984). 
123 
A.F. Walls, The Third Dimension in Chemistry (Oxford University Press 1956). 
H. Weyl, Symmetry (Princeton University Press 1952)_ N. Wiener, The Human Use of Human Beings (Sphere 1968, first published in the US in 1958). 
S. Wolfram, 'Universality and Complexity in Cellular Automata', Physics. 10D, 1984, pp. 1-35. 
B. Wooley, Virtual Worlds (81ackwell1992). 
A. Wuensche and M . Lesser, The Global Dynamics of Cellular Automata (Addison-Wesley 1992). 
K.M. Veang, 'Bionics: The Use of Biological Analogies for Design', Architectural Association Quarterly, No.4, 1973, pp. 48-57. 
-'A Theoretical Framework for Incorporating Ecological Considerations in the Design and Planning of the Built Environment', PhD dissertation, Cambridge 1981. 
J.Z. Young, Programs of the Brain (Oxford University Press 1987). 
PREVIOUSLY PUBLISHED ARTICLES ON THIS WORK A selected bibliography of papers and articles by or about the author which are relevant to this theme, arranged in chronological order. 
J.H. Frazer and A. Pike 'Simple Societies and Complex Technologies'. RIBA Journal, September 1972, pp. 376-7. 
J. Gowan, Projects- Architectural Association 1946-71 (AA Publications 1972), p. 75. 
P. Cook and D. Crompton. projects in AA 125, catalogue of exhibition at Institute of Contemporary Arts 
(AA Publications 1972). 
C. Moorcraft 'Design for Survival', Architectural Oesign, July 1972, pp. 414-45. 
J.H. Frazer, 'Prototype Inflatable Darkroom', Architectural Design, March 1974, pp. 191-2. 
-'Reptiles', Architectural Design, April 1974, pp. 231-9. 
F. Guerra. 'The Use of Random in Architectural Design', Bulletin of Computer-Aided Architectural Design, No. 14, January 1974. 
- 'Hypothetico-Deductive Model for Design, Architectural Design, April1974. 
T. Mclaughlin, 'Technology Goes Back to Nature', Observer Magazine, 12 May 1974. 
J. H. Frazer and J.M. Connor, 'A Conceptual Seeding Technique for Architectural Design', PArC 79, proceedings of International Conference on the Application of 
Computers in Architectural Design, Berlin (Online 
Conferences with AMK. 1979), pp. 425-34. 
Anon., 'Cedric Price's Generator', Building Design, 23 February 1979. 
J.H., J.M., P.A. Frazer, 'Intelligent Physical Three-Dimensional Modelling System', Computer Graphics 80, conference proceedings (Online Publications 1980), pp. 359-70. 
B. Lawson, How Designers Think (Architectural Press 1980), pp. 196-7. 
A.E. Scott, P.S. Coates, J.H. Frazer 'Problem-Worrying Program' (1981) in A. Pedretti and G. de Zeeuw (eds.), Levels and Boundaries: Themes in Systems Research, conference proceedings (Princelet 1983). 
J.H., J.M., P.A. Frazer, 'New Developments in Intelligent Modelling', Computer Graphics 81, conference 
proceedings (Online Publications 1981), pp. 139-54. P.S. Coates, J.H. Frazer, J.M. Frazer, A.E. Scott, 'Commercial and Educational Impact of Shape Processors', Computer Graphics 81, conference proceedings (Online Publications 1981). pp. 383-95. 
P. Purcell, 'Technical State-of-the-Art', CAD Journal, Vol. 13, January 1981. 
W.S. Elliott, 'Critical Technologies Contributing to CAD/CAM', Symposium on Trends in CAD/CAM, conference 
proceedings, 1981. 
Anon .. 'A Building that Moves in the Night', New Scientist, 19 March 1981. 
D. Sudjic, 'Birth of the Intelligent Building', Design, January 1981. 
J.H. Frazer, 'Electronic Models as an Aid to Design in Self Build', Helping Others to Help Themselves, proceedings of Northern Ireland Federation of Housing Associations Annual Conference, Coleraine, 1981. 
B. Reffin-Smith, 'Designing for the Animated Standard Person', Computing, 17 December 1981. 
Anon., 'World's First Intelligent Building'. RIBA Journal, January 1982. 
J.H., J.M., P.A. Frazer, 'Three-Dimensional Data Input Devices', Computers/Graphics in the Building Process, conference proceedings, Washington D.C., 1982. 
J.H. Frazer, 'Use of Simplified Three-Dimensional Computer Input Devices to Encourage Public Participation in Design' and (with P.S. Coates, J.M. Frazer, A.E. Seen), 'Low-Cost Microprocessor Based Draughting Systems', Computer Aided Design 82, conference proceedings (Butterworth Scientific 1982), pp. 143-51 and 525-35. 
-'Hardware and Software Techniques for Simplifying Computer Graphics', Computer Graphics 82, conference proceedings (Online Publications 1982), pp. 33-44. 
-'Computer-Aided Design for Design and Crafts Students' in P. Arthur (ed.). CADCAM in Education and Training (Kogan Page 1982), pp. 170-8. 
G. Pask, Microman (Century Publishing 1982), pp. 54, 128-30. 
B. Reffin-Smith, 'Away with Paper', Practical Computing, July 1982. 
J.H. Frazer, P.S. Coates, J.M. Frazer, 'Software and Hardware Approaches to Improving the Man-Machine Interface', Computer Applications in Production and Engineering Conference, proceedings of First lnternationaiiFIP Conference (North Holland 198]l. pp. 1083-94. 
R. Wooley, 'Micropower-Assisted Drafting for Local Authority Standard Drawings', Computer Graphics 83, conference procedings (Online Publications 1983). 
J.H. Frazer, 'The Role of the Designer and the Impact of the Electronic Muse'. The Role of the Designer- Design Research Society Conference (Bath 1984). 
T. Stevens, 'Low-Cost CAD', RIBA Journal, December 1984. pp. 50-1. 
B. Reffin-Smith, Soh Computing (Addison-Wesley 1984), pp. 138-40. . 
J.H. Frazer, 'Future Implications of Low-Cost CAD', Low-Cost Computer-Aided Design, proceedings of Construction Industry Computer Association/Royal Institute of British Architects seminar, London 1985, paper 7, pp. 1-12. 
-'Future Developments in Low-Cost Modelling and Graphics', Construction Industry Computer Conference (London 1985). 
H. Pearman, 'Call Up a Fine Design', Observer, 3 November 1985. 
-'Computer-Aided Living', Design, September 1985, pp. 48-9. 
R. Sarson, 'It's Coming: Intelligent Lege', Computer Talk, 14 January 1985. 
B. Evans, 'Intelligent Building Blocks', ArchiUJct's Journal, January 1985, pp. 47-54. 
J.H. Frazer, 'Future Trends in CAD for the Building Industry', Computers for Architects and the Design Team (Northern Ireland Information Technology Awareness with the Royal Society of Ulster Architects, 1985). 
-'Design Conversion or Design Creation?: The Case for the Creative Use of Computing in Textile Design', Computers in Textiles (UMIST 1985). 
-'Intelligent Draughting Versus Modelling Systems', CADCAM 86, conference proceedings (EMAP 1986), pp. 161-5. 
-'How Soon will Design Education be Able to Benefit from Computer-Aided Design Systems?', Studies in Design Education, Craft and Technology, Vol. 18, No.2, 1986, pp. 79-85. 
-'An Intelligent Approach', CADCAM International, Vol. 5, May 1986, pp. 34-6 (EMAP 1987), pp. 237-43. 
-'Buildings for Tomorrow: Intelligent Architecture', (British Computer Society, Bristol Branch 1987). 
-'Plastic Modelling: The Flexible Modelling of the Logic of Structure and Space' in T. Maver and H. Wagter (eds.), CAAD Futures 87, proceedings of the Second 
International Conference on Computer-Aided Design Futures, Eindhoven (Elsevier 1988), pp. 199-208: 
J. Greig, 'Celebrating the Cerebral', Building Design, May 1988 supplement. 
M.E.C. Hull, J.H. Frazer, R.J. Millar, 'The Transputer for Geometric Modelling Systems'. Parallel Processing for Computer Vision and Display, conference proceedings (Addison-Wesley 1988). 
-'The Transputer: An Effective Tool for Geometric Modelling Systems', International Journal of Computer Applications in Technology, Vol. 1, No. 1/2 1988, pp. 67-73. 
- 'Transputer-based Manipulation of Computed-tomography Images', proceedings of the lEE International Conference on Image Processing and its Applications, University of Warwick, July 1989, pp. 146-50. 
J.H. Frazer, 'Development of Computer Software for an International Market: A Partnership between Academia and Industry' in E.R. Petty and V. John (eds.), Engineering Education and Product Development (SEFI1989), pp. 10-15. 
- 'A Genetic Approach to Design -Towards an Intelligent Teacup?', The Many Faces of Design (Nottingham 1990). 
- 'Universal Constructor' in proceedings of Second International Symposium on Electronic Art, Groningen, Holland 1990. 
126 
D. Brett, 'The Electronic Craftsman', Circa, January 1990, pp. 28-32. 
R. Twinch, 'Generation Games', Building Design, 
14 August 1990. 
J.H. Frazer, 'Computer- Muse or Amanuensis?' in Wim van der Plas (ed.), SISEA Proceedings of Second International Symposium on Electronic Art, Groningen 1990 
(SISEA 1991). pp. 176--83. 
A. Pipes, Drawings for 3-Dimensional Design (Thames and Hudson 1990), pp. 74, 87, 167. 
J.H. Frazer, 'Can Computers be Just a Tool?' Systemica: Mutual Uses of Cybernetics and Science, Vol. 8, 
Amsterdam 1991, pp. 27-36. 
- 'Datastructures for Rule-Based and Genetic Design' in Visual Computing -Integrating Computer Graphics with Computer Vision (Springer Verlag 1992). 
- and P. Graham, 'Genetic Algorithms and the Evolution of Form'. proceedings of Third International Symposium on Electronic Art, Sydney 1992 (publication pending). 
M.E.C. Hull. J.H. Frazer, R.J. Millar, ~The Millar Polyhedron', The Computer Journal, No. 36, 1993, pp. 186-94. 
J.H. Frazer, 'The Use of Computer-Aided Design to Extend Creativity' in H. Freeman and B. Allison (eds.). National Conference Art and Design in Education, electronic proceedings (NSEAO 1993). 
- and P. Graham, 'The Application of Genetic Algorithms to Design Problems with Ill-defined or Conflicting Criteria', proceedings of Conference on Values and (ln)Variants, Amsterdam 1993 (publication pending). 
-and J.M. Frazer, 'Design Thinking- Creativity in Three Di mensions', Creative Thinking: A Multifaceted Approach, proceedings of the First International Conference on Creative Thinking, Malta 1993 (publication pending). 
- 'The Architectural Relevance of Cybernetics', Systems Research, Vol. 10, No.3, 1993, pp. 43-7. 
B. Hatton, interview with John Frazer, Lotus, No. 79, 1993, pp. 15-25. 
J. H. Frazer, 'The Genetic Language of Design' inS. Braddock and M. O'Mahony (eds.), Textiles and New Technology: 2010 (Artemis 1994). 
See also the annual review of student work published by the Architectural Association: Projects Review 1989/90, 1990/91, 1991/92, 1992/93, 1993/94. 
BIOGRAPHICAL NOTE 
John Frazer is unit master of Diploma Unit 11 at the Architectural Association (where he studied from 1963 to 1969) and Director of Computer-Aided Design at the University of Ulster. He has also been a lecturer at the University of Cambridge. In 1983 he co founded Autographics Software Ltd, which pioneered microprocessor graphics. He was awarded a personal chair at the University of Ulster in 1984. 
ACKNOWLEDGEMENTS 
Left: 
The first AA Diploma Unit 7 7 in Ireland, 1989. 
Below: 
HRH the Duke of Edinburgh presenting John Frazer with a British Design Award for Autographics software, 1988. 
I would personally like to thank: 
The Architectural Association, the University of Cambridge and the University of Ulster for supporting and encouraging this work over a long period. 
The Arts Council of England for funding this publication and the accompanying exhibition. 
Those who have generously sponsored this and other recent exhibitions: Aluminium Company of Malaysia Bhd .. Building Centre Trust, ERM Architects, Forward Publishing, Sir Norman Foster. T. A. Hamzah & Yeang Sdn Bhd., Stuart and Ruth Lipton, Roderick G. Newton, Panel Technologies Sdn Bhd ., Shephard Robson Architects. 
Those who have given support in the form of equipment or expertise: Autographics, Blue Chip Technology, Callhaven pic, Cityscape Internet Services Ltd, Ellipsis London Ltd. Pilkingtons. Pyronix Ltd. Rediffusion. AS Components Ltd, Silicon Graphics Computer Systems. H. C. Slingsby, Solaglass. Sarkpoint, Warner Electric Ltd. 
The institutions which have given research grants: the Science and Engineering Research Council, the Royal Institute of British Architects and the University of Ulster. 
The clients who have commissioned work which has assisted in the development and implemenation of these ideas: John Phipps, Cedric Price. Walter Segal and Ken Yeang. My tutors whilst I was a student at the AA Alvin Boyarsky, Peter Cook. James Gowan. Roy Landau, John Starling and Sam Stevens who gave me early encouragement. 
Research students. in particular Francisco Guerra and Ken Yeang at Cambridge and Robin Clarke, Barbara Dass. Peter Graham. Paul Hanna. Nicola Lefever and Richard Millar at Ulster. And research assistants at Ulster. in particular Julia Connor, Julian Fowler, Peter Graham. David McMahon. John Potter. Heather Rea and Gerry Smith . 
All the workshop and photographic technicians at Ulster. in particular Roger Gould. 
The students of AA Diploma Unit 11 over the five years from October 1989 to June 1994: Ade Adek.ola. Victor Baladi. Suha Bekki, Rannveig Bore. Elena Cecchi, lan Chee. 
127 
Juanita Cheung, Swee-Ting Chua. Raymond Chui, Miles Dobson, Alexander Gendell, Lydia Haack, Wassim Halabi, Samantha 
Hardingham. Manuel Herz. Sophie Hicks. Heather Hilburn. John Hopfner. Faisa 
Mohd.lsa, Nazari Jaafar. Tim Jachna, Gary Kong, Nicola Lefever, Sichin Lin. Steven Mankouche. William M clean. lchiro 
Nagasaka. Tomas Quijano. Manit Rastogi, Caroline Rochlitz. Atsushi Sasa. Stefan Seemuller. Pete Silver. Susanna Sirefman Helene Stub, Jui-Pang Tseng, Matthew Waltman. Guy Westbrook. Paola Yacoub. All have made a significant contribution to the development of the ideas in this publication. 
And the new students of Diploma Unit 11: Gianni Botsford, Chris Ceccato. Thomas Fechtner. Hitoshi lhara, Patrick Janssen, Raile Nakajima. Ostap Rudakeych. Tami Segalis, Dominic Skinner and Michael Weinraub, who are already making a valuable contribution to the next phase o/ materialization . 
All photographs are by members of the unit, with the exception of pp. 80 and 85 (Richard Cheatle) and pp.126-7 0: ric Thorburn). 
Isabel Allen and Maureen Herron, my 
secretaries at Ulster. who patiently typed all the technical papers which preceded this publication. and Daphne McEieevy at 
Autographics. who typed all the technical specifications for the program designs. 
The colleagues who have worked with me over the years. in particular the late Alex Pike at Cambridge; Paul Coates and Julian Fowler and all the other staff at Au'tographics. 
especially Pete Silver and Julia Frazer who taught with me in Diploma Unit 11 , and Guy Westbrook w ho has now joined us. I would also like to thank the other ex-students from the AA, Cambridge and Ulster with whom I have continued these discussions. in 
particular Richard Twinch and David Wilson. Gordon Pask deserves a warm mention for his loyal support throughout. My brother Peter Frazer has given generous and vital help with the electronics. And. finally, a very special thanks to my constant companion. supporter. co-researcher. partner. programmer and helm, the long-suffering Julia. without whom nothing would have been achieved. 
Thanks to you all . John Frazer 





1

