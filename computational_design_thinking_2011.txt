COMPUTATIONAL DESIGN THINKING 
Edited by Achim Menges and Sean Ahlquist 


CONTENTS 
PREFACE 
Achim Menges and Sean Ahlquist 

10 
INTRODUCTION 
Computational Design Thinking 
Sean Ahlquist and Achim Menges 

30 
Formation and Transformation 
Johann Wolfgang von Goethe 

32 
On the Theory of Transformations, or the Comparison of Related Forms 
D'Arcy Wentworth Thompson 

42 
Variational Evolution 
Ernst Mayr 

50 
The Meaning of General System Theory 
Ludwig von Bertalanffy 

58 
Systems Generating Systems 
Christopher Alexander 

68 
The Architectural Relevance of Cybernetics 
Gordon Pask 

78 
Towards a Humanism Through Machines 
Nicholas Negroponte 

86 
A New Agenda for Computer-Aided 
Design 
William J Mitchell 

94 
Algorithmic Form 
Kostas Terzidis 

102
Architecture and Practical Design Computation 
Mark Burry 

120 
An Introduction to Creative Evolutionary Systems 
Peter J Bentley and David W Corne 

131 
Constrained Generating Procedures 
John H Holland 

142 
Real Virtuality 
Manuel DeLanda 

149 
A Natural Model for Architecture 
John Frazer 

158 
Morphogenesis and the Mathematics of Emergence 
Michael Weinstock 

168 
Philosophy of Mathematics for Computational Design: Spatial Intuition Versus Logic 
Jane Burry 

179 
Associative Design: From Type to Population 
Peter Trummer 

198 
Integral Formation and Materialisation: Computational Form and Material Gestalt 
Achim Menges 

211 
The Computational Fallacy 
Sanford Kwinter 

216 
SELECT BIBLIOGRAPHY 

218 
INDEX 

PREFACE 
Computational design comprises both an immense chance and a considerable challenge to architecture and related design disciplines. Most often, the challenge is understood to be merely of a technical nature, as computational design requires acquiring scripting and programming skills that traditionally do not form part of the discipline's repertoire and education. There are a vast number of books addressing this aspect, both by equipping the interested designers with the related coding skills as well as exposing them to reference projects from academia and practice. Yet, over many years of teaching computational design, we have realised that the main challenge does not lie in mastering computational design techniques, but rather in acculturating a mode of computational design thinking. A syndrome of this intellectual struggle may be the fact that in contemporary practice, with very few exceptions, design computation still merely serves the purpose of extending or accelerating well established design processes. This, arguably, can be further traced into the traditional paradigms of design education, based upon a model of investing skills with an overlay of theoretical and practical application. We find the capacities of computational design are only fully unfolded at the convergence of exercising both computational thinking and practice. 

Computational design, when aiming to tap into its profound architectural potential, requires and enables us to rethink some central disciplinary concerns. The intellectual foundation for the nature of computational design rests at a particular confluence of domains in fields ranging from mathematics, computer science and systems science to biology and philosophy. We found, however, that no book covered this spectrum of topics in this specific context. This became even more apparent to us when we were in the fortunate situation of acquiring and viewing hundreds of related titles for the library of our newly founded Institute for Computational Design at Stuttgart University, while at the same time building up the educational curriculum. Based on the realisation that the seminal texts, from which a conceptual model for critical computational design thinking in architecture may originate, are scattered across a vast body of literature, we attempted to collect, filter out and selectively sample the most influential pieces by authors that have either provided a foundation to which a computational approach to design could be pursued or have played a considerable role in shaping the field. We believe that this requires connecting to adjacent fields and including historical texts that present original concepts on thinking computationally, juxtaposed against more contemporary statements that are influential to the formation of today's discourse on computational design thinking in architecture. The result is the book in your hands. 

We do not consider the book as an all-encompassing survey, but rather as point of departure for a continued progression of novel modes of design thinking for the architect of the 21st century. Due to the complex nature of the concepts presented one should not expect a linear progression of arguments, but rather a field of multifaceted reflections and speculations on the design possibilities arising from computation. We hope the collection will contribute to establishing a foundation for such thought processes in architecture. 

Achim Menges and Sean Ahlquist, 2011 



INTRODUCTION 
COMPUTATIONAL DESIGN THINKING 
Sean Ahlquist and Achim Menges 

The manifest form -that which appears- is the result of a computational interaction between internal rules and external (morphogenetic) pressures that, themselves, originate in other adjacent forms (ecology). The (pre-concrete) internal rules comprise, in their activity, an embedded form, what is today clearly understood and described by the term algorithm. (Sanford Kwinter)1 

Computation has a profound impact on both the perception and realisation of architectural form, space and structure. It shifts the way one perceives form, the way in which form is purposed, and the way in which form is produced. The fundamental concepts which underlie computational theory and techniques expose form as a subsidiary component of environment, and environment as a complex web of energies in dynamic exchange of both regeneration and degeneration. The emergence of this perspective has come at the confluence of different modes of thinking, in alignment with various scientific, technological and cultural advancements spanning several decades if not centuries. Specifically, computation in design represents an accumulation of multilayered concepts ranging from systems theory and cybernetics, to morphogenesis and developmental biology. 
hesaplamanın soykütüğü

This book presents texts which characterise the initial foundations of these concepts in biology, mathematics, computer science, systems science and philosophy, along with contemporary translations of such to the application in architectural design. The intention of this outlay of concepts, methods and perspectives is to establish a multifaceted yet comprehensive description of what computational design thinking may constitute in architecture. 

WHAT IS COMPUTATION? 
The dominant mode of utilizing computers in architecture today is that of computerization; entities or processes that are already conceptualized in the designer's mind are entered, manipulated, or stored on a computer system. In contrast, computation or computing, as a computer-based design tool is generally limited. The problem with this situation is that designers do not take advantage of the computational power of the computer. (Kostas Terzidis)2 

To understand computation, and its relevance for architectural design, one must understand the distinction between computation and computerisation. In principle, this can be broken down as methods which either deduce results from values or sets of values, or simply compile or associate given values or sets of values. One increases the amount and specificity of information, while the other only contains as much information as is initially supplied.3 This basic conflicting premise is ever present in the various methods by which the computer has been integrated into architectural design. Fundamentally, the distinction rests in the approach towards design, rather than in particular skill sets or knowledge. A computer-aided approach assumes an object-based strategy for encapsulating information into symbolic representations - methods of organising information. In contrast, a computational approach enables specific data to be realised out of initial abstraction - in the form of codes which encapsulate values and actions. These distinctions are reflected back in the philosophical perspectives by which architecture, born of computer-based processes, is perceived. 

One critical aspect of working in a computational manner is the processing of information algorithmically. The idea of the algorithm can be traced back to the work of the Norwegian mathematician Axel Thue, who invented a rewriting program operating on strings of signs in 1914.5 While it was later demonstrated that Thue's problem was actually recursively unsolvable, his decision-making procedures encapsulated all basic aspects of algorithmic computing. Most generally, one can extract from this the definition of an algorithm as a set of procedures consisting of a finite number of rules, which define a succession of operations for the solution of a given problem. In this way, an algorithm is a finite sequence of explicit, elementary instructions described in an exact, complete yet general manner. The application and execution of algorithms on a computer happens through programming languages, which enable computing procedure. This is a fundamental property of computation as a technical achievement, but also as a theoretical framework in which systems of virtual and physical existence are culminations of methodically developed information sets. 

To fully clarify the definition of computation, it must be placed within the context of architectural practice, theory and technology. The first graphical systems for computer design were developed in the 1960s. The initial premise was to mimic the predictive possibilities that, at that time, were being well practised in the field of structural engineering. What was soon realised was that optimisation, as a primary objective, was not an applicable approach given the multiplicity of constraints, requirements and intentions that envelope (of) an architectural problem. Termed automated design, there was a holistic vision of computer programs which could process complex design briefs into specific architectural solutions. This alluded to the question of how a computer can, or even should, mimic human thought processes and ingenuity. This came at the same moment at which cybernetic theory was popularised. Cybernetics advanced the notion of systems theory to address the new existence of the man-machine relationship, stimulating the notion of …how computers may be utilised to expand the human intellect, rather than work redundantly to the processes which form such knowledge. What arose, in short, was a perspective at which architecture could be both perceived and pursued …as a 'system'. This implies that the formulation of architecture occurs through the understanding and development of the complex interrelation of material parts, and social engagement, shaping form, space and structure. 

Initially, graphical systems such as Sketchpad, GRASP and LOKAT developed as early as the mid-1960s were modelled upon a 'systems-based' approach. Sketchpad, developed by Ivan Sutherland at MIT, applied the idea of constraints which could be varied to test and flex the relationships between geometries in the formation of an overall system. 
+Achim Menges, Morphogenetic Design Experiment 01, 2003, Over seven growth steps, two correlated surfaces are computationally generated as part of a morphogenetic and evolutionary design process based on the constructional constraints of the system. The computational growth process employs mathematical rewriting rules based on Hemberg- Extended Lindenmayer Systems. Achim Menges. 
Geometries could be drawn as parametric instances representing different aspects of form, space and structure. Sketchpad was the first system to allow this method of variation and design formation to occur through a graphical input. A light-pen would register points of geometry creation and geometry dependencies. Eric Teicholz, working with some of Sutherland's graphical methods, developed GRASP at Harvard's Graduate School of Design. While the software was concerned, like others, primarily with space planning, it was already a generative system utilising random form-making procedures and constraints based in the specific interrelation of structure, solar exposure and programmatic organisation. LOKAT, also developed in conjunction with Harvard GSD, interjected evaluation into the process defining 'goodness of fit' according to rules of program associations and proximity. Sketchpad introduced an interface while instituting a methodology for maintaining rule-based geometric associations. Among these design environments, Sketchpad was exemplary in defining such fundamental computational methods as parametrics, associativity and rule-based systems generation. Its interaction posed a breakthrough for the cybernetic view of the man-machine relationship, and synthesis of process as a mirror to the working of the resulting system. 

Construction of a drawing with Sketchpad is itself a model of the design process. The locations of the points and lines of the drawing model the variables of a design, and the geometric constraints applied to the points and lines of the drawing model the design constraints which limit the values of design variables. (Ivan Sutherland)10

How this was represented in architectural theory was a shift from the view of architecture as a material object to architecture as a system comprised of and working with a series of interrelated systems. Sketchpad unfolded a logic to capture interrelated geometries, and how associations cause ripple effects in the development and manipulation of form. The description of design was no longer symbolically the representation of the physical elements as they lie against each other but rather a summation of the forces, pressures and constraints which realise the form. The first experiments with computation in design were typically based on specific, singular design briefs. Over time, as these initial programs were to address a larger audience of design scenarios, they became more biased towards methods and objects which were standardised.11
bu ilk hesaplama denemeleri yaygınlaştıkça standartlaştırmaya karşı daha önyargılı?? (takıntılı) hale geliyorlar
Although this has been the conventional trajectory of software development in design, the roots for a computational approach were seen already in these very early experiments. 

It might turn out that all that chaos is the natural consequence of information overload, in which case the power of information-processing machines might prove useful. (Murray Milne)12 

So what is computation, then? In relation to design, computation is the processing of information and interactions between elements which constitute a specific environment, the pivotal word being interactions. This definition pulls computation out of only the virtual realm. In greater relevance to design and the designer, the most general application of computation is in producing outcomes realised from the processing of internal and external properties. Computation provides a framework for negotiating and influencing the interrelation of these datasets of information, with the capacity to generate complex order, form and structure. Computation as a design methodology is to formulate the specific. Where computer-aided processes begin with the specific and end with the object, computational processes start with elemental properties and generative rules to end with information which derives form as a dynamic system. 

SYSTEMS THINKING 
Every aspect of form, whether piecelike or patternlike, can be understood as a structure of components. Every object is a hierarchy of components, the large ones specifying the pattern of distribution of the small ones, the small ones themselves, though at first sight more clearly piecelike, in fact again patterns specifying the arrangement and distribution of still smaller components. (Christopher Alexander)13 

Aristotle, in his definition of Holism, lays the first stone in understanding systems as a whole which is more than the sum of its parts. He proceeds, though, to understand the parts in isolation. Descartes added to this view the notion of understanding processes via simple causality, with relationships deduced from, at most, two interacting elements. In the mid- 20th century, Ludwig von Bertalanffy introduced general system theory as a response to the long evolution of the conception of the 'whole' of functioning systems. His theory is based on a fundamental critique on classic physics and its deductive methods and focus on isolated phenomena. 

+Marco Baur, Sebastian Kron, Christian Schmidts, Christian Weitzel, Evolutionary Design Strategies Project 02, ICD Stuttgart University (Sean Ahlquist, Achim Menges), Stuttgart, 2009. Emulating processes of biological development and evolution, this scripted process utilises an iterative Boolean technique and applies fitness of area to volume ratio, solid to void ratio, and degree of geometric 'contouring' to evolve geometries across multiple generations. In strict rules of selection, ranking and breeding, the populations of geometries converge towards a highly effective outcome given the criteria applied. © Institute for Computational Design, Stuttgart University. 
Bertalanffy considered such methods as unsuitable for biology, reasoning that nothing in nature exists in isolation or simple dependencies, but rather needs to be understood as complex systems of interactions and reciprocities. Beyond simple and linear causal relations, Bertalanffy proposed the concept and mathematical operations to define systems based on nonlinear causalities. Examples of these are the causal loops called 'feedback', that are the processes involving mutually stabilising causes. Here, the effects react back on their causes, and thus open up the possibility to conceptualise what Bertalanffy defines as an 'open system', one of elemental matter and processes in reciprocal exchange between organism and environment. This was a model which could ascertain global behaviour by understanding the full collection of individual elements and their intimate interactions. 
parçaların davranışlarından bütünün davranış anlamak

An open system is defined as a system in exchange of matter with its environment, presenting import and export, building-up and breaking-down of its material components. (Ludwig von Bertalanffy)14 

General system theory was formulated as a universal scientific discipline to decipher the laws which rule the definition of 'organised wholes'. Folded into architecture, as exemplified by Christopher Alexander's writings on design process and physical organisations, form and functionality in the 1960s, the theory implied profound changes. to design thinking. Contrary to established design approaches??, Alexander argued that an overall design problem cannot be divided into sub-problems, and consequently, that it is impossible to arrive at a novel design solution as a summary process of solving individual problems one after the other. 
Alexander'in karmaşık bir bütün olarak tasarımın tekil problemleri çözümü olmadığı tezi
60'lardaki yerleşik tasarım yaklaşımı modernizm mi?

Alexander describes a system as that which focuses on an overall behaviour accomplished through the 'interaction among parts'. The use of the words among parts is critical in this definition as it states that the knowledge of the components has to be complemented with the knowledge of how the components interact, whether it be in competition or in contiguity. As defined initially by Aristotle and later expanded upon by Bertalanffy, systems do not function simply as the summation of the whole. The holistic nature cannot be seen in the individual part, nor can it be seen with the addition of its parts. The system behaviour emerges only in the dynamics of the interactions of the parts. This is not a cumulative linear effect but rather a cyclical causal effect in which the complexity of the level and amount of interaction cannot be directly deciphered. 

The introduction of system theory in architecture and the consequential focus on systems thinking in design applied a twofold shift. First, the shift signified a dismissal of the view of architecture as comprised of entities in static isolation for one which defines form as the culmination of systems which interact with its context in matter, physicality and personal engagement. Second, this shift in architectural thinking introduced fundamental concepts for how the computation of such interrelational, complex behaviour-based systems could be achieved. 
yaklaşım (izolasyonu reddederek bağlamın dikkate alınması) ve yöntem(karmaşık sistemlerin hesaplanması için temel kavramlar) açısından değişim

Where cybernetics, evolving in parallel with general system theory, contributes to the concept of cyclic-causality is the understanding of how machines may be able to resolve such complexity. In particular, the notion of feedback and stability provides functions for determining how the whole is organised. Balanced with the idea of 'goodness of fit”, prediction and the accuracy of prediction serve as critical factors in what Norbert Wiener, who coined the term 'cybernetics', calls the 'learning machine".15 

Fundamental to computational design is the understanding of how systems, as form and as mathematical ordering constructs, operate. Fundamental to understanding their operation is the level of prediction contained in the model which envisions the system. Initially, this was a primary consideration for the first graphical systems in architecture, stated as: 'With the help of computer-aided models, the designer will be able to predict the performance of any design alternative he may generate."16 The success of this depends upon design paradigms which are oriented towards defining systems with concise parameters, and using the effort of prediction and feedback to inform, further specify and define overall the behaviour of the organised system. 

PARAMETRIC DEPENDENCIES 
Goethe's formalism, like all rigorous and interesting ones, actually marks a turning away from the simple structure of end-products and toward the active, ever-changing processes that bring them into being. (Sanford Kwinter)" 

Long predating systems theory was Goethe's doctrine of morphology. In 1796 Goethe introduced the notion of 
, outlining the critical distinction between form and formation. Conceiving of gestalt, the individual form, as inseparable from bildung, the continuous unfolding of formation and transformation, morphology sought to illuminate the processes that governed form rather than form itself. Goethe approached this through the study of physical attributes as a registration of its development, trying to decipher its metamorphosis, and understanding the mechanics of constant development and formation in nature. His description of the 'vertical tendency that occurs in particular plants elucidates a set of dependencies in form and structure in an organism's activation and suppression of specific movements. The geometric relationship in certain plants, as Goethe discovered, expressed verticality so that it may have connection to the roots, and spiralled so that it may blossom, allowing for the possibility of pollination. Although he thought that the actual process of genesis was driven by a divine force, he clearly laid the foundation for linking geometric behaviour with a functional logic, and understanding the general principles mathematically. Through Goethe's concept of natural morphology being in a constant state of becoming, form and the processes of formation and re- formation were all a part of the same interdependent system. 

We pass quickly and easily from the mathematical concept of form in its statical aspect to form in its dynamic relations. We rise from the conception of form to an understanding of the forces which gave rise to it; and in the representation of form and in the comparison of kindred forms, we see in the one case a diagram of forces in equilibrium, and in the other case we discern the magnitude and the direction of the forces which have sufficed to convert the one form into the other. (D'Arcy Thompson)*8 
+Ocean North and Scheffler+Partner Architects, Czech National Library International Competition Entry, 2006. Through an analytic computational procedure, the stress distribution within the envelope of the Czech National Library's cantilevering volumes is evaluated and mapped as a vector field of principal forces (top). According to this structural information, together with view axes and spatial characteristics, a network of merging branches is computationally derived (centre), which is developed into a cantilevering structural envelope (bottom). © Achim Menges. 

+Achim Menges, Morphogenetic Design Experiment 03 - Paper Strip Morphologies, 2004. Morphological differentiation (top left) of a parametric system defined by the physical constraints of paper-strips (bottom) leads to varying performative patterns of structural behaviour (top centre: contour plots of finite element analysis under gravity load) and modulation of light conditions (top right: geographically specific illuminance analysis on the system and a register surface), © Achim Menges. 

The mathematical understanding of the relationship between form and formation was solidified in the work of D'Arcy Thompson. Similar to Goethe, D'Arcy Thompson's quest was to define the abstract mathematical systems which underlie organic structural form and their transformations. In difference, Thompson sought to define form through the understanding of how physical forces produced structure and pattern. The attempt was to devise the underlying set of geometric laws which shape form in relation to external force. Thompson offers a critical component to the understanding of form in that it is a system which organises itself in the presences of both internal and external forces, and that the organisations can shape patterns traced through mathematical rules. 

What Thompson lays the groundwork for in terms of computation and design is parametrics and associative logics. Parametrics might be understood, conventionally, as regarding solely the interdependencies of certain geometrical constraints. But, its deeper understanding is that of establishing methods for interrelating particular behaviours of forms and forces, and how they might be represented as associated mathematical and geometric rules.19 Thompson uses the Cartesian grid as the ultimate parametric device. It is extensible and transformational in multiple dimensions, globally and locally. It is a coordinate system, and, ultimately for Thompson, it supplies a measure to devise comparison between the geometries which he studies. 

One particularly striking aspect of Thompson's mathematical concept of form and transformation is the fact that it does not remain merely analytical, but that it suggests how the mathematical operations can become generative. The passage on the development of an undiscovered, intermediate pelvis type between Archaeopteryx and Apatornis in his 'Theory of Transformations' marks the critical point, at which the natural transition from reconstructing known forms to generating unknown forms with mathematical rigour occurs. 

GENERATIVE MORPHOGENESIS 
Natural morphogenesis, the sequential biological events which define the development of an organism from initial generation to mature system, derives complex organisation, form and structure from the interaction of system-intrinsic material capacities and external influences and forces. The resultant astounding functional integration, performative capacity and material resourcefulness emerges through morphological differentiation, the summary process of each element's response and adaptation to its specific environment. While Goethe and D'Arcy Thompson contributed to the understanding of transformational laws which influence the expression of form, the full knowledge of biological formation did not come into existence until the discoveries of genetics. Morphogenesis and evolution provide both an overarching and intricate conception of the formation and functioning of natural systems. What this combination projects, to architecture, is that computational design can be developed as a set of instructions regarding the process of formation, with the trigger being the interaction with internal and external forces. 

The work of John Frazer, described in his aptly titled book An Evolutionary Architecture,20 details the transfer of the biological principles of morphogenesis and evolution to architecture. His intention was that an architecture based on the robustness, variability and complexity of natural systems must be thought of as being formed abstractly through the same methods. What is also proposed is that the produced architecture is a participant of the natural system, exhibiting 'metabolism' and acting like the mechanisms to which it was formed: in exchange with environment, responsive to feedback and evolutionary in its own right. 

What morphogenesis also portrays are the intimate and irreducible relationships between formation and materialisation. In any organic system there is no distinction, for instance, between material and structure. They are inseparable - always serving the dual purpose, at the most fundamental level, of generating mass and being a self- supporting system. As material is of a particular arrangement of matter and in service of specific capacities, it is devoid of an a priori defined way how that arrangement should be formed. Manuel DeLanda addresses this in defining Deleuze's definition of multiplicities as distinctly opposite to forms being defined by their essence: 'Unlike essences which assume that matter is a passive receptacle for external forms, multiplicities are immanent to material processes, defining their spontaneous capacity to generate pattern without external intervention."21 

If there is no essence, or predicated driver for the arrangement of matter, then it is clear that the properties of matter and capacities to organise in response to internal and external influences are what drive formation. What DeLanda refers to as 'external intervention' implies a directed, or top-down, development rather than responsive, or bottom-up, formation. In computation, the product becomes distinguished from the process. Because there is no pre-existing knowledge of a specific formation, computation can only capture the means and constraints to be inflected upon a specific set of material properties. Computational processes contain everything about the material system except for the system itself. This is often referred to as 'state space'. 

EVOLVING POPULATIONS 
Nothing in biology makes sense except in the light of evolution. (Theodosius Dobzhansky)22 

Evolutionary computation has been employed in various disciplines, and is often described as being all about search. In computer science, a computational problem can be defined in terms of a search space. A search space is a notional field of all existing parameters within a defined problem. The iterative sampling of parameters generates populations of potential solutions. Among these particular solutions, a search algorithm navigates this space to find the most suitable one. Many different search algorithms exist, but evolutionary searches are particularly relevant as the algorithms computationally evolve solutions without explicitly specifying the direct evolution process, which becomes an emergent property of the algorithm.23 

A large component of evolvability must be attributed to inherent structural properties of features that originated (...) for one reason, but also manifest a capacity for subsequent recruitment (with minimal change) to substantially different and novel functions. (Stephen J Gould)24 

The open-ended characteristic of evolutionary computation is of particular interest for architecture, as a design task can usually not be comprehensively described as a problem, but rather as an opportunity for creating novel possibilities within the framework of a given brief. Here, evolution is more about finding than searching, about the continual extension of the search space through novel solutions that emerge as the by-product of the evolutionary dynamics of selection, mutation and inheritance. In this way, evolutionary algorithms, which were traditionally utilised for optimisation, also enable performative morphological processes, adaptiogenesis and inherent novelty to be investigated in bounds greatly superseding hand-driven experimentation.25 

Evolutionary processes are significant as they always operate on a multitude or population of solutions rather than just one. What this adds to the knowledge of design computation, and simultaneously to architecture, is the complete displacement of the concept of type within a design process. Contrary to other approaches to design computation, as for example Frame Based Design2 or Case Based Design,27 the notion of historical precedents as registered in a formal language is not an active participant in this kind of computational generation. It is of critical conceptual importance to understand that in this way evolutionary computation provides for truly explorative processes, rather than reinforcing existing knowledge (fused into the description of a type) that is simply varied or recombined. This is also registered in the understanding of population within an evolutionary context. 

In population thinking, type is a complete abstraction, something which only can be overlaid, not embedded. This works on the notion that acquired characteristics are not embedded in the genotype. They are repercussions from the interactions of development and environment. As evolution is a continual process, the notion of type as a static or suddenly shifting definition also does not work. The forces of variation and selection drive the formation and constant reformation of populations, not a pre- defined essential characterisation.28

In computation, the response is in the use of variational methods based on extensibility rather than type. This is not to say that all computational methods are intended to be universal. It is that natural systems function through the differentiation that develops in their morphogenetic processes. The differentiation occurs via the genotype, and the reactions between the phenotype and the environment. Frazer's work, Darwin's description of natural selection and evolution, and Mayr's detailing of population thinking and evolutionary development depict how such interrelations of form and formation occur. 

+Achim Menges, Morphogenetic Design Experiment 02 - AA Strawberry 

+Bar, 2003. Based on the dynamics of selection, mutation and reproduction, an evolutionary algorithm generates populations of individual morphologies across multiple generations. This evolutionary process is driven by the evaluation of fitness criteria related to the constructional characteristics of the pneumatic structure, which is intended to form a strawberry bar. © Achim Menges. 

+Sean Ahlquist, Moritz Fleischmann, Evolutionary Spatial Subdivision, AA Emergent Technologies and Design Programme (Michael Hensel, Achim Menges, Michael Weinstock), London, 2008. Based upon a simple principle of volumetric subdivision, this algorithm evolves complex relationships within a prescribed envelope between distributions of programmatic allocations, spatial voids and overall articulated massing. Fitness is determined through the generation of multiple geometric populations via conditions of external and internal spatial articulation and programmatic adjacencies. © Sean Ahlquist/Moritz Fleischmann. 

........ 

EMERGENT FORMATIONS 
Emergence is generally understood to be a process that leads to the appearance of structure not directly described by the defining constraints and instantaneous forces that control a system. Over time 'something new appears at scales not directly specified by the equations of motion. An emergent feature also cannot be explicitly represented in the initial and boundary conditions. In short, a feature emerges when the underlying system puts some effort into its creation. (James P Crutchfield)29 

What the summation of these concepts regarding computational design presents is a generative approach to the formation of complex material arrangements. Underlying each individual concept is the demand for a set of procedures to operate in a generally non- hierarchical and undirected fashion. Algorithms serve to set the range of possibilities, while analytical measures apply to set the level of 'fit'-ness of specific instances within the set of possibilities. What this defines is a system by which emergence characterises the resulting behaviours of forms generated from these computational means. Without the presence or imposition of an envisioned 'essence' as mentioned previously, characteristics which define a system can only emerge through the process of integration and formation. It must be understood, though, that emergence is not only a property of pattern- formation or physical self-organisation. Emergence is also a factor in behaviour and function.30 This unfolds a particularly unique opportunity for the relation of designer, design and computation. Novelty arises not only in formal arrangement (a typical desire in conventional computerised approaches), but it is more fruitfully applied in the realisation of multiple behavioural capacities - ones not initially imagined during the genesis of the process. 

The most compelling aspect of systems which exhibit emergent behaviour is that such behaviours are derivatives of simple conditions, often called "agents"." This depends upon, again, the separation of properties and actions. An agent holds a simple set of properties. The environment defines a set of rules in which the agents interact. At neither of the levels does an indication of the emergent behaviour exist. 

To find an emergent behaviour is to find the interaction of unit behaviours which contributes to the global fitness. (J Poon and ML Maher)32 

In computational design, it is rather idealistic to imagine that a functional and highly effective behaviour can emerge from a fundamental nondeterministic process. That a design process evolves implies that the criteria for effectiveness shift as well. In this effort, emergence is a property to not only be observed but to be deciphered, traced and ultimately tabulated by means which can determine its relative capacity. While not traceable explicitly, the concept that a computational process relies upon feedback provides the avenue by which emergent conditions can be honed in a non- directed set of procedures. 
+Sean Ahlquist, ICD Stuttgart University (Achim Menges), Deep Surfaces, Stuttgart, 2010. Based upon Hooke's Law of Elasticity, form is generated as an emergent condition through the determination of tension-equilibrium in the system. The emergent behaviours being traced reach beyond simple formal capacity, but also to include such thinking as particular pattern formation in self-shading and shadow casting. This method employs a Particle System and advanced methods in topological mapping programmed in the Java-based environment Processing. © Sean Ahlquist. 
DESIGNING COMPUTATION 
Before a given scientific discipline can begin to gain from the use of virtual environments, more than just casting old assumptions into mathematical form is necessary. In many cases the assumptions themselves need to be modified. (Manuel DeLanda)33 

The approach for computational design is one which focuses on the execution of variational methods for the purposeful intent of resolving the complexities that exist in the interrelation and interdependencies of material structures and dynamic environments. Design processes consist of two states: how they function, and what they consist of. As mentioned in relation to morphogenetic processes, one state describes the parameters of materiality, while the other describes the influences which will activate those material constraints to arrange in a particular manner. Non-existent is a state of computation which explicitly and singularly describes the resultant condition. This occurs only in the execution of the process as a whole. 

Computation as defining processes for comparative, informational means has the potential to function as a universal application, but the mechanism works only in the processing of specific, not symbolic, conditions relating to materiality, spatiality and context. While the procedures define a vast state space of potentials, the result embodies specific descriptions of an overall system. Computational processes are iterative and recursive, but most importantly, expanding. They work by growing and specifying the information which describes form through procedures which recursively generate form, calling variable parameters within the state space. The variational condition is intended to address not only the complexity of emergent behaviour in active systems, but the relation between the material and the adaptive, customisable and controllable capabilities of current and future processes of materialisation. 

In developing computer programs one is forced to question how people think and how design evolves. In other words, computers must be acknowledged not only as machines for imitating what is understood, but also as vehicles for exploring what is not understood. (Kostas Terzidis)34 

The explorative nature of design computation is not limited but enabled by the inherent finiteness of algorithmic procedures. In his essay on 'Algorithm and Creativity', Peter Weibel posits that: 

The determinism of algorithms leads to an indeterminism. In this way the character of creativity is an open horizon, even though it is generated through a finite number of rules. Creativity means algorithmic design, which needs to be conceived in a way that even the unpredictable, that is occurrences far beyond the subjective horizon of design, can be designed for. Creativity entails algorithmic design plus the complementarity of designing additional system components. (...) The creative figure will be both the designer of algorithms and the interpreter of their outcomes. (Peter Weibel)35 

+Florian Krampe, Christopher Voss, Integrated Urban Morphologies, ICD Stuttgart University (Prof. Achim Menges, Sean Ahlquist), Stuttgart, 2011. The project implements an Evolutionary Algorithm to negotiate constraints for spatial and circulatory organization with potentials for climatic modulation in the deployment of an urban housing block. In this non-typologically based generative process, the methods evolve multi-performing morphologies as well as utilize scripted analyses to identify novel emergent spatial formations. Institute for Computational Design, Stuttgart University. 
In designing the algorithms, Steven Coons addresses more directly the criteria for the algorithms themselves. Where the aim is simply to 'furnish information', Coons sees the explicit difference between a computerised and computational approach: 

There are two quite different philosophies of approach to the achievement of these aims. One approach would be to imbed in the computer a large set of special purpose packaged processes, each designed to perform some special task. If the assembly of such a library of special routines could be made complete enough, then the system would exhibit to the user on the outside an appearance of complete flexibility and generality. This would be satisfactory so long as the designer never called for a capability not already rigidly imbedded in the mechanism. But the design process is unpredictable. Indeed part of the design process consists in designing new ways to perform the design function itself. This is a higher order of design activity, a sort of meta-design (like meta-mathematics) that clearly is outside the scope of any rigid set of special processes that can be anticipated at the beginning. (Steven Coons)36 

In completing the mode of thinking for computational design, the position of design- er must be addressed. Computation does not function with the explicit description of form or finality. Therefore, the designer is posited as the author of the rules as implicit descriptions for the development of form. This is not a trivial matter, nor is it directly an abstract conceptual one. It is the determination of concrete rules regarding material, and criteria for the engagement of architecture with its contextual systems. Frazer and Bentley describe the value in computational means for invention, though more importantly for deducing fitness and effectiveness in complex solutions. Terzidis describes, more universally, a general approach of authorship which eschews precedents and forms information about functioning systems through the very criteria and logics in which it will exist. Design is not only in the selection of criteria, but it is in the fashioning of the arguments themselves, in both a technical computational manner and a theoretical, material and architectural vein. It is therefore important that a critical approach towards computational design exacts knowledge from the historical and practical foundations of these interrelated fields. This knowledge must also reflect a reciprocal maturation of both intellectual discourse and technical advancement, fulfilling a vital positioning of thinking in computational design. 

NOTES 
1 Sanford Kwinter, Far From Equilibrium: Essays on Technology and Design Culture, Actar (Barcelona), 2008, p 147

2 Kostas Terzidis, Algorithmic Architecture, Architectural Press (Oxford), 2006, p xi. 

3 Kostas Terzidis, Expressive Form, Spon Press (London; New York), 2003, pp 65-73. 

4 Paul Coates, Programming Architecture, Routledge (Abingdon), 2010, pp 1-4. 

5 Arto Salomaa, Computation and Automata, Cambridge University Press (Cambridge, UK), 1985, p 131. 6 Friedrich L Bauer, Historische Notizen zur Informatik, Springer (Berlin), 2009, p 162. 

7 Rob Howard, Computing in Construction, Reed Elsevier (Oxford), 1998, pp 19-38. 

8 Gordon Pask, 'The Architectural Relevance of Cybernetics', in Architectural Design, 1969, pp 494-6. 

9 Nick Chrisman, Charting the Unknown: How Computer Mapping at Harvard Became GIS, ESRI 

Press (Redlands, CA), 2006, pp 69-79. 

10 Ivan Sutherland, 'Sketchpad: A Man-Machine Graphical Communication System', Proceedings of the AFIPS Spring Joint Computer Conference, Detroit, 1963, pp 21-3. 

11 Earl Mark, Mark Gross and Gabriela Goldschmidt, 'A Perspective on Computer Aided Design after Four Decades', AMIT, 2008, pp 1-8. 

12 Murray Milne, 'Whatever Became of Design Methodology?", in Computer Aids to Design and Architecture, by Nicholas Negroponte, pp 30-6, Mason/Charter Publishers (London), 1975, p 31. 

13 Christopher Alexander, Notes on the Synthesis of Form, Harvard University Press (Cambridge, 

MA), 1964, p 130. 

14 Ludwig von Bertalanffy, General System Theory: Foundations, Development, Applications, George 

Braziller (New York), 1969, p 141. 

15 Norbert Wiener, Cybernetics: or Control and Communication in the Animal and the Machine, MIT 

Press (Cambridge, MA), 1965, pp 1-29. 

16 Vladimir Bazjanac, "The Promises and Disappointments of Computer-Aided Design', in 

17 Negroponte, Computer Aids to Design and Architecture, pp 17-26. Kwinter, Far From Equilibrium, p 149. 

18 D'Arcy Thompson, On Growth and Form, abridged edition, Cambridge University Press 

(Cambridge), 1961, p 270. 

19 Robert Woodbury, Elements of Parametric Design, Routledge (Abingdon), 2010, p 24. 

20 John Frazer, An Evolutionary Architecture, AA Publications (London), 1995, pp 9-21. 

21 Manuel DeLanda, Intensive Science and Virtual Philosophy, Continuum (New York), 2002, p 26. 

22 Theodosius Dobzhansky, American Biology Teacher 35, 1972, pp 125-9. 

23 Peter J Bentley and David W Come, Creative Evolutionary Systems, Academic Press (San Diego), 2001, pp 1-78. 

24 Stephen Jay Gould, The Structure of Evolutionary Theory, Harvard University Press (Cambridge, MA), 2002, p 801. 

25 M Hemberg, A Menges and U O'Reilly, 'Evolutionary Computation in Architecture", in Architectural Design, vol 74, no 3, Wiley-Academy (London), 2004, pp 48-53. 

26 Marvin Minsky, Society of Mind, MIT Press (Cambridge, MA), 1986, pp 260-72. 

27 Ann Heylighen and Herman Neuckermans, 'A Case Base of Case-Based Design Tools for Architecture', in Computer-Aided Design, vol 33, no 14, Elsevier (Oxford), 2001, pp 1111-22. 

28 Ernst Mayr, What Evolution Is, Basic Books (New York), 2001, pp 83-7. 

29 James P Crutchfield, "The Calculi of Emergence: Computation, Dynamics, and Induction', in the Physica D (1994) special issue on the Proceedings of the Oji International Seminar Complex Systems from Complex Dynamics to Artificial Reality held 5-9 April 1993, Numazu, Japan, p 2. 30 John S Gero (1996), 'Creativity, Emergence and Evolution in Design: Concepts and Framework, Knowledge-Based Systems 9(7), pp 435-48. 

31 John Holland, Emergence: From Chaos to Order, Oxford University Press (Oxford), 2000, pp 115–241. 

32 J Poon and ML Maher, 'Co-Evolution and Emergence in Design', in Artificial Intelligence in Engineering, vol 11, issue 3, July 1997, pp 319-27, Elsevier Science, p 323. 

33 Manuel DeLanda, Virtual Environments and the Emergence of Synthetic Reason', in Joan Broadhurst Dixon and Eric ] Cassidy (eds), Virtual Futures, Routledge (London), 1998, pp 65-76. 

34 Terzidis, Algorithmic Architecture, p 59. 

35 Peter Weibel, 'Algorithmus und Kreativität, in Berka Walter, Emil Brix and Christian Smekal (eds), Woher kommt das Neue? Kreativität in Wissenschaft und Kunst, Böhlau Verlag (Vienna), 2003, p 96. 

36 Steven Coons, 'An Outline of the Requirements for a Computer-Aided Design System', in AFIPS Joint Computer Conference Proceedings, 299-304, ACM (New York), 1963, p 301. 



FORMATION AND TRANSFORMATION 
Johann Wolfgang von Goethe 
In this brief text by Goethe, a translation from the original text 'Ideen über organische Bildung' (1806) as part of his groundbreaking collection of 'Schriften zur Morphologie", the definition of morphology is first established. Morphology, as Goethe describes, depicts the elicited character of systems, of both an animate and inanimate nature, by the inextricable link of form and formation. That a system is in constant transformation, Goethe breaks from the Aristotelian view that the elements of a dynamic organism can be isolated to dissect their functioning within the character of the whole. The characteristics of natural behaviour - conceptualised as interaction, iteration, generation and variation to which Goethe alludes, and the manner by which they are deciphered is fundamental to the exacting of computation in design. Important is not only the set of instructions, but also the meta-instructions which describe the transformational activities giving rise to an overall functioning system. 

When our interest in natural objects, especially the organic ones, is awakened to the extent that we desire to obtain an insight into relationships between character and function, we believe ourselves best able to acquire such knowledge through analysis of the parts. This method is indeed likely to take us far - it requires but a word or two to remind friends of science what chemistry and anatomy have contributed toward an intensive and extensive view of Nature. 

But these analytical efforts, if continued indefinitely, have their disadvantages. To be sure, the living thing is separated into its elements, but one cannot put these elements together again and give them life. This is true even of inorganic bodies, to say nothing of organic ones. 

For this reason, the man of science has always evinced a tendency to recognise living forms as such, to understand their outwardly visible and tangible parts in relation to one another, to lay hold of them as indicia of the inner parts, and thus, in contemplation,' to acquire a degree of mastery over the whole. How closely this scientific aspiration is bound up with the creative and imitative urges need not be dealt with in detail. 

Hence several attempts are found in the progress of art, learning and science to establish and develop a theory to which we should like to give the name 'morphology. The varied forms these attempts assume, will be spoken of in the historical portion of our work. The German language has the word 'Gestalt' to designate the complex of life in an actual organism. In this expression the element of mutability is left out of consideration: it is assumed that whateverforms a composite whole is made fast, is cut off, and is fixed in its character. 

However, when we study forms, the organic ones in particular, nowhere do we find permanence, repose or termination. We find rather that everything is in ceaseless flux. This is why our language makes such frequent use of the term 'Bildung' to designate what has been brought forth and likewise what is in the process of being brought forth. 

In introducing a science of morphology, we must avoid speaking in terms of what is fixed. Thus, if we use the term 'Gestalt at all, we ought to have in mind only an abstract idea or concept, or something which in actuality is held fast for but an instant. 

What has just been formed is instantly transformed, and if we would arrive, to some degree, at a vital intuition of Nature, we must strive to keep ourselves as flexible and pliable as the example she herself provides. 

NOTES 
1 Goethe's method of thinking has been described as contemplative cognition, 'a mode of cognition which is at once sensory (grasping the phenomenon) and spiritual (perceiving the spirit which manifests itself in the phenomenon)'. Karl Vietor, Goethe the Thinker, Harvard University Press (Cambridge, MA), 1950, p 12. 

2 Goethe's discussion in the following paragraphs revolves about the derivation of the nouns, Gestalt 

and Bildung. Gestalt is derived from the Middle High German past participle of stellen: to set, place, put; Bildung is derived from the verb bilden, allied in meaning to, though not a translation of, the English verb to build. The distinction between the two German nouns may be brought out by translating Gestalt as form, Bildung as formation. 

+From Johann Wolfgang von Goethe, translated by Bertha Mueller, 'Formation and Transformation', Goethe's Botanical Writings, Ox Bow Press (Woodbridge, CT), 1989, pp 21-4. Reproduced by permission of University of Hawaii Press. © University of Hawaii Press. For this edition the notes have been renumbered. 


ON THE THEORY OF TRANSFORMATIONS, OR THE COMPARISON OF RELATED FORMS 
D'Arcy Wentworth Thompson 

D'Arcy Wentworth Thompson's seminal book On Growth and Form (1917) lays the foundation for understanding the associated repercussions of environmental pressures and geometric formation. In this selected text, Thompson establishes mathematics as a route to which these repercussions can be mapped. Not only is it described as a means for deciphering an individual course of development or growth, but Thompson also outlays the interpolation between multiple morphometric mappings as a means to project potentials in form. This sets two fundamental branches of a conceptual framework for computational geometry: parametrics and homologies. A parametric equation is defined as a constant equation in which relational parameters vary. It results in producing families of products where each instance will always carry a particular commonness with others. Thompson defines these embedded relationships as homologies. The framework that emerges encapsulates, within formal rules for geometric organisation, the capacities of form, in physical stature and robustness, and their transformation through external influences. 

In the foregoing chapters of this book we have attempted to study the interrelations of growth and form, and the part which the physical forces play in this complex interaction; and, as part of the same enquiry, we have tried in comparatively simple cases to use mathematical methods and mathematical terminology to describe and define the forms of organisms. We have learned in so doing that our own study of organic form, which we call by Goethe's name of Morphology, is but a portion of that wider Science of Form which deals with the forms assumed by matter under all aspects and conditions, and, in a still wider sense, with forms which are theoretically imaginable. 

The study of form may be descriptive merely, or it may become analytical. We begin by describing the shape of an object in the simple words of common speech: we end by defining it in the precise language of mathematics; and the one method tends to follow the other in strict scientific order and historical continuity. Thus, for instance, the form of the earth, of a raindrop or a rainbow, the shape of the hanging chain, or the path of a stone thrown up into the air, may all be described, however inadequately, in common words; but when we have learned to comprehend and to define the sphere, the catenary, or the parabola, we have made a wonderful and perhaps a manifold advance. The mathematical definition of a 'form' has a quality of precision which was quite lacking in our earlier stage of mere description; it is expressed in few words or in still briefer symbols, and these words or symbols are so pregnant with meaning that thought itself is economised; we are brought by means of it in touch with Galileo's aphorism (as old as Plato, as old as Pythagoras, as old perhaps as the wisdom of the Egyptians), that 'the Book of Nature is written in characters of Geometry'

We are apt to think of mathematical definitions as too strict and rigid for common use, but their rigour is combined with all but endless freedom. The precise definition of an ellipse introduces us to all the ellipses in the world; the definition of a 'conic section' enlarges our concept, and a 'curve of higher order all the more extends our range of freedom. By means of these large limitations, by this controlled and regulated freedom, we reach through mathematical analysis to mathematical synthesis. We discover homologies or identities which were not obvious before, and which our descriptions obscured rather than revealed: as for instance, when we learn that, however we hold our chain, or however we fire our bullet, the contour of the one or the path of the other is always mathematically homologous. 

Once more, and this is the greatest gain of all, we pass quickly and easily from the mathematical concept of form in its statical aspect to form in its dynamical relations: we rise from the conception of form to an understanding of the forces which gave rise to it; and in the representation of form and in the comparison of kindred forms, we see in the one case a diagram of forces in equilibrium, and in the other case we discern the magnitude and the direction of the forces which have sufficed to convert the one form into the other. Here, since a change of material form is only effected by the movement of matter,3 we have once again the support of the schoolman's and the philosopher's axiom, ignorato motu, ignoratur natura. 

There is yet another way - we learn from Henri Poincaré - to regard the function of mathematics, and to realise why its laws and its methods are bound to underlie all parts of physical science. Every natural phenomenon, however simple, is really composite, and every visible action and effect is a summation of countless subordinate actions. Here mathematics shows her peculiar power, to combine and to generalise. The concept of an average, the equation to a curve, the description of a froth or cellular tissue, all come within the scope of mathematics for no other reason than that they are summations of more elementary principles or phenomena. Growth and Form are throughout of this composite nature; therefore the laws of mathematics are bound to underlie them, and her methods to be peculiarly fitted to interpret them. 

In the morphology of living things the use of mathematical methods and symbols has made slow progress; and there are various reasons for this failure to employ a method whose advantages are so obvious in the investigation of other physical forms. To begin with, there would seem to be a psychological reason, lying in the fact that the student of living things is by nature and training an observer of concrete objects and phenomena and the habit of mind which s/he possesses and cultivates is alien to that of the theoretical mathematician. But this is by no means the only reason; for in the kindred subject of mineralogy, for instance, crystals were still treated in the days of Linnaeus as wholly within the province of the naturalist, and were described by him after the simple methods in use for animals and plants: but as soon as Haüy showed the application of mathematics to the description and classification of crystals, his methods were immediately adopted and a new science came into being. 

A large part of the neglect and suspicion of mathematical methods in organic morphology is due... to an ingrained and deep-seated belief that even when we seem to discern a regular mathematical figure in an organism, the sphere, the hexagon, or the spiral which we so recognise merely resembles, but is never entirely explained by, its mathematical analogue; in short, that the details in which the figure differs from its mathematical prototype are more important and more interesting than the features in which it agrees; and even that the peculiar aesthetic pleasure with which we regard a living thing is somehow bound up with the departure from mathematical regularity which it manifests as a peculiar attribute of life. This view seems to me to involve a misapprehension. There is no such essential difference between these phenomena of organic form and those which are manifested in portions of inanimate matter. The mathematician knows better than we do the value of an approximate result. The child's skipping-rope is but an approximation to Huygens' catenary curve - but in the catenary curve lies the whole gist of the matter. We may be dismayed too easily by contingencies which are nothing short of irrelevant compared with the main issue; there is a principle of negligibility. Someone has said that if Tycho Brahe's instruments had been 10 times as exact there would have been no Kepler, no Newton, and no astronomy. 

If no chain hangs in a perfect catenary and no raindrop is a perfect sphere, this is for the reason that forces and resistances other than the main one are inevitably at work. The same is true of organic form, but it is for the mathematician to unravel the conflicting forces which are at work together. And this process of investigation may lead us on step by step to new phenomena, as it has done in physics, where sometimes a knowledge of form leads us to the interpretation of forces, and at other times a knowledge of the forces at work guides us towards a better insight into form. After the fundamental advance had been made which taught us that the world was round, Newton showed that the forces at work upon it must lead to its being imperfectly spherical, and in the course of time its oblate spheroidal shape was actually verified. But now, in turn, it has been shown that its form is still more complicated, and the next step is to seek for the forces that have deformed the oblate spheroid. As Newton somewhere says, 'Nature delights in transformations'. 

The organic forms which we can define more or less precisely in mathematical terms, and afterwards proceed to explain and to account for in terms of force, are of many kinds, as we have seen; but nevertheless they are few in number compared with Nature's all but infinite variety. The reason for this is not far to seek. The living organism represents, or occupies, a field of force which is never simple, and which as a rule is of immense complexity. And just as in the very simplest of actual cases we meet with a departure from such symmetry as could only exist under conditions of ideal simplicity, so do we pass quickly to cases where the interference of numerous, though still perhaps very simple, causes leads to a resultant complexity far beyond our powers of analysis. Nor must we forget that the biologist is much more exacting in his requirements, as regards form, than the physicist; for the latter is usually content with either an ideal or a general description of form, while the student of living things must needs be specific. Material things, be they living or dead, show us but a shadow of mathematical perfection. The physicist or mathematician can give us perfectly satisfying expressions for the form of a wave, or even of a heap of sand; but we never ask him to define the form of any particular wave of the sea, nor the actual form of any mountain-peak or hill. [...] 

[...] For one reason or another there are very many organic forms which we cannot describe, still less define, in mathematical terms: just as there are problems even in physical science beyond the mathematics of our age. We never even seek for a formula to define this fish or that, or this or that vertebrate skull. But we may already use mathematical language to describe, even to define in general terms, the shape of a snail-shell, the twist of a horn, the outline of a leaf, the texture of a bone, the fabric of a skeleton, the stream-lines of fish or bird, the fairy lace-work of an insect's wing. Even to do this we must learn from the mathematician to eliminate and to discard; to keep the type in mind and leave the single case, with all its accidents, alone; and to find in this sacrifice of what matters little and conservation of what matters much one of the peculiar excellences of the method of mathematics.? 

In a very large part of morphology, our essential task lies in the comparison of related forms rather than in the precise definition of each; and the deformation of a complicated figure may be a phenomenon easy of comprehension, though the figure itself have to be left unanalysed and undefined. This process of comparison, of recognising in one form a definite permutation or deformation of another, apart altogether from a precise and adequate understanding of the original 'type' or standard of comparison, lies within the immediate province of mathematics, and finds its solution in the elementary use of a certain method of the mathematician. This method is the Method of Coordinates, on which is based the Theory of Transformations. 
I imagine that when Descartes conceived the method of coordinates, as a generalisation from the proportional diagrams of the artist and the architect, and long before the immense possibilities of this analysis could be foreseen, he had in mind a very simple purpose; it was perhaps no more than to find a way of translating the form of a curve (as well as the position of a point) into numbers and into words. This is precisely what we do, by the method of coordinates, every time we study a statistical curve; and conversely, we translate numbers into form whenever we 'plot a curve, to illustrate a table of mortality, a rate of growth, or the daily variation of temperature or barometric pressure. In precisely the same way it is possible to inscribe in a net of rectangular coordinates the outline, for instance, of a fish, and so to translate it into a table of numbers, from which again we may at pleasure reconstruct the curve. 

But it is the next step in the employment of coordinates which is of special interest and use to the morphologist; and this step consists in the alteration, or deformation, of our system of coordinates, and in the study of the corresponding transformation of the curve or figure inscribed in the coordinate network. 

+Pelvis of Archaeopteryx. D'Arcy Thompson - from original publication. Reproduced by permission of Cambridge University Press. Cambridge University Press. 

+The coordinate systems of the first and second figures with three intermediate systems interpolated. D'Arcy Thompson- from original publication. Reproduced by permission of Cambridge University Press. © Cambridge University Press. 

+Pelvis of Apatornis. D'Arcy Thompson - from original publication. Reproduced by permission of Cambridge University Press. © Cambridge University Press. 

+The first intermediate coordinate network, with its corresponding inscribed pelvis. D'Arcy Thompson - from original publication. Reproduced by permission of Cambridge University Press. Cambridge University Press. 
Let us inscribe in a system of Cartesian coordinates the outline of an organism, however complicated, or a part thereof: such as a fish, a crab, or a mammalian skull. We may now treat this complicated figure, in general terms, as a function of x, y. If we submit our rectangular system to deformation on simple and recognised lines, altering, for instance, the direction of the axes, the ratio of x/y, or substituting for x and y some more complicated expressions, then we obtain a new system of coordinates, whose deformation from the original type the inscribed figure will precisely follow. In other words, we obtain a new figure which represents the old figure under a more or less homogeneous strain, and is a function of the new coordinates in precisely the same way as the old figure was of the original coordinates x and y. 

The problem is closely akin to that of the cartographer who transfers identical data to one projection or another; and whose object is to secure (if it be possible) a complete correspondence, in each small unit of area, between the one representation and the other. The morphologist will not seek to draw his organic forms in a new and artificial projection; but, in the converse aspect of the problem, he will enquire whether two different but more or less obviously related forms can be so analysed and interpreted that each may be shown to be a transformed representation of the other. This once demonstrated, it will be a comparatively easy task (in all probability) to postulate the direction and magnitude of the force capable of effecting the required transformation. Again, if such a simple alteration of the system of forces can be proved adequate to meet the case, we may find ourselves able to dispense with many widely current and more complicated hypotheses of biological causation. For it is a maxim in physics that an effect ought not to be ascribed to the joint operation of many causes if few are adequate to the production of it: frustra fit per plura, quod fieri potest per pauciora. 

Before we pass from this brief discussion of transformations in general, let us glance at one or two cases in which the forces applied are more or less intelligible, but the resulting transformations are, from the mathematical point of view, exceedingly complicated. 

The 'marbled papers' of the bookbinder are a beautiful illustration of visible 'stream- lines'. On a dishful of a sort of semi-liquid gum the workman dusts a few simple lines or patches of colouring matter; and then, by passing a comb through the liquid, he draws the colour-bands into the streaks, waves and spirals which constitute the marbled pattern; and which he then transfers to sheets of paper laid down upon the gum. By some such system of shears, by the effect of unequal traction or unequal growth in various directions and superposed on an originally simple pattern, we may account for the not dissimilar marbled patterns which we recognise, for instance, on a large serpent's skin. But it must be remarked, in the case of the marbled paper, that though the method of application of the forces is simple, yet in the aggregate the system of forces set up by the many teeth of the comb is exceedingly complex, and its complexity is revealed in the complicated 'diagram of forces' which constitutes the pattern. 
To take another and still more instructive illustration. To turn one circle (or sphere) into two circles (or spheres) would be, from the point of view of the mathematician, an extraordinarily difficult transformation; but, physically speaking, its achievement may be extremely simple. The little round gourd grows naturally, by its symmetrical forces of expansive growth, into a big, round, or somewhat oval pumpkin or melon." But the Moorish husbandman ties a rag round its middle, and the same forces of growth, unaltered save for the presence of this trammel, now expand the globular structure into two superposed and connected globes. And again, by varying the position of the encircling band, or by applying several such ligatures instead of one, a great variety of artificial forms of 'gourd' may be, and actually are, produced. It is clear, I think, that we may account for many ordinary biological processes of development or transformation of form by the existence of trammels or lines of constraint, which limit and determine the action of the expansive forces of growth that would otherwise be uniform and symmetrical. This case has a close parallel in the operations of the glass- blower, to which we have already, more than once, referred in passing." The glass- blower starts his operations with a tube, which he first closes at one end so as to form a hollow vesicle, within which his blast of air exercises a uniform pressure on all sides; but the spherical conformation which this uniform expansive force would naturally tend to produce is modified into all kinds of forms by the trammels or resistances set up as the workman lets one part or another of his bubble be unequally heated or cooled. It was Oliver Wendell Holmes who first showed this curious parallel between the operations of the glass-blower and those of Nature, when she starts, as she so often does, with a simple tube.12 The alimentary canal, the arterial system including the heart, the central nervous system of the vertebrate, including the brain itself, all begin as simple tubular structures. And with them Nature does just what the glass- blower does, and, we might even say, no more than he. For she can expand the tube here and narrow it there; thicken its walls or thin them; blow off a lateral offshoot or caecal diverticulum; bend the tube, or twist and coil it; and infold or crimp its walls as, so to speak, she pleases. Such a form as that of the human stomach is easily explained when it is regarded from this point of view; it is simply an ill-blown bubble, a bubble that has been rendered lopsided by a trammel or restraint along one side, such as to prevent its symmetrical expansion - such a trammel as is produced if the glass-blower lets one side of his bubble get cold, and such as is actually present in the stomach itself in the form of a muscular band. 
The Florence flask, or any other handiwork of the glass-blower, is always beautiful, because its graded contours are, as in its living analogues, a picture of the graded forces by which it was conformed. It is an example of mathematical beauty, of which the machine- made, moulded bottle has no trace at all. An alabaster bottle is different again. It is no longer an unduloid figure of equilibrium. Turned on a lathe, it is a solid of revolution, and not without beauty; but it is not nearly so beautiful as the blown flask or bubble. 
The gravitational field is part of the complex field of force by which the form of the organism is influenced and determined. Its share is seldom easy to define, but there is a resultant due to gravity in hanging breasts and tired eyelids and all the sagging wrinkles of the old. Now and then we see gravity at work in the normal construction of the body, and can describe its effect on form in a general, or qualitative, way. Each pair of ribs in man forms a hoop which droops of its own weight in front, so flattening the chest, and at the same time, twisting the rib on either hand near its point of suspension.13 But in the dog each costal hoop is dragged straight downwards, into a vertical instead of a transverse ellipse, and is even narrowed to a point at the sternal border. 

NOTES 
Cf Plutarch, Symp. viii, 2, on the meaning of Plato's aphorism (if it actually was Plato's'); mS Πλάτων έλεγε τὸν θεὸν ἀεὶ γεωμετρεῖν. 

2 So said Gustav Theodor Fechner, the author of Fechner's Law, a hundred years ago. (Über die mathematische Behandlung organischer Gestalten und Processe, Berichte d. k. sächs. Gesellsch., Math.-phys. Cl., Leipzig, 1849, pp 50-64.) Fechner's treatment is more purely mathematical and less physical in its scope and bearing than ours, and his paper is but a short one, but the conclusions to which he is led differ little from our own. Let me quote a single sentence which, together with its context, runs precisely on the lines which we have followed in this book: 'So ist also die mathematische Bestimmbarkeit im Gebiete des Organischen ganz eben so gut vorhanden als in dem des Unorganischen, und in letzterem eben solchen oder äquivalenten Beschränkungen unterworfen als in ersterem; und nur sofern die unorganischen Formen und das unorganische Geschehen sich einer einfacheren Gesetzlichkeit mehr nähern als die organischen, kann die Approximation im unorganischen Gebiet leichter und weiter getrieben werden als im organischen. Dies wäre der ganze, sonach rein relative, Unterschied.' Here, in a nutshell, is the gist of the whole matter. 

3 'We can move matter, that is all we can do to it' (Oliver Lodge): 

4 Henri Bergson repudiates, with peculiar confidence, the application of mathematics to biology; cf · Creative Evolution, p 21, 'Calculation touches, at most, certain phenomena of organic destruction. Organic creation, on the contrary, the evolutionary phenomena which properly constitute life, we cannot in any way subject to a mathematical treatment.' Bergson thus follows Bichat: 'C'est peu connaître les fonctions animales que de vouloir les soumettre au moindre calcul, parce que leur instabilité est extrême. Les phénomènes restent toujours les mêmes, et c'est ce qui nous importe; mais leurs variations, en plus ou en moins, sont sans nombre' (La Vie et la Mort, p 257). 

5 When we make a 'first approximation' to the solution of a physical problem, we usually mean that we are solving one part while neglecting others. Geometry deals with pure forms (such as a straight line), defined by a single law; but these are few compared with the mixed forms, like the surface of a polyhedron, or a segment of a sphere, or any ordinary mechanical construction or any ordinary physical phenomenon. It is only in a purely mathematical treatment of physics that the 'single law' can be dealt with alone, and the approximate solution dispensed with accordingly. 6 Cf Haton de la Goupillière, op cit: 'On a souvent l'occasion de saisir dans la nature un reflet des formes rigoureuses qu'étudie la géometrie." 

7 CfWH Young, "The Mathematical Method and its Limitations', Congresso dei Matematici, Bologna, 1928. 

8 The mathematical Theory of Transformations is part of the Theory of Groups, of great importance in modern mathematics. A distinction is drawn between Substitution-groups and Transformation- groups, the former being discontinuous, the latter continuous - in such a way that within one and the same group each transformation is infinitely little different from another. The distinction among biologists between a mutation and a variation is curiously analogous. 

9 Cf (eg) Tissot, Mémoire sur la représentation des surfaces, et les projections des cartes géographiques, Gauthier-Villars (Paris), 1881. 

10 Analogous structural differences, especially in the fibrovascular bundles, help to explain the differences between (eg) a smooth melon and a cantaloupe, or between various elongate, flattened and globular varieties. These breed true to type, and obey, when crossed, the laws of Mendelian inheritance. CfEW Sinnett, 'Inheritance of Fruit-shape in Cucurbita', Botan. Gazette, LXXIV. pp 95-103, 1922, and other papers. 

11 Where gourds are common, the glass-blower is still apt to take them for a prototype, as the prehistoric potter also did. For instance, a tall, annulated Florence oil-flask is an exact but no longer a conscious imitation of a gourd which has been converted into a bottle in the manner described. 

12 Cf Elsie Venner, chap II. 

13 See TP Anderson Stuart, 'How the Form of the Thorax is Partly Determined by Gravitation', Proc. R.S. XLIX, p 143, 1891. 

From D'Arcy Wentworth Thompson, ‘On the Theory of Transformations, or the Comparison of Related Forms', On Growth and Form: The Complete Revised Edition, Dover Publications (New York), 1992, pp 1026-51.© Cambridge University Press, reproduced with permission. For this edition notes have been renumbered. 



VARIATIONAL EVOLUTION 
Ernst Mayr 

[...] 

Ernst Mayr has made a significant contribution to definition of the 'modern synthesis', which combines the theories of heredity, evolution and natural selection. In this excerpt, he defines speciation and emphasises the fundamental necessity of distinguishing populations from types or classes. Extracted from his book What Evolution Is (2001), this text describes the roots of the typologist as being bound to an essentialist point of view, stating that all phenomena are qualities which are absolute. Evolution follows a path from heredity to genotype to environment to variation to phenotype. With the imposition of environment, the possibility of wilfully unbending traits, held within the genotype, is shown as less feasible. With the ultimate goal being fitness within a particular environment, the identification of traits can only occur with the phenotype; a population is being defined by common characteristics at the phenotypic level. Mayr's concept of population thinking separates the direct link between genotype and phenotype. One handles the pool of information while the other deals with organising that information in response to natural selection and environment. This offers a distinct understanding of the basis of formation as a repercussion of contextual interaction rather than a direct lineage of initial intent. 

VARIATION AND POPULATION THINKING 
Darwin showed that one simply could not understand evolution as long as one accepted essentialism. Species and populations are not types, they are not essentialistically defined classes, but rather are biopopulations composed of genetically unique individuals. This revolutionary insight required an equally revolutionary explanatory theory of evolution: Darwin's theory of variation and selection. Two sources of evidence led Darwin to this new concept. One was the empirical study of variable natural populations (particularly during his study of the barnacles), and the other was the observation by animal and plant breeders that no two individuals of their herds or breeding stocks were identical. These individuals were not members of essentialistic classes, and, as we now know, all individuals in a sexual population are genetically unique. 

Apparently, most people find it difficult to grasp the significance of this uniqueness. Let them remember that no two individuals among the six billion humans are identical, not even so-called identical (monozygous) twins. An understanding of the fundamental difference between a class of essentially identical objects and a biopopulation of unique individuals is the foundation of so-called 'population thinking', one of the most important concepts of modern biology. 

The assumptions of population thinking are diametrically opposed to those of the typologist. The populationist stresses the uniqueness of everything in the organic world. What is true for the human species - that no two individuals are alike is equally true for all other species of animals and plants. Indeed, even the same individual changes continuously throughout its lifetime and when placed into different environments. All organisms and organic phenomena are composed of unique features and can be described collectively only in statistical terms. Individuals, or any kind of organic entities, form populations of which we can determine the arithmetic mean and the statistics of variation. Averages are merely statistical abstractions, only the individuals of which the populations are composed have reality. The ultimate conclusions of the population thinker and of the typologist are precisely the opposite. For the typologist, the type (eidos) is real and the variation an illusion, while for the populationist the type (average) is an abstraction and only the variation is real. No two ways of looking at nature could be more different. (Mayr 1959)1 

DARWIN'S VARIATIONAL EVOLUTION 
It was Darwin who introduced this new way of thinking into science. His basic insight was that the living world consists not of invariable essences (Platonian classes), but of highly variable populations. And it is the change of populations of organisms that is designated as evolution. Thus, evolution is the turnover of the individuals of every population from generation to generation. 

When Darwin, in 1837, became an evolutionist, he asked himself, how can the process of evolution be explained? Could he adopt one of the already proposed explanations? He realised eventually that neither transmutationism nor transformationism nor any other theory based on essentialism would do. And he was right. All essentialistic theories of organic evolution are badly flawed, as was convincingly established during the post- Darwinian controversies. 

Darwin had to develop an entirely new kind of explanation that accounted for the abundance of variation in nature. This led him to his theory of natural selection, which was based on population thinking. The same theory was found independently by Alfred Russel Wallace. 

Although Darwin published On the Origin of Species in 1859 (actually Wallace and Darwin published a first statement in 1858), the explanatory theory of variational evolution was not universally adopted until c 80 years later. It is a theory based on the variability of populations. There were two sets of practitioners who had already appreciated this variability for a long time, the taxonomists and the animal and plant breeders, and Darwin had close connections to both of them. 

When sorting out the collections he had made on the voyage of the Beagle, Darwin encountered the same question again and again; are some slightly different specimens merely variants within a population or are they different species? Indeed, in the 1840s when he wrote his monographs on the classification of the barnacles, Darwin came to the conclusion that no two specimens in a collection from a single population were exactly identical. They all were as uniquely different from each other as are human individuals. And the animal and plant breeders, with whom Darwin was associated since his Cambridge student days, told him the same. They always knew which individuals in their herds they should select as the breeding stock for the next generation. Individuality made this possible. 

Since the terms 'transmutationism' and 'transformationism' are not suitable for this new theory, Darwin's theory of evolution through natural selection is best referred to as the theory of variational evolution. According to this theory, an enormous amount of genetic variation is produced in every generation, but only a few individuals of the vast number of offspring will survive to produce the next generation. The theory postulates that those individuals with the highest probability of surviving and reproducing successfully are the ones best adapted, owing to their possession of a particular combination of attributes. Since these attributes are largely determined by genes, the genotypes of these individuals will be favoured during the process of selection. As a consequence of the continuous survival of individuals (phenotypes) with genotypes best able to cope with the changes of the environment, there will be a continuing change in the genetic composition of every population. This unequal survival of individuals is due in part to competition among the new recombinant genotypes within the population, and in part to chance processes affecting the frequency of genes. The resulting change of a population is called evolution. Since all changes take place in populations of genetically unique individuals, evolution is by necessity a gradual and continuous process. 

DARWIN'S THEORIES OF EVOLUTION 
Darwin's views on evolution are often referred to as The Darwinian Theory. Actually they consist of a number of different theories that are best understood when clearly distinguished from each other. The most important of Darwin's theories of evolution are discussed opposite (see Box 1). That they are indeed five independent theories is documented by the fact that the leading 'Darwinians' among Darwin's contemporaries accepted some and rejected others (see Box 2). 

Two of these five theories, evolution as such and the theory of common descent, were widely accepted by biologists within a few years of the publication of the Origin. This represented the first Darwinian revolution. The acceptance of man as a primate in the animal kingdom was a particularly revolutionary step. Three other theories, gradualism, speciation and natural selection, were strongly resisted and were not generally accepted until the evolutionary synthesis. This was the second Darwinian revolution. The Darwinism. proposed by August Weismann and Wallace, in which an inheritance of acquired characters is rejected, was named Neodarwinism by George John Romanes. The Darwinism accepted since the evolutionary synthesis is best simply called Darwinism, because in most crucial aspects it agrees with the original Darwinism of 1859, while the belief in an inheritance of acquired characters is by now totally obsolete. 
Box 1 Darwin's Five Major Theories of Evolution 
1. The nonconstancy of species (the basic theory of evolution). 
2. The descent of all organisms from common ancestors (branching evolution). 
3. The gradualness of evolution (no saltations, no discontinuities). 
4. The multiplication of species (the origin of diversity). 
5. Natural selection 

Box 2 Rejection of Some of Darwin's Theories by Early Evolutionists 
The following table shows the composition of the evolutionary theories of various evolutionists. All of these authors accepted a fifth theory, that of evolution as opposed to a constant, unchanging world. They differed in accepting or rejecting some of Darwin's four other evolutionary theories. 
Darwin's theory of gradualism fitted well into the thinking of the transformationists, but the resistance of the saltationists was sufficiently great that the universal acceptance of the gradualness of evolution had to await the evolutionary synthesis. Darwin's concept of gradualness, however, was of an entirely different nature from that of the transformationists. Their gradualness was due to the gradual change of an essential type, whereas Darwinian gradualism is due to the gradual restructuring of populations. This makes it quite clear why Darwinian evolution, being a populational phenomenon, must always be gradual. A Darwinian must be able to show that every seeming case of saltation or discontinuous evolution can be explained as being caused by a gradual restructuring of populations. 

VARIATION 
The availability of variation is the indispensable prerequisite of evolution, and the study of the nature of variation is therefore a most important part of the study of evolution. Variation, the uniqueness of every individual, is, as we said, characteristic of every sexually reproducing species. To be sure, at first sight all the individuals of a species of snail or butterfly or fish might seem identical, but a closer study of these individuals will reveal all sorts of differences in size, proportions, colour pattern, scaling, bristles, and whatever characteristic one studies. Further studies have shown that variability affects not only visible characters, but also physiological traits, patterns of behaviour, aspects of ecology (eg, adaptation to climatic conditions) and molecular patterns, all of this reinforcing the conclusion that in one way or another every individual is unique. And it is this always available variability that makes the process of natural selection possible. 

Although the variability of the phenotype was appreciated by naturalists as far back as Darwin's day, the early geneticists treated the genotype as rather uniform. When the studies by the population geneticists from the 1920s to the 1960s revealed the presence of a great deal of cryptic variation, this was questioned by some of the classical authors. Yet not even the most enthusiastic Darwinians suspected the amount of genetic variation in populations that was eventually revealed by the methods of molecular genetics. Not only was it discovered that much of the DNA consists of noncoding DNA ('junk), but it was also found that many, perhaps the majority of alleles are 'neutral', that is, their mutation does not affect the fitness of the phenotype (see below). As a result, it is now realised that seemingly identical phenotypes may conceal considerable variation at the level of the gene. 

POLYMORPHISM 
Sometimes variation falls into definite classes, a phenomenon referred to as polymorphism. In the human species we have polymorphisms for eye colour, hair colour, straightness or curliness of the hair, different blood groups and many other of the genetic variants of our species. The study of polymorphisms has greatly contributed to our understanding of the strength and direction of natural selection, as well as the causal factors underlying variation. Two outstanding studies are those on the colour polymorphism of banded snails (Cepaea) by Cain and Sheppard and on chromosome arrangements in Drosophila by Dobzhansky. In most cases it is unknown what is responsible for the maintenance of polymorphism in a population over long periods. A balance of selection pressures is usually assumed, but it may be reinforced by some superiority of the heterozygous carriers that favours the retention of the rarer gene in the population. In a highly diverse environment, phenotypic diversity may be selected, as in the case of the banded snails. 

THE SOURCE OF VARIABILITY 
What is the source of this variability? Where does it come from? How is it maintained from generation to generation? This is what puzzled Darwin all of his life, but in spite of all his efforts he never found the answer. An understanding of the nature of this variability was finally made possible, after 1900, by advancements in genetics and molecular biology. One can never fully understand the process of evolution unless one has an understanding of the basic facts of inheritance, which explain variation. Therefore the study of genetics is an integral part of the study of evolution. But only the heritable part of variation plays a role in evolution. 

GENOTYPE AND PHENOTYPE 
As early as the 1880s it was recognised by perceptive biologists that the genetic material (germ plasm) was something different from the body of an organism (soma), and this distinction was satisfied when the early Mendelians introduced the terms genotype and phenotype. But the prevailing opinion at that time was that the genetic material consisted of proteins like those that make up the body. It came as a real shock when Avery demonstrated in 1944 that the genetic material consisted of nucleic acids. The terminological distinction between an organism and its genes now acquired a new meaning. The genetic material itself is the genome (haploid) or the genotype (diploid), which controls the production of the body of an organism and all of its attributes, the phenotype. This phenotype is the result of the interaction of the genotype with the environment during development. The amplitude of variation of the phenotype produced by a given genotype under different environmental conditions is called its norm of reaction. For instance, a given plant may grow to be larger and more luxuriant under favourable conditions of fertilising and watering than without these environmental factors. Leaves of the Water Buttercup (Ranunculus flabellaris) produced under water are feathery and very different from the broadened leaves on the branches above water. As we shall see, it is the phenotype that is exposed to natural selection, and not individual genes directly. It has been heatedly argued in the past whether a particular property of an organism was due to 'nature' (its genes) or 'nurture' (its environment). All research in the last 100 years indicates that most characteristics of an organism are affected by both factors. This is particularly true for characters that are controlled by multiple genes. There are two sources of variation in a sexually reproducing population, superimposed on each other: the variation of the genotype (because in a sexual species no two individuals are genetically identical) and the variation of the phenotype (because each genotype has its own norm of reaction). Different norms of reaction may react rather differently to the same environmental conditions. 

*** 

THE NATURE OF VARIATION 
In Darwin's day, the nature of variation in populations was not yet understood. This understanding was possible only following developments in the late 19th and 20th centuries. What Darwin did know as a naturalist, taxonomist and student of natural populations was that variation in natural populations seemed to be virtually inexhaustible. It provides abundant material for natural selection in all organisms, at least in sexually reproducing species of animals and plants. The visible characteristics of an organism, its phenotype, are due to instruction during development by their genes and by the genotype interacting with the environment. 

THE IMPACT OF THE MOLECULAR REVOLUTION 
Although the basic principles of inheritance were worked out between 1900 and the 1930s, the real understanding of the nature of inheritance was achieved only through the molecular revolution. It began in 1944 (Avery et al) when it was established that the genetic material consisted not of proteins but of nucleic acids. In 1953 Watson and Crick discovered the structure of DNA, and after this one major discovery followed the other, culminating in the discovery of the genetic code by Nirenberg in 1961... Finally, every step in the translation of the genetic information in the course of the developing organism was understood in principle. Unexpectedly, the basic Darwinian concepts of variation and selection were not affected in any way. Not even the replacement of proteins by nucleic acids as the carriers of genetic information required a change in the evolutionary theory. On the contrary, an understanding of the nature of genetic variation greatly strengthened Darwinism, for it confirmed the finding of the geneticists that an inheritance of acquired characters is impossible. 

Molecular biology's greatest contribution to evolutionary biology was the creation of the field of developmental genetics. Developmental biology, which had so long resisted the evolutionary synthesis, now adopted Darwinian thinking and analysed the functional role of the genotype. This led to the discovery of regulatory genes (hox, pax, etc) and thus vastly enlarged our understanding of the evolutionary aspects of development. 

EVOLUTIONARY DEVELOPMENTAL BIOLOGY 
One of the most important discoveries of molecular genetics was that some genes are very old. This means that the same gene (essentially the same sequence of base pairs) is found in organisms that are only very distantly related, say in Drosophila and mammals. A second discovery was that certain genes, often referred to as regulatory genes, control such basic developmental processes as the determination of anterior vs posterior or of dorsal vs ventral. These findings shed considerable light not only on previously completely puzzling developmental processes, but also on the causation of fundamental events (branching points) in phylogeny. 

Scientists had always assumed that the same gene, no matter where found, always had the same phenotypic effect. But developmental geneticists have now shown that this is not necessarily so. The same gene may have rather different expressions in annelids (polychaetes) and arthropods (crustaceans). Selection seems to be able to recruit genes in new developmental processes that previously had seemed to have other functions. 

It had been shown by morphological-phylogenetic research that photoreceptor organs (eyes) had developed at least 40 times independently during the evolution of animal diversity. A developmental geneticist, however, showed that all animals with eyes have the same regulatory gene, Pax 6, which organises the construction of the eye. It was therefore at first concluded that all eyes were derived from a single ancestral eye with the Pax 6 gene. But then the geneticist also found Pax 6 in species without eyes, and proposed that they must have descended from ancestors with eyes. However, this scenario turned out to be quite improbable and the wide distribution of Pax 6 required a different explanation. It is now believed that Pax 6, even before the origin of eyes, had an unknown function in eyeless organisms, and was subsequently recruited for its role as an eye organiser. 

CONCLUSIONS 
It is shown in this chapter that Darwin, by making biopopulations the foundation of his evolutionary theorising, rather than Platonic types, had found an entirely novel solution for the explanation of evolution. He postulated that the inexhaustible genetic variation of a population, together with selection (elimination), is the key to evolutionary change. To understand how this is implemented one must understand inheritance ... The genetic material is constant and does not permit an inheritance of acquired characteristics. The genotype, interacting with the environment, produces the phenotype during development. Mutations continually replenish the variability of the gene pool. However, the variation of the phenotypes that provide the material for selection is produced by recombination in meiosis, a process of restructuring and reassorting the chromosomes. 

NOTE 
1 E Mayr, 'Typological versus Population Thinking', in Evolution and Anthropology: A Centennial Appraisal, pp 409-12, Anthropological Society of Washington (Washington), 1959. 

From Ernst Mayr, 'Variational Evolution', What Evolution Is, Basic Books (New York) - Perseus Books Group, 2001, pp 83-144. Copyright © 2002 Ernst Mayr. Reprinted by permission of Basic Books, a member of the Perseus Books Group. The note has been added for this edition. 
THE MEANING OF GENERAL SYSTEM THEORY 
Ludwig von Bertalanffy 

*** 

In this text, published as a part of the book General System Theory: Foundations, Development, Applications (1969), which was based upon the essay 'An Outline of General System Theory' (1950), Bertalanffy provides a universal description of the means and outcomes by which dynamic systems operate. Citing a confluence of studies across many fields of science, Bertalanffy specifically discusses two critical conditions that apply to all scenarios: equifinality and feedback. Equifinality states that a system's functioning state can be achieved from various initial starting conditions. Feedback, whereby the system is in constant transformation, poses a mechanism by which information is reinvested into the system to provide a constant rebalancing and recalibration of its functioning state. As Bertalanffy explains, both rely upon the determination of a system to achieve a steady, or 'homeostatic', state. Of particular relevance to design is the understanding that the result of a functioning system is a 'state' and that given inputs, outputs and feedback, this is one of many states in constant fluctuation. 

AIMS OF GENERAL SYSTEM THEORY 
We may summarise these considerations as follows. Similar general conceptions and viewpoints have evolved in various disciplines of modern science. While in the past, science tried to explain observable phenomena by reducing them to an interplay of elementary units investigatable independently of each other, conceptions appear in contemporary science that are concerned with what is somewhat vaguely termed 'wholeness', ie, problems of organisation, phenomena not resolvable into local events, dynamic interactions manifest in the difference of behaviour of parts when isolated or in a higher configuration, etc; in short, 'systems' of various orders not understandable by investigation of their respective parts in isolation. Conceptions and problems of this nature have appeared in all branches of science, irrespective of whether inanimate things, living organisms or social phenomena are the object of study. This correspondence is the more striking because the developments in the individual sciences were mutually independent, largely unaware of each other, and based upon different facts and contradicting philosophies. They indicate a general change in scientific attitude and conceptions. 

Not only are general aspects and viewpoints alike in different sciences; frequently we find formally identical or isomorphic laws in different fields. In many cases, isomorphic laws hold for certain classes or subclasses of 'systems', irrespective of the nature of the entities involved. There appear to exist general system laws which apply to any system of a certain type, irrespective of the particular properties of the 

system and of the elements involved. These considerations lead to the postulate of a new scientific discipline which we call general system theory. Its subject matter is formulation of principles that are valid for 'systems' in general, whatever the nature of their component elements and the relations or 'forces' between them. 

General system theory, therefore, is a general science of 'wholeness' which up till now was considered a vague, hazy and semi-metaphysical concept. In elaborate form it would be a logico-mathematical discipline, in itself purely formal but applicable to the various empirical sciences. For sciences concerned with 'organised wholes', it would be of similar significance to that which probability theory has for sciences concerned with 'chance events'; the latter, too, is a formal mathematical discipline which can be applied to most diverse fields, such as thermodynamics, biological and medical experimentation, genetics, life insurance statistics, etc. 

This indicates major aims of general system theory: 

1 There is a general tendency towards integration in the various sciences, natural and social. 

2 Such integration seems to be centred in a general theory of systems.. 

3 Such theory may be an important means for aiming at exact theory in the nonphysical fields of science. 

4 Developing unifying principles running 'vertically' through the universe of the individual sciences, this theory brings us nearer to the goal of the unity of science. 

5 This can lead to a much-needed integration in scientific education. 

A remark as to the delimitation of the theory here discussed seems to be appropriate. The term and programme of a general system theory was introduced by the present author a number of years ago. It has turned out, however, that quite a large number of workers in various fields had been led to similar conclusions and ways of approach. It is suggested, therefore, to maintain this name which is now coming into general use, be it only as a convenient label. 

It looks, at first, as if the definition of systems as 'sets of elements standing in interrelation' is so general and vague that not much can be learned from it. This, however, is not true. For example, systems can be defined by certain families of differential equations and if, in the usual way of mathematical reasoning, more specified conditions are introduced, many important properties can be found of systems in general and more special cases. 

The mathematical approach followed in general system theory is not the only possible or most general one. There are a number of related modern approaches, such as information theory, cybernetics, game, decision and net theories, stochastic models, operations research, to mention only the most important ones. However, the fact that differential equations cover extensive fields in the physical, biological, economical, and probably also the behavioural sciences, makes them a suitable access to the study of generalised systems. 
INFORMATION AND ENTROPY 
Another development which is closely connected with system theory is that of the modern theory of communication. It has often been said that energy is the currency of physics, just as economic values can be expressed in dollars or pounds. There are, however, certain fields of physics and technology where this currency is not readily acceptable. This is the case in the field of communication which, due to the development of telephones, radio, radar, calculating machines, servomechanisms and other devices, has led to the rise of a new branch of physics. 

The general notion in communication theory is that of information. In many cases, the flow of information corresponds to flow of energy, eg, if light waves emitted by some objects reach the eye or a photoelectric cell, elicit some reaction of the organism or some machinery, and thus convey information. However, examples can easily be given where the flow of information is opposite to the flow of energy, or where information is transmitted without a flow of energy or matter. The first is the case in a telegraph cable, where a direct current is flowing in one direction, but information, a message, can be sent in either direction by interrupting the current at one point and recording the interruption at another. For the second case, think of the photoelectric door openers as they are installed in many supermarkets: the shadow, the cutting off of light energy, informs the photocell that somebody is entering, and the door opens. So information, in general, cannot be expressed in terms of energy. 

There is, however, another way to measure information, namely, in terms of decisions. Take the game of Twenty Questions, where we are supposed to find out an object by receiving simple 'yes' or 'no' answers to our questions. The amount of information conveyed in one answer is a decision between two alternatives, such as animal or nonanimal. With two questions, it is possible to decide for one out of four possibilities, eg, mammal - nonmammal, or flowering plant - nonflowering plant. With three answers, it is a decision out of eight, etc. Thus, the logarithm at the base 2 of the possible decisions can be used as a measure of information, the unit being the so-called binary unit or bit. The information contained in two answers is log, 4 = 2 bits, of three answers, log, 8 = 3 bits, etc. This measure of information happens to be similar to that of entropy or rather negative entropy, since entropy also is defined as a logarithm of probability. But entropy, as we have already heard, is a measure of disorder; hence negative entropy or information is a measure of order or of organisation since the latter, compared to distribution at random, is an improbable state. 

A second central concept of the theory of communication and control is that of feedback. A simple scheme for feedback is the following (Figure 1). The system comprises, first, a receptor or 'sense organ', be it a photoelectric cell, a radar screen, a thermometer, or a sense organ in the biological meaning. The message may be, in technological devices, a weak current, or, in a living organism, represented by nerve conduction, etc. Then there is a centre recombining the incoming messages and transmitting them to an effector, consisting of a machine like an electromotor, a heating coil or solenoid, or of a muscle which responds to the incoming message in such a way that there is power output of high energy. Finally, the functioning of the effector is monitored back to the receptor, and this makes the system self-regulating, ie, guarantees stabilisation or direction of action. 

Figure 1. Simple feedback scheme. Ludwig von Bertalanffy - reprinted from original publication George Braziller. 
Feedback arrangements are widely used in modern technology for the stabilisation of a certain action, as in thermostats or in radio receivers; or for the direction of actions towards a goal where the aberration from that goal is fed back, as information, till the goal or target is reached. This is the case in self-propelled missiles which seek their target, anti- aircraft fire control systems, ship-steering systems, and other so-called servomechanisms. There is indeed a large number of biological phenomena which correspond to the feedback model. First, there is the phenomenon of so-called homeostasis, or maintenance of balance in the living organism, the prototype of which is thermoregulation in warm-blooded animals. Cooling of the blood stimulates certain centres in the brain which 'turn on' heat-producing mechanisms of the body, and the body temperature is monitored back to the centre so that temperature is maintained at a constant level. Similar homeostatic mechanisms exist in the body for maintaining the constancy of a great number of physicochemical variables. Furthermore, feedback systems comparable to the servomechanisms of technology exist in the animal and human body for the regulation of actions. If we want to pick up a pencil, a report is made to the central nervous system of the distance by which we have failed to grasp the pencil in the first instance; this information is then fed back to the central nervous system so that the motion is controlled till the aim is reached. 

So a great variety of systems in technology and in living nature follow the feedback scheme, and it is well known that a new discipline, called cybernetics, was introduced by Norbert Wiener to deal with these phenomena. The theory tries to show that mechanisms of a feedback nature are the base of teleological or purposeful behaviour in man-made machines as well as in living organisms, and in social systems. 

It should be borne in mind, however, that the feedback scheme is of a rather special nature. It presupposes structural arrangements of the type mentioned. There are, however, many regulations in the living organism which are of an essentially different nature, namely, those where the order is effectuated by a dynamic interplay of processes. Recall, eg, embryonic regulations where the whole is re-established from the parts in equifinal processes. It can be shown that the primary regulations in organic systems, ie, those which are most fundamental and primitive in embryonic development as well as in evolution, are of the nature of dynamic interaction. They are based upon the fact that the living organism is an open system, maintaining itself in, or approaching a steady state. Superposed are those regulations which we may call secondary, and which are controlled by fixed arrangements, especially of the feedback type. This state of affairs is a consequence of a general principle of organisation which may be called progressive mechanisation. At first, systems - biological, neurological, psychological or social - are governed by dynamic interaction of their components; later on, fixed arrangements and conditions of constraint are established which render the system and its parts more efficient, but also gradually 

diminish and eventually abolish its equipotentiality. Thus, dynamics is the broader aspect, since we can always arrive from general system laws to machinelike function by introducing suitable conditions of constraint, but the opposite is not possible. 

CAUSALITY AND TELEOLOGY 
Another point 1 would like to mention is the change the scientific world picture has undergone in the past few decades. In the world view called mechanistic, which was born of classical physics of the 19th century, the aimless play of the atoms, governed by the inexorable laws of causality, produced all phenomena in the world, inanimate, living and mental. No room was left for any directiveness, order or telos. The world of the organisms appeared a product of chance, accumulated by the senseless play of random mutations and selection; the mental world as a curious and rather inconsequential epiphenomenon of material events. 

The only goal of science appeared to be analytical, ie, the splitting up of reality into ever smaller units and the isolation of individual causal trains. Thus, physical reality was split up into mass points or atoms, the living organism into cells, behaviour into reflexes, perception into punctual sensations, etc. Correspondingly, causality was essentially one- way: one sun attracts one planet in Newtonian mechanics, one gene in the fertilised ovum produces such and such inherited character, one sort of bacterium produces this or that disease, mental elements are lined up, like the beads in a string of pearls, by the law of association. Remember Kant's famous table of the categories which attempts to systematise the fundamental notions of classical science: it is symptomatic that the notions of interaction and of organisation were only space-fillers or did not appear at all. We may state as characteristic of modern science that this scheme of isolable units acting in one-way causality has proved to be insufficient. Hence the appearance, in all fields of science, of notions like wholeness, holistic, organismic, gestalt, etc, which all signify that, in the last resort, we must think in terms of systems of elements in mutual interaction. 

Similarly, notions of teleology and directiveness appeared to be outside the scope of science and to be the playground of mysterious, supernatural or anthropomorphic agencies; or else, a pseudoproblem, intrinsically alien to science, and merely a misplaced projection of the observer's mind into a nature governed by purposeless laws. Nevertheless, these aspects exist, and you cannot conceive of a living organism, not to speak of behaviour and human society, without taking into account what variously and rather loosely is called adaptiveness, purposiveness, goal-seeking and the like. 

It is characteristic of the present view that these aspects are taken seriously as a legitimate problem for science; moreover, we can well indicate models simulating such behaviour. 

Two such models we have already mentioned. One is equifinality, the tendency towards a characteristic final state from different initial states and in different ways, based upon dynamic interaction in an open system attaining a steady state; the second, feedback, the homeostatic maintenance of a characteristic state or the seeking of a goal, based upon circular causal chains and mechanisms monitoring back information on deviations from the state to be maintained or the goal to be reached. A third model for adaptive behaviour, a 'design for a brain', was developed by Ross Ashby, who incidentally started with the same mathematical definitions and equations for a general system as were used by the present author. Both writers have developed their systems independently and, following different lines of interest, have arrived at different theorems and conclusions. Ashby's model for adaptiveness is, roughly, that of step functions defining a system, ie, functions which, after certain critical value is passed, jump into a new family of differential equations. This means that, having passed a critical state, the system starts off in a new way of behaviour. Thus, by means of step functions, the system shows adaptive behaviour by what the biologist would call trial and error: it tries different ways and means, and eventually settles down in a field where it no longer comes into conflict with critical values of the environment. Such a system adapting itself by trial and error was actually constructed by Ashby as an electromagnetic machine, called the homeostat. I am not going to discuss the merits and shortcomings of these models of teleological or directed behaviour. What should be stressed, however, is the fact that teleological behaviour directed towards a characteristic final state or goal is not something off limits for natural science and an anthropomorphic misconception of processes which, in themselves, are undirected and accidental. Rather it is a form of behaviour which can well be defined in scientific terms and for which the necessary conditions and possible mechanisms can be indicated. 

*** 

GENERAL SYSTEM THEORY AND THE UNITY OF SCIENCE 
Let me close these remarks with a few words about the general implications of interdisciplinary theory. The integrative function of general system theory can perhaps be summarised as follows. So far, the unification of science has been seen in the reduction of all sciences to physics, the final resolution of all phenomena into physical events. From our point of view, unity of science gains a more realistic aspect. A unitary conception of the world may be based, not upon the possibly futile and certainly far-fetched hope finally to reduce all levels of reality to the level of physics, but rather on the isomorphy of laws in different fields. Speaking in what has been called the 'formal' mode, ie, looking at the conceptual constructs of science, this means structural uniformities of the schemes we are applying. Speaking in 'material' language, it means that the world, ie, the total of observable events, shows structural uniformities, manifesting themselves by isomorphic traces of order in the different levels or realms. 

We come, then, to a conception which in contrast to reductionism, we may call perspectivism. We cannot reduce the biological, behavioural and social levels to the lowest level, that of the constructs and laws of physics. We can, however, find constructs and possibly laws within the individual levels. The world is, as Aldous Huxley once put it, like a Neapolitan ice cream cake where the levels - the physical, the biological, the social and the moral universe - represent the chocolate, strawberry and vanilla layers. We cannot reduce strawberry to chocolate - the most we can say is that possibly in the last resort, all is vanilla, all mind or spirit. The unifying principle is that we find organisation at all levels. The mechanistic world view, taking the play of physical particles as ultimate reality, found its expression in a civilisation which glorifies physical technology that has led eventually to the catastrophes of our time. Possibly the model of the world as a great organisation can help to reinforce the sense of reverence for the living which we have almost lost in the last sanguinary decades of human history. 

*** 

From Ludwig von Bertalanffy, General System Theory: Foundations, Development, Applications, George Braziller (New York), 1969. George Braziller. 
SYSTEMS GENERATING SYSTEMS 
Christopher Alexander 

In his earlier writings, Christopher Alexander established the notion of the unselfconscious process. Initially, this refers to the notion of process, in a social context, as being interrelational by which culture, building and environment are concurrently formed. In this selected text, published in an issue of Architectural Design (1968), it is extended into the notion of how architectural problems may be solved through an analogous process where design forms through the iterative readings and responses to interrelational conditions, with the intention of producing environments synchronous with their cultural settings. Inherent in this position regarding process and form is the substantial step that Alexander makes to transfer the conception of form as an observed object to one as an externalised operating system. In this article, he argues how such a system is born, itself, of a generative system, establishing the duality between the object as a computing agent and the method as a computational process. Alexander lays out particular aspects of such a process, describing three conditions: the global behaviour, the components that form such behaviour and the types of local relationships among those components. While these three aspects characterise a system, this does not extensively describe the process by which a system can be achieved. Alexander explains, through defining four characteristics of a system, the distinction between the behaviour, as a collection of actions, and the system which generates that behaviour, as a series of interactions, and at what level of abstraction such complexity can be understood. 

There are two ideas hidden in the word system: the idea of a system as a whole and the idea of a generating system. 

A system as a whole is not an object but a way of looking at an object. It focuses on some holistic property which can only be understood as a product of interaction among parts. 

A generating system is not a view of a single thing. It is a kit of parts, with rules about the way these parts may be combined. 

Almost every 'system as a whole is generated by a 'generating system'. If we wish to make things which function as 'wholes' we shall have to invent generating systems to create them. 

In a properly functioning building, the building and the people in it together form a whole: a social, human whole. The building systems which have so far been created do not in this sense generate wholes at all. 

1. There are two ideas hidden in the word system: the idea of a system as a whole and the idea of a generating system. 

The word system, like any technical word borrowed from common use, has many meanings and is imprecise. This lack of precision in a technical word might seem dangerous at first; in fact it is often helpful. It allows new ideas to flourish while still vague, it allows connections between these ideas to be explored, and it allows the ideas to be extended, instead of having them cut short by premature definition and precision. 

The word 'system' is just such a word. It still has many meanings hidden in it. Among these meanings there are two central ones: the idea of a system as a whole, and the idea of a generating system. 

These two views, though superficially similar, are logically quite different. In the first case the word 'system' refers to a particular holistic view of a single thing. In the second case, the word 'system' does not refer to a single thing at all, but to a kit of parts and combinatory rules capable of generating many things. 
2. A system as a whole is not an object but a way of looking at an object. It focuses on some holistic phenomenon which can only be understood as a product of interaction among parts, 

Let us consider some examples of holistic phenomena which need to be viewed as systems. 

The great depression is an obvious example of a holistic phenomenon. We cannot understand the depression, except as a result of interaction among rates of consumption, capital investment and savings: the interactions can be specified in the form of equations; if we follow these equations through to their conclusion, we see that under certain conditions they must always lead to a depression. 

 

The stability of a candle flame is another example of a holistic phenomenon. Why does it maintain approximately the same size and shape throughout its flickering? In this case, the 'parts' are flows of vapourised wax, oxygen and burnt gases - the processes of combustion and diffusion give the interaction between these flows and these interactions show us at what size and shape the flame will be approximately stable. 

The strength of a rope is another example of a holistic property. This strength is a result of interaction among the individual strands, caused by the twisting of the rope: untwisted, the rope's strength is governed by the weakest strand; twisted, the strands act together and increase their strength. 

Another example of a holistic property is the relation between input and output in any computer. In the toy computer called ThinkADot, a ball dropped into one of three holes comes out on one of two sides. The output side is not determined by the input hole, but by the input hole and the internal state of the machine, which is itself determined by the sequence of past inputs. In order to understand this behaviour, we must understand the machine as a whole, considering the past inputs and the internal states as parts, and the way that different sequences of inputs and internal states create specific new internal states and outputs as interactions. 

Another kind of holistic behaviour is that instability which occurs in objects that are very vulnerable to a change in one part: when one part changes, the other parts change also. We see this in the case of erosion: cutting down trees robs the soil of the roots which hold it together, so that wind and water can strip the soil of all remaining plants, and make a desert. We see it again in the death of the traditional farm: when the combine harvester replaced traditional harvesting, the entire balance of scale economies was destroyed, the little farms collapsed, and gave way to giant farms. 

Let us summarise the content of these examples. In every case we are confronted with an object which displays some kind of behaviour which can only be understood as a product of interaction among parts within the object. We call this kind of behaviour, holistic behaviour. 

The central point of the whole argument can be stated very simply. The most important properties which anything can have are those properties that deal with its stability. It is stability which gives a thing its essential character. The strength of an arch, the even burning of a flame, the growth of an animal, the balance of a forest ecology, the steady flow of a river, the economic security of a nation, the sanity of a human individual, the health of a society: these are all, in one way or another, concerned with stability. 

Stability, no matter in which of its many forms, is a holistic property. It can only be understood as a product of interaction among parts. The essential character of anything whatever, since it must at heart be based on some kind of stability, must be understood as a product of interactions within the whole. When we view a thing in such a way as to reveal its character in holistic terms, we speak of it as a system. 

In order to speak of something as a system, we must be able to state clearly: (1) the holistic behaviour which we are focusing on; (2) the parts within the thing, and the interactions among these parts, which cause the holistic behaviour we have defined; (3) the way in which this interaction, among these parts, causes the holistic behaviour defined. 

If we can do these three, it means we have an abstract working model of the holistic behaviour in the thing. In this case, we may properly call the thing a system. If we cannot do these three, we have no model, and it is meaningless to call the thing a system. The idea of a system is synonymous with the idea of an abstract model of some specific holistic behaviour. We may speak of the economic system in a country, because we can construct a system of equations which reproduce important holistic phenomena like depressions or inflation. If we couldn't do this, it would be meaningless to speak of economic systems. 

We must not use the word system, then, to refer to an object. A system is an abstraction. It is not a special kind of thing, but a special way of looking at a thing. It is a way of focusing attention on some particular holistic behaviour in a thing, which can only be understood as a product of interaction among the parts. Everything under the sun may be viewed as a system: a man smoking a cigarette may be viewed as a system; so may a leaf drifting in the wind; so may a brick; so may mankind on earth. But it only becomes a system if we abstract from it some special holistic property, which we cannot explain except in terms of interactions within the whole. Without a specific statement of what holistic behaviour we have in mind, what interactions among what parts cause this behaviour, and how they do so, calling a thing a system is no more than saying: 'This is a pretty complicated thing, and I don't understand it very well.' 
THESE ARE KITS OF PARTS 
(WHICH MAKE) 
THIS KIT OF PARTS 
ABCDEFGHIJKLMNOPQRSTUVWXYZ 
(WHICH MAKES+) · 
THIS KIT OF PARTS (WHICH MAKES SENTENCES) 
Image from original publication. Reprinted by permission of Christopher Alexander. © Christopher Alexander. 
The idea that a system is an abstraction needs emphasis. Think of a flower as a system. If we want to understand the fact that the flower buds, and swells, and blooms - that we must certainly do by looking at the flower as a system. In this case it is the interaction among the parts which creates the behaviour of the whole. But the same flower has other properties which are not helped at all by thinking of the flower as a system: if it is used as a projectile, then its trajectory cannot be explained as a result of interactions among its parts, and if it is given as a gift, there is nothing that the flower does, no matter how complex the situation, that needs to be understood as a result of interactions among the flower's parts. The idea of a system is helpful only in understanding kinds of behaviour which result from interactions among parts. 

Furthermore, even though we call a thing a system when we try to view it as a whole, this does not mean that we ever really view the thing in its entirety. 
When we look at an airline from a systems point of view, we may focus on its scheduling - and we shall learn that because the airline only has a limited number of aircraft, the schedule of a flight from New York to Chicago turns out to be dependent on the schedule of another flight from Minneapolis to Salt Lake City. In this instance, we are looking at the airline 'as a whole", because we are looking at the interactions among parts, but we are not concerned with the last button on the last mechanic's cap. The notion of 'whole' refers only to the breadth of vision, not to the inclusion of detail: it is still abstract. Most often common language obscures this very badly. When we speak of the solar system, or a hi-fi system, or an airline system or of a plumbing system, the words are used in such a way as to suggest that the 'system' is synonymous with the objects. But just occasionally the word is used correctly, even in common language. For instance, when we speak of the Ptolemaic system as opposed to the Copernican system, in each of the cases the word 'system' is used correctly: it refers to an abstract way of looking at the interaction among earth, planets, sun and stars - not to the objects themselves. 

The discipline of abstraction has one drawback. Occasionally we are confronted with phenomena which are clearly the products of interactions - but the interactions are so complex that we cannot see them clearly, and we cannot make the effort of abstraction successfully. Take, for instance, the baffling complexity of a seagull landing, or of an ecstatic, screaming, laughing girl. In these cases a too rigid insistence on the idea that a system is an abstract model, might easily lead us to abstract out some facile inessential system - at the cost of the wonder which is really there. 

This is exactly what happens when a systems analyst looks at a building: he/she manages to describe the circulation, the acoustics, the heating and the load-bearing structure as systems - and fails to identify the most interesting human and social systems, because he/she can't describe them in explicit terms. 
The ways in which man has viewed the solar system have resulted in many ideas about its structure. A single set of objects may be thought of as a system in a number of different ways. Image from original publication. Reprinted by permission of Christopher Alexander. © Christopher Alexander. 

Thus, there is a second lesson to be learned. The first lesson said: don't call a thing a system unless you can identify the abstract system you are talking about. The second lesson says: learn the first lesson, but don't let it railroad you into making facile abstractions. 

When we are confronted with a complex thing, we often begin with nothing more than a feeling or a 'sense' that it functions as a system. Driven by this feeling, we then try, painstakingly, to abstract out just that holistic behaviour which seems essential, and those interactions which cause the behaviour. This is an active process. It begins with feeling, and sensing, and only turns to thinking later. Start with some aspect of life so interwoven that you feel in your bones it must be a system, only you can't state it yet – and then, once you can feel it clearly, try to pin the system down, by defining the holistic behaviour you are discussing and which interactions among which parts create it. But feel it clearly first, before you try to think it. 

The systems point of view is not neutral. It will change your whole view of the world. It will lead you to realise that the most important characteristics of human individuals are products of their interactions with other people. It will lead you to realise that the life of nations - though these nations may seem self-sufficient - is produced by interactions in the whole world, and that they only get their strength from their position in this larger whole. It will lead you to see that the health of cities is produced by interactions among interdependent parts, including houses, cafés and theatres, yes, but also equally including slums and graveyards. 

The system viewpoint is a modern, disciplined, version of the sense of wonder. It is that view of things which man takes when he becomes aware of oneness and wholeness in the world. 

3. A generating system is not a view of a single thing. It is a kit of parts, with rules about the way these parts may be combined. 

This is a different use of the word system from the first one. In colloquial English we often use the word system to mean 'a way to do something': that's what a betting system is; that's what the Montessori system is; that's what the democratic system is. 

Each of these systems is, at heart, a system of rules. A betting system tells you how to place your bets, the Montessori system lays down rules to be followed by children and teachers in nursery school, the democratic system of government lays down certain rules about the nature of representation, the choice of representatives and the conduct of elections. In all these cases, the rules are designed to generate things. A betting system supposedly generates winning bets, an educational system generates well-educated pupils, the democratic system supposedly generates freedom and good government. 

We may generalise the notion of a generative system. Such a system will usually consist of a kit of parts (or elements) together with rules for combining them to form allowable 'things'. The formal systems of mathematics are systems in this sense. The parts are numbers, variables, and signs like + and. The rules specify ways of combining these parts to form expressions, ways of forming expressions from other expressions, ways of forming true sentences from expressions, and ways of forming true sentences from other true sentences. The combinations of parts, generated by such a system, are the true sentences, hence theorems, of mathematics. Any combination of parts which is not formed according to the rules is either meaningless or false. 

A generating system, in this sense, may have a very simple kit of parts, and very simple rules. Thus the system of triangles which may be put together to form a square, is a generating system. Its rules generate all the ways of putting these triangles together to form a square. It is typical of a system that the rules rule out many combinations of the parts. Thus these triangles could be put together in an infinite variety of ways - but most of these ways are ruled out, because the outside perimeter is not a square and this thing is not connected. 

Another example of a generating system, is the system of language. Here we have rules at several different levels. At one level, the letters are the parts, and there are rules which govern the way that letters may be put together to form words. In English there could be no word beginning with Rx. The rules of phonology prohibit it. At another level, the words are themselves parts, and there are rules which govern the kinds of sentences which may be made from words. 

Perhaps the most interesting and important generating system in the world is the genetic system. Every animal in the animal kingdom is generated by a set of chromosomes specific to that animal. Each chromosome in turn is generated by four bases (like a necklace which uses only four kinds of bead). The four bases form a kit of parts which generates the chromosome. These chromosomes themselves provide the rules for building amino acids (another kit of parts), proteins from amino acids (another kit of parts), cells from proteins (another kit of parts) and then build the animal from cells. The kit of parts formed by the four bases, and their rules of combination, indirectly generates every animal there is. A building system is a generating system in this sense. It provides a kit of parts - columns, beams, panels, windows, doors which must be put together according to certain rules. 

4. Almost every 'system as a whole' is generated by a generating system. If we wish to make things which function as 'wholes' we shall have to invent generating systems to create them. 

There is a relationship between the two ideas of system which have been defined. Almost every object with behaviour that depends on some 'system as a whole' within the object, is itself created by a generating system. 

Take an obvious and simple case: a hi-fi system. Its purity of performance can only be understood as a product of the combined effect of all the various components, working as a whole. The same hi-fi system is also generated by a generating system: the kit of all the parts on the market, and the rules governing the electrical connections and impedance matching between these parts. 
To take a more complicated case: the railroad switch-yard. It plainly functions as a whole. In order to understand it as a device for breaking up and making trains, we must focus on the sequence of switches, and on the fact that the length of track in front of the switches depends on the length of track behind the switches and on the length of trains. At the same time, the switch-yard is also plainly generated by a generating system. The pieces of track, switches, couplings, cars, together with the rules for putting them together, form a kit of parts which generates properly functioning switch-yards. 

The most complicated case of all, and the clearest, is that of an animal. A landing seagull certainly needs to be seen as a system: so does almost everything else that seagulls do. At the same time, this seagull is created by a generating system: the genetic system. An animal is both something which needs to be seen holistically, and generated by a generating system. 

The relationship between holistic systems and generating systems is easy to understand. If an object has some holistic property caused by interaction among parts - then it is clear that these particular parts and these particular interactions will only come into being if the parts have very constrained relationships to one another. The object then, must be generated by some process which assembles parts according to certain constraints, chosen to ensure the proper interaction of these parts, when the system operates. This is exactly what a generating system is. 

The generating system need not be conscious (as in the case of the switch-yard), nor even always explicit (as in the genetic case). Sometimes the processes which make up the generating systems are integral with the object being formed - thus the candle flame is generated by chemical processes which are the same as those processes which then maintain the system's equilibrium and make up the interacting parts, when we view the flame as a holistic system. 

It is true then, that almost every 'system as a whole' is generated by a generating system. This axiom contains a remarkable lesson for designers. Man as a designer is concerned with the design and construction of objects which function as wholes. Most of the important properties a city needs to support life, for instance, are holistic properties. Our axiom means this: to ensure the holistic system properties of buildings and cities, we must invent generating systems, whose parts and rules will create the necessary holistic system properties of their own accord. 

This is a radical step in the conception of design. Most designers today think of themselves as the designers of objects. If we follow the argument presented here, we reach very different conclusion. To make objects with complex holistic properties, it is necessary to invent generating systems which will generate objects with the required holistic properties. The designer becomes a designer of generating systems - each capable of generating many objects - rather than a designer of individual objects. 

A final word of caution. As we have already seen, a building system is an example of a generating system. It is a kit of parts with rules of combination. But not every generating system necessarily creates objects with valuable holistic properties. The generating system which makes squares out of triangles is an example. It is a perfectly good generating system; yet the objects it produces do nothing: they have no holistic system properties whatever. In the same sense, those building systems which have so far been conceived make buildings, but they do not make buildings with any really important holistic system properties. In a properly functioning building, the building and the people in it together form a whole: a social, human whole. The building systems which have so far been created do not in this sense generate wholes at all. While it is inherent in the generating system of an animal that the finished animal will work as a whole, it is not inherent in any of today's building systems that the buildings they produce will work as social or human wholes. Creating building systems in the present sense is not enough. We need a new, more subtle kind of building system, which doesn't merely generate buildings, but generates buildings guaranteed to function as holistic systems in the social, human sense. 

Christopher Alexander, 'Systems Generating Systems', Architectural Design, December issue No 7/6, John Wiley & Sons Ltd (London), 1968, pp 90-1. Originally published in Systemat, a journal of the Inland Steel Products Company. Reproduced by permission of Christopher Alexander. Christopher Alexander. 
THE ARCHITECTURAL RELEVANCE OF CYBERNETICS 
Gordon Pask 

The discipline of cybernetics, of which Gordon Pask was a leading figure in the advancement of this intellectual domain, states, in principle, that systems are based on regulation, control, adjustment and purpose, filtered through means of feedback. The transdisciplinary domain, permeating such disparate fields as engineering, biology, sociology, economics and design, institutes a paradigm for thinking which emphasises circular reasoning, interrelating output, adaptation and self-organisation. Pask expresses these notions in this article, first published in Architectural Design (1969), as the underpinnings to the comprehension of architecture as a compilation of active systems, in contrast to the perception of a building as simply a static material object, where the engagement of the human is, most critically, lost. He locates a source for this distinction in the contrast between design formulation based on 'language", defining a finite, often historically based, set of possibilities being inherently limited by style, and one derived of environment and action, responsive to the prospects of behavioural changes. Cybernetics, intended as a unifying theory and related to architecture here by Pask, contributes to computational thinking a logic for perceiving architecture as an environmental, social and cultural device, and proposes the fundamental components of design processes which may calculate, determine and predict such systems. 

It is easy to argue that cybernetics is relevant to architecture in the same way that it is relevant to a host of other professions; medicine, engineering or law. PERT programming, for example, is unequivocally a 'cybernetic' technique and it is commonly employed in construction scheduling. Computer-aided design is a 'cybernetic' method and there are several instances of its application to architecture (for example, the WSCC's planning scheme in which the designer uses a graphic display to represent the disposition of structural modules on a grid and in which the computer summarises the cost effort consequences of a proposed layout). Of these cases the first (PERT programming) is a valuable but quite trivial application of cybernetics; the second is likely to have a far-reaching influence upon architectural design. But neither of them demonstrates more than a superficial bond between cybernetics and architecture. If we leave the matter at this level, then architects dive into a cybernetic bag of tricks and draw out those which seem to be appropriate. That is a perfectly reasonable thing to do, of course. But cybernetics and architecture really enjoy a much more intimate relationship; they share a common philosophy of architecture in the sense that Stafford Beer has shown it to be the philosophy of operational research. 

The argument rests upon the idea that architects are first and foremost system designers who have been forced, over the last 100 years or so, to take an increasing interest in the organisational (ie, nontangible) system properties of development, communication and control. Design problems were coped with as they cropped up, but for some time it has been evident that an underpinning and unifying theory is required. Cybernetics is a discipline which fills the bill insofar as the abstract concepts of cybernetics can be interpreted in architectural terms (and, where appropriate, identified with real architectural systems), to form a theory (architectural cybernetics, the cybernetic theory of architecture).' 

HISTORICAL ROOTS2 
In or before the early 1800s 'pure' architecture existed as an abstraction from the art of building. Its rules were essentially condensed statements of what could be observed by looking at builders working on a site, and by looking at buildings constructed during different periods and in different places. Architects added a modicum of engineering practice and of historical or aesthetic sensibility to their discipline and created new structures with stability and style. On the whole, their structures were judged, within 'pure' architecture, according to these canons. 
Even in those days, of course, architects were asked to solve problems entailing the regulation and accommodation of human beings; hence, to design systems. But, in a sense, their brief was quite narrow. The problems could all be solved by the judicious application of pure architectural rules. The form of the artefact (house, college or theatre) was largely determined by the quite rigid codes of architecture (dictating, for example, its acceptable whole-part relationships) and by the conventions of society or the individual practitioner. Speaking technically, there were well-accepted communication media for conveying instructions, directives and ideas (style manuals and so on). Further, there was a metalanguage for talking about these instructions, directives and ideas, for comparing them, criticising them and evaluating them (as in statements of stability or style). Indeed, when interpreted, the body of metalinguistic statements formed the theory of pure architecture. Consequently, architects did not need to see themselves as systems designers, even though they designed systems, and the evidence suggests that they did not do so.3 Instead the professional image was that of a sophisticated house, college or theatre builder. 

In the course of the Victorian era new techniques were developed too rapidly to be assimilated into pure architecture and new problems were posed and could no longer be solved by applying the rules of pure architecture, for example, make a 'railway station' or make a 'great exhibition'. The solution to such (in those days) outlandish problems clearly depends upon seeing the required building as a part of the ecosystem of a human society. Of course the problems were solved and the novel techniques were mustered for this purpose (Temple Meads, the Tropical House at Kew, the Crystal Palace). To my own taste the solutions are exceptionally beautiful.4 Nevertheless, they are individual and idiosyncratic solutions because, in the new context, there was no way of carrying on a general and critical discussion. Let us be clear about this point. There obviously was a great deal of discussion over IK Brunei, D Burton and J Paxton's use of glass and ironwork; technical discussion and aesthetic discussion. But nobody seems to have appreciated the full significance of their structures in the context of the architectural potentialities of the age, ie, as examples of system design. The reason is fairly obvious. Whereas the pure architecture of the early 1800s had a metalanguage, albeit a restrictive one which discouraged innovation, the new (augmented) architecture had not yet developed one. Another way of putting it is to say there was no theory of the new architecture.5 

ARCHITECTURAL SUB-THEORIES 
In place of a general theory there were sub-theories dealing with isolated facets of the field; for example, theories of materials, of symmetry, of human commitment and responsibility, of craftsmanship and the like. But (it is probably fair to say) these sub- theories developed more or less independently during the late 1800s. 

Naturally enough, each sub-theory fostered a certain sort of building or a certain sort of socio-architectural dogma; for example, Futurism. However, the point of immediate interest is that many of the sub-theories were system orientated; although they anticipated the invention of the word they were, in an embryonic sense, 'cybernetic' theories and the thinking behind them made a valuable contribution to the development of cybernetics as a formal science. 

ARCHITECTURAL FUNCTIONALISM AND MUTUALISM 
A structure exists chiefly to perform certain functions, for example, to shelter its occupants or to provide them with services. At this level, a 'functional' building is contrasted with a 'decorative' building; it is an austere structure, stripped of excrescences. But, the concept of functionalism can be usefully refined in a humanistic direction. The functions, after all, are performed for human beings or human societies. It follows that a building cannot be viewed simply in isolation. It is only meaningful as a human environment. It perpetually interacts with its inhabitants, on the one hand serving them and on the other hand controlling their behaviour. In other words structures make sense as parts of larger systems that include human components and the architect is primarily concerned with these larger systems; they (not just the bricks and mortar part) are what architects design. I shall dub this notion architectural 'mutualism' meaning mutualism between structures and men or societies. 

One consequence of functionalism and mutualism is a shift of emphasis towards the form (rather than the material constitution) of structures; materials and methods come into prominence quite late in the design process. Another consequence is that architects are required to design dynamic rather than static entities. Clearly, the human part of the system is dynamic. But it is equally true (though less obvious) that the structural part must be imaged as continually regulating its human inhabitants. 

ARCHITECTURAL HOLISM 
Once a rudimentary version of the functional/mutualistic hypothesis has been accepted, the integrity of any single system is questionable. Most human/structural systems rely upon other systems to which they are coupled via the human components. By hypothesis, there are organisational wholes which cannot be meaningfully dissected into parts. Holism is of several types: 

a.A functionally interpreted building can only be usefully considered in the context of a city (notice that the city is also functionally interpreted and, as a result, is a dynamic entity). 

b.A (functionally interpreted) structure, either a building or an entire city, can only be meaningfully conceived in the context of its temporal extension, ie, its growth and development. 

c.A (functionally interpreted) structure exists as part of an intention, ie, as one product of a plan. 

d.If (assumed dogma) man should be aware of his natural surroundings, then buildings should be wedded to or arise from these surroundings (Wright's organic thesis). 

It is a corollary of a, b and c that the structure of a city is not just the carapace of society. On the contrary, its structure acts as a symbolic control programme on a par with the ritual constraints which are known to regulate the behaviour of various tribes and which render this behaviour homeostatic rather than divergent. Hence, the architect is responsible for building conventions and shaping the development of traditions (this comment simply elevates the idea that a building controls its inhabitants to a higher level of organisation). 

EVOLUTIONARY IDEAS IN ARCHITECTURE 
Systems, notably cities, grow and develop and, in general, evolve. Clearly, this concept is contingent upon the functionalist/mutualist hypothesis (without which it is difficult to see in what sense the system itself does grow), though the dependency is often unstated. An immediate practical consequence of the evolutionary point of view is that architectural designs should have rules for evolution built into them if their growth is to be healthy rather than cancerous. In other words, a responsible architect must be concerned with evolutionary properties; he cannot merely stand back and observe evolution as something that happens to his structures. The evolutionary thesis is closely related to holism, type c, but it is a carefully specialised version of c as manifest in the work of the Japanese. 
SYMBOLIC ENVIRONMENTS IN ARCHITECTURE 
Many human activities are symbolic in character. Using visual, verbal or tactile symbols, man 'talks with his surroundings. These consist in other men, information systems such as libraries, computers or works of art and also, of course, the structures around him. 

Buildings have always been classified as works of art. The novel sub-theory is that structures may be designed (as well as intuited) to foster a productive and pleasurable dialogue. This way of thinking is most clearly manifest in connection with the literary art forms, notably Surrealism which relies upon a variety-(novelty-) producing juxtaposition of releasers and supernormal stimuli (evoking inbuilt emotive responses) within a thematic matrix. At the architectural level, this type of design appears in the vegetable surrealism of some of the Art Nouveau. But it reaches maturity in Gaudi's work, especially the Parque Güell which, at a symbolic level, is one of the most cybernetic structures in existence. As you explore the piece, statements are made in terms of releasers, your exploration is guided by specially contrived feedback, and variety (surprise value) is introduced at appropriate points to make you explore. 

It is interesting that Gaudi's work is often contrasted with functionalism. Systemically it is functionalism pure and simple, though it is aimed at satisfying only the symbolic and informational needs of man. 

THE MACHINERY OF ARCHITECTURAL PRODUCTION 
Just as a functionally interpreted building constitutes a system, so also the construction of this building is a system. The new techniques developed in the last century and the general mechanisation of production facilities led to sub-theories concerned with the achievement of forms (the most important centred around the Bauhaus) and these, in turn, restricted the forms that could be produced. 

THE WIDENING BRIEF 
As a result of these, essentially cybernetic, sub-theoretical developments, many architects wanted to design systems but, on the whole, they were expected to design buildings. To a large extent this is still (quite reasonably) true. All the same, there is a sense in which the brief given to an architect has widened during the last decades. 

In part this is due to a spate of problems for which no conventional solution exists (structures connected with aerospace developments, industry, research, entertainment, the use of oceans, etc). Here, the architect is in much the same position as his Victorian predecessor when asked to build a railway station. In part, however, the restraints have been relaxed because of the greater prevalence of system orientated thinking among clients and public sponsors. It is, nowadays, legitimate to enter the design process much earlier, even for a conventional project. For example, it is quite commonplace to design (or at least to plan) cities as a whole with provision for their evolution. A university need not be conceived as a set of buildings around a courtyard with living accommodation and lecture theatre. The educational system might, in certain circumstances, be spatially distributed rather than localised. In any case, architects are positively encouraged to anticipate trends such as the development of educational technology and to provide for their impact upon whatever structure is erected. By token of this the architect quite often comes into the picture at the time when a higher educational system is being contemplated, without commitment to whether or not it is called a university. The Fun Palace project, by Joan Littlewood and Cedric Price, was an early entry project of this type in the field of entertainment and it is not difficult to find examples in areas ranging from exhibition design to factory building. 

The point I wish to establish is that nowadays there is a demand for system orientated thinking whereas, in the past, there was only a more or less esoteric desire for it. Because of this demand, it is worthwhile collecting the isolated sub-theories together by forming a generalisation from their common constituents. As we have already argued, the common constituents are the notions of control, communication and system. Hence the generalisation is no more nor less than abstract cybernetics interpreted as an overall architectural theory. 

It would be premature to suggest that the necessary interpretation and consolidation is complete. But a creditable start has been made by a number of people; citing only those with whom I have personal contact, Christopher Alexander, Nicholas Negroponte, many students and ex-students from the AA School of Architecture and from Newcastle. 

STATUS OF THE NEW THEORY 
In common with the pure architecture of the 1800s, cybernetics provides a metalanguage for critical discussion. But the cybernetic theory is more than an extension of 'pure' architecture. As we noted somewhat earlier, pure architecture was descriptive (a taxonomy of buildings and methods) and prescriptive (as in the preparation of plans) but it did little to predict or explain. In contrast, the cybernetic theory has an appreciable predictive power. For example, urban development can be modelled as a self-organising system (a formal statement of 'evolutionary ideas in architecture") and in these terms it is possible to predict the extent to which the growth of a city will be chaotic or ordered by differentiation. Even if the necessary data for prediction is unavailable we can, at least, pose and test a rational hypothesis. Much the same comments apply to predictions in which time is not of primary importance; for instance, in predicting the influence of spatial and normative constraints upon the stability of a (functionally interpreted) structure. 

The cybernetic theory can also claim some explanatory power insofar as it is possible to mimic certain aspects of architectural design by artificial intelligence computer programs (provided, incidentally, that the program is able to learn about and from architects and by experimenting in the language of architects, ie, by exploring plans, material specifications, condensed versions of clients' comments, etc). Such programs are clearly of value in their own right. They are potential aids to design; acting as intelligent extensions of the tool-like programs mentioned at the outset. Further, they offer a means for integrating the constructional system (the 'machinery of production') with the ongoing design process since it is quite easy to embody the constraints of current technology in a special part of the simulation. However, I believe these programs are of far greater importance as evidencing our theoretical knowledge of what architecture is about. Insofar as the program can be written, the cybernetic theory is explanatory. 

SPECULATIONS 
It seems likely that rapid advances will be made in at least five areas guided by the cybernetic theory of architecture. 

1 Various computer-assisted (or even computer-directed) design procedures will be 

developed into useful instruments. 
Concepts in very different disciplines (notably social anthropology, psychology, sociology, ecology and economics) will be unified with the concepts of architecture to yield an adequately broad view of such entities as 'civilisation', 'city' or 'educational system'. 

There will be a proper and systematic formulation of the sense in which architecture acts as a social control (ie, the germ of an idea, mentioned under 'Holism', will be elaborated). 

The high point of functionalism is the concept of a house as a 'machine for living in'. But the bias is towards a machine that acts as a tool serving the inhabitant. This notion will, I believe, be refined into the concept of an environment with which the inhabitant cooperates and in which he can externalise his mental processes, ie, mutualism will be emphasised as compared with mere functionalism. For example, the machine for living in will relieve the inhabitant of the need to store information in memory and the need to perform calculations as well as helping out with more obvious chores like garbage disposal and washing up dishes. Further, it will elicit his interest as well as simply answering his enquiries. 

Gaudi (intentionally or not) achieved a dialogue between his environment and its inhabitants. He did so using physically static structures (the dynamic processes depending upon the movement of people or shifts in their attention). The dialogue can be refined and extended with the aid of modern techniques which allow us to weave the same pattern in terms of a reactive environment. If, in addition, the environment is malleable and adaptive the results can be very potent indeed. I have experimented along these lines myself but the work of Brodey and his group at the environmental ecology laboratory is a project on a much more impressive scale. As a broad statement of what is going on, a computer controls the visual and tactile properties of environmental materials (which are available in sufficient diversity for most architectural purposes). These materials contain sensors, tactile or visual as the case may be, which return messages to the computer at several levels of generality. In the absence of a human inhabitant, the feedback leads to stabilisation with respect to certain preprogrammed invariants (for example, that a body of material shall maintain mechanical stability and occupy a prescribed value), and to a search process in which the material actively looks for signs of a human being in contact with it. If there is a human being in the environment, the computer, material and all, engages him in dialogue and, within quite wide limits, is able to learn about and adapt to his behaviour pattern. There is thus one sense in which the reactive environment is a controller and another in which it is controlled by its inhabitants. 

A SIMPLE CYBERNETIC DESIGN PARADIGM 
In the context of a reactive and adaptive environment, architectural design takes place in several interdependent stages. 

Specification of the purpose or goal of the system (with respect to the human inhabitants). It should be emphasised that the goal may be and nearly always will be underspecified, ie, the architect will no more know the purpose of the system than he really knows the purpose of a conventional house. His aim is to provide a set of constraints that allow for certain, presumably desirable, modes of evolution. 

Choice of the basic environmental materials. 

Selection of the invariants which are to be programmed into the system. Partly at this stage and partly in (2) above, the architect determines what properties will be relevant in the man-environment dialogue. 

Specification of what the environment will learn about and how it will adapt. 

Choice of a plan for adaptation and development. In case the goal of the system is underspecified, as in (1), the plan will chiefly consist in a number of evolutionary principles. 

Of course, this paradigm applies to systems which adapt over rather short time intervals (minutes or hours). In contrast, the adaptation in a project such as the Fun Palace system took place over much longer time intervals (for instance, an eight-hourly cycle and a weekly cycle formed part of the proposal). Depending upon the time constraints and the degree of flexibility required, it is more or less convenient to use a computer (for example, the weekly cycle is more economically programmed by a flexible office procedure). But exactly the same principles are involved. 

Urban planning usually extends over time periods of years or decades and, as currently conceived, the plan is quite an inflexible specification. However, the argument just presented suggests that it need not be inflexible and that urban development could, perhaps with advantage, be governed by a process like that in the dialogue of a reactive environment (physical contact with the inhabitants giving place to an awareness of their preferences and predilections; the inflexible plan to the environmental computing machine). If so, the same design paradigm applies, since in all of the cases so far considered the primary decisions are systemic in character, ie, they amount to the delineation or the modification of a control program. This universality is typical of the cybernetic approach. One final manoeuvre will indicate the flavour of a cybernetic theory. Let us turn the design paradigm in upon itself; let us apply it to the interaction between the designer and the system he designs, rather than the interaction between the system and the people who inhabit it. The glove fits, almost perfectly in the case where the designer uses a computer as his assistant. In other words, the relation 'controller/controlled entity' is preserved when these omnibus words are replaced either by 'designer/system being designed' or by 'systemic environment/inhabitants' or by 'urban plan/city'. But notice the trick, the designer is controlling the construction of control systems and consequently design is control of control, ie, the designer does much the same job as his system, but he operates at a higher level in the organisational hierarchy. 

Further the design goal is nearly always underspecified and the 'controller is no longer the authoritarian apparatus which this purely technical name commonly brings to mind. In contrast the controller is an odd mixture of catalyst, crutch, memory and arbiter, These, I believe, are the dispositions a designer should bring to bear upon his work (when he professionally plays the part of a controller) and these are the qualities he should embed in the systems (control systems) which he designs.10 

NOTES 
1 Very similar comments apply to engineering, since engineers, like architects, prescribe artefacts. Surely, also, some engineers make use of a cybernetic theory. But the requirement is not so ubiquitous in engineering; nor is the impact of cybernetics so great because a creditable body of engineering theory, a predictive and explanatory theory, existed long before the cybernetic concepts came along as daring innovations. Moreover, while all architects design systems that interact closely with human beings and societies, most engineers (there are obvious exceptions) are not forced to do so. Human interaction is a major source of difficulties which can only be overcome by cybernetic thinking. 

2 The choice of a historical origin is somewhat arbitrary and depends upon the author's emphasis. For example, Alexander, preoccupied with the logic of form, traces essentially cybernetic concepts back to Lodoli and Laugier. In the present article I am anxious to follow the pragmatic development of cybernetic ideas and to see them emerging in the history of modem architecture. 

3 There are two important sorts of exception: 

(i) Architects of genius, with a breadth of vision that impels them to see things in a systemic and interdisciplinary fashion. They have existed over the years: Sir Christopher Wren and Sir John Soane, for example. 

(ii) Men like John Nash, whose talents lay in conceiving an urban development as a functional and aesthetic whole. But, within the tenets of the early 1800s, such men are probably 'organisers with a vision', rather than 'architects'. 

4 I have chosen these examples partly because they are well known in the textbooks but mainly because I am impressed by their systemic qualities and the way in which they convey their designer's purpose to the occupant. Two of them still exist. I just recall the Palace. Even in its tawdry reincarnation it was a remarkable structure. Since it was one of the first instances of a prefabricated building it also counts as a piece of system design at the engineering level. 

5 Lack of an adequate metalanguage was not the only factor. As Nikolaus Pevsner points out, the engineers and the artists pursued divergent paths of development more or less in conflict with one another and this accounted for at least some of the architectural idiosyncrasy. However, if a metalanguage had existed, then the synthesis of the present century could have been achieved much earlier. 

6 Clearly, in other respects, it would be uncomfortably prickly to live in. 

7 The impact of cybernetics upon architecture is considerable just because the theory does have much more predictive power than pure architecture had. Cybemetics did relatively little to alter the shape of biochemistry for instance, because although these concepts are bound up with everything from enzyme organisation to molecular biology, the discipline of biochemistry already had a predictive and explanatory theory of its own. I made the same point for engineering in an earlier footnote. 

8 I have the work of Negroponte's group (see pp 78-85) chiefly in mind, though there are other exemplars. 

9 For example, the Colloquy of Mobiles project and the Musicolour system, A Comment, A Case History and a Plan in Computer Art, (ed) Jasia Reichardt. 

10 The cybernetic notions mooted in this article are discussed in An Approach to Cybernetics, Hutchinson (London), 1961 (paperback 1968) and, in a lighter vein, in 'My Predictions for 1984' in Prospect, The Schweppes Book of the New Generation, Hutchinson (London), 1962. 

Gordon Pask, "The Architectural Relevance of Cybernetics', Architectural Design, September issue No 7/6, John Wiley & Sons Ltd (London), 1969, pp 494-6. John Wiley & Sons Ltd. 
TOWARDS A HUMANISM THROUGH MACHINES 
Nicholas Negroponte 

With the emergence of the computer and computerised methods in design in the 1960s, Nicholas Negroponte, a leading exponent of the infusion of computer processes to architecture, focused particularly on the relationship between designer, computer and the media of instruction, feedback and intelligence. His conception of what he calls 'computer-aided' is more aligned with a computational approach, proposing the transformation of the computer from the 'subservient machine to the intelligent environment, described in Negroponte's Reflections on Computer Aids to Design and Architecture (1975). The context for Negroponte's proposal of an 'Architecture Machine' is to counter the designer's tendency to, within his own methods, eschew the complexity of large-scale problems, and generalises the specificity of small-scale concerns. As described in this text from an issue of Architectural Design (1969), the dialogue between designer and machine functions only through recursive exchange and reply, moving towards specificity of procedures, conditions and information. While Negroponte refers to the physicality of computers, their components and circuitry, the notion of transparency, exchange and contextual specificity are all relevant aspects of a computational approach. Of particular importance is the consideration that the language of a dialogue between man and machine is particularly foreign to that of conventional design processes. This accentuates the distinction between computational processes and computerised procedures, seen basically as simple translations of standardised means. 

Given that the physical environment is not in perfect harmony with Everyman's life style; given that architecture is not the faultless response to human needs; given that the architect is not the consummate manager of form and use; let us consider the evolution of physical environments. In particular let us consider an evolution aided by the use of a specific class of machines, as Warren McCulloch calls them, ethical robots. In the context of architecture, we shall call them Architecture Machines. 

There are three possible ways of having machines assist the design process: 

1 Current procedures can be automated, thus speeding up and reducing the cost of 

existing practices. 

2 Existing methods can be altered to fit within the specifications and constitution of a machine, where only those issues are considered that are supposedly machine- compatible. 

3 The process, considered as being evolutionary, can be introduced to a mechanism (also considered as evolutionary), and a mutual training, resilience and growth can be developed. 

We shall consider only the third possibility and shall treat the problem as the intimate association of two dissimilar species: man and machine; and two dissimilar processes: design and computation. We shall further define our concern as the acquaintanceship of two intelligent systems, the architect and the Architecture Machine. By virtue of ascribing intelligence to an artefact or the artificial, the partnership is not one of master (smart, leader) and slave (dumb, follower), but rather of two associates which each have the potential for self-improvement. 

Imagine a machine which could respond to local situations in the physical environment (a family that moves, a residence that is expanded, income that decreases). Such a device could report on and concern itself specifically with the unique and the exceptional. In effect, it would concentrate on the particulars. The human designer cannot do this. He cannot accommodate the particular; he obliges the general. Britton Harris suggests, 'He is forced to proceed in this way because the effectuation of planning requires rules of general applicability and because watching each sparrow is too troublesome for any but God'. The reader surely needs little reminder of the results. 

PRELUDE TO AN ARCHITECT-MACHINE DIALOGUE 
Consider that you are in a foreign country, do not know the language, and are in desperate need of help. At first your hand movements and facial expressions carry most of your meaning to the silent observer, Your behaviour uses a language of gestures and strange (to the observer) utterances to communicate your purpose. The puzzled listener searches for bits of content he can understand and link to his own language. You react to his reactions and a language of pantomime begins to unfold. This new language has evolved from the mutual effort to communicate. Returning to the same person a second time, with a new need, you will find that the roots of a dialogue already exist. But this second conversation might be gibberish to a third party observing the exchange for the first time. A designer-to-machine introduction should be a similar linguistic evolution. Each should track the other's design manoeuvres, evoking a rhetoric that cannot be anticipated. The event is circular since the designer-machine unity provokes a dialogue and the dialogue promotes a stronger unity. This progressively intimate association of two dissimilar species is the symbiosis. It evolves through mutual training - in this case, through the dialogue. 

A man-machine dialogue has no history. The historical and presently antagonised mismatch (non-dialogue) between man and machine has generated a great deal of preoccupation for it. In less than a decade, the term 'man-machine communication' has passed from concept to cliché to platitude. 

The theory, however, is important and straightforward: in order to have a cooperative interaction between a designer of a certain expertise and a machine of some scholarship, the two must be congenial. They must, for example, share the labour of establishing a common language. Thus a designer, when addressing a machine, must not be forced to resort to machine-oriented codes. And in spite of computational efficiency, the paradigm for fruitful conversations must be machines that can speak and respond to natural language. 

With direct, fluid and natural man-machine discourse, two former barriers between architects and computing machines could be removed. First, the designers using computer- aided design hardware would not have to be specialists. With natural communication, the 'this is what I want to do' and 'can you do it?' gap can be bridged. The design task would no longer be described to a knobs and dials person to be executed in his secret vernacular. Instead, the job would be formulated and executed in the designer's own idiom, with simple negotiations. A vibrant stream of ideas could be directly channelled from the designer to the machine (and back). 

The second obstruction overcome by such close communion would be the potential of re-evaluating the procedures themselves. In a direct exchange, the designer could exercise his proverbial capriciousness. At first, a designer might have only a meagre understanding of his specific problem and thus require machine tolerance and compatibility in his search for the consistency among criteria and form and method, between intent and purpose. The progression from visceral to intellectual could be articulated in provisional statements that converge on both design and methods. 

But the tête-à-tête must be more direct and fluid; it is gestures, smiles and frowns which turn a conversation into a dialogue. In an intimate human-to-human dialogue, hand-waving often carries as much meaning as text. The 'manner carries cultural information; the Arabs use their noses, the Japanese nod their heads. Customarily, in machine communication studies, such 'manners' are ignored and frequently are referred to as 'noise'. But such silent languages are not noise; Warren Brodey and Nilo Lindgren submit that a dialogue is composed of 'whole body involvement with hands, eyes, mouth, facial expressions using many channels simultaneously, but rhythmised into a harmoniously simple exchange'. 

 

Imagine a machine that could follow your design methodology and at the same time discern and assimilate your conversational idiosyncrasies. This same machine, after observing your behaviour, could build a predictive model of your conversational performance. Such a machine could then reinforce the dialogue by using the predictive model to respond to you in a manner that is in rhythm with your personal behaviour. This dialogue would be so intimate (even exclusive) that only mutual persuasion and compromise would bring about perceptions and ideas ideas, in fact, unrealisable by either converser alone. In such a symbiosis, it would not be solely the designer who would decide when the machine is relevant. 

The overlaying of a specific design character upon a generalised machine is not fanciful; computer programs exist that illustrate some primitive attempts. An anonymous machine, after identifying a speaker, can transform itself into an exclusive apparatus that reflects previous encounters with that speaker. The extent of the metamorphosis depends on the length of acquaintance. At the onset of the partnership, the machine gathers gross features; later it avails itself of subtleties. 

It might be argued that we are proposing the creation of a design machine that is an extension of and in the image of a designer who, as he stands, has already enough. error and fault. However, we have indicated that the maturation would be a reciprocal ripening of ideas and ways. At first, jobs in which the man is particularly inept would stimulate a nontrivial need for cooperation. Subsequently, each interlocutor would opt out of situations notably clumsy for his constitution, while at the same time he would pry into issues which were originally outside his scope of concern (or the concern of his profession). Eventually, a separation of the parts could not occur; the entire 'symbiotic' system would be, as Gordon Pask described, 'an artificial intelligence that cannot be partitioned'. It would be computer-aided design. 

COMPUTER-AIDED VERSUS COMPUTERISED 
'Computerised' operations are too often misnamed 'computer-aided'. The computerised/ computer-aided distinction is too often confused with or solely embodied in the mode of machine usage. 

The conventional mode of computer usage for the past 20 years, 'batch processing", entails a computation centre to which a user delivers a 'program' (a deck of cards, magnetic tape, paper tape) to be 'run'. Then several hours or days later the user returns to receive his 'output'. More recently, real-time computation depending upon 'time-sharing' techniques allows the user a prompt machine response and permits terminals (usually teletypes) to reside in the office or at home. These terminals are connected to the large central machine, and they can be interconnected with each other. The rapid switching of users' programs in and out of the large machine provides each user with the illusion of a dedicated machine and permits him continual use of his terminal. 

It is commonly suggested that the online nature of the interaction in a time-sharing system is in itself a dialogue and transforms computerised procedures into computer- aided ones. This is simply not true. For example, let us suppose you desire the average apartment-to-parking-space distance for some design project. In a batch processing mode (presuming the program exists), you supply as data the description of your design and the answer returns hours later indeed a computerised procedure. On the other hand, in a 'real-time environment, the project description resides in the machine, and you simply type on your teletype terminal the apartment-to-parking distance command. But just because the answer comes back in three seconds rather than three days, 'computerised' does not become 'computer-aided". It simply becomes more convenient. Computer-aidedness demands a dialogue; events cannot be merely a fast-time manifestation of cause and effect. 

Online communication, therefore, is not a sufficient (though necessary) condition for a computer-aided environment. Computer-aided design requires three further features: mutual interruptibility for man and for machine, local and dedicated computing power within the terminal and a machine intelligence. 

Interruptibility gives a dimension of interaction that allows the process, as well as the product, to be manipulated. In a computer-aided system, the machine may interrupt the user and present unsolicited information - for example, that the cost of his low-income housing project is $38 per square foot. The apparent high cost might have been due to inadequacies in the original estimating routine provided for the computer; for instance, substantial indirect savings might have been overlooked. In this case, the designer could tamper with the estimating routine and incorporate hitherto neglected parameters. Thus the architect might welcome the remark, ignore it, or take offence and request that such interludes of finance be restricted. 
URBAN5, a primitive effort at man-machine dialogue useful in architectural design.(Illustration is from a motion picture, URBANS, by Professor Negroponte in cooperation with Leon B Groisser.} Image from original publication. Reprinted by permission of MIT Technology Review. © MIT Technology Review. 
Unfortunately, the present time-sharing philosophy fosters a cause-and-effect conversation. Time-sharing assumes that a designer's explicit manipulations will occupy between 1 and 10 per cent of any sitting; the remaining time represents his deliberations and distractions. Each user's moments of contemplation are in effect another user's instants of computation. A designer can interrupt his own program, but a routine cannot easily interrupt its partner-in-thought. This is because, in order to leave the computational utility available for other users, each routine resides in the machine only when explicitly called into service by its particular user. In other words, the user's machines are encouraged to listen, but not to interrupt. 

To retain assets of 'time-sharing', while avoiding the anathema of batch processing, and to acquire mutual interruptibility, we must adjust the allocation of computing power. Some information processing power and a certain manipulative and storage capacity must be transferred to the terminal that was originally a teletype transmission and receptive device. This semi-autonomous terminal (possibly portable) would become a small computer that would be a 'machine in residence'. An Architecture Machine would be such a machine. The designer would speak directly to this satellite machine. In turn, this small remote computer would interactively converse with larger parent machines. (Sending out work to a central mechanism would not be apparent to the designer; his Architecture Machine would elect this course for reasons of speed, memory, information, or all three.) It is the Architecture Machine in residence, located in habitation with the designer, which would undergo the personalisation. It would be composed of additive and subtractive pieces of hardware as determined by the discipline of its partner. The local consortium of parts would do the interrupting, the dialoguing and the evolving. Observe that the interrupting and the re-interrupting would be dependent on the nature of the designer's activities, on the context of his efforts. Through familiarity with the specific designer's idiosyncrasies, the appropriateness of the machine's interruptions can be suitably reinforced by context; this is the inception of an intelligent act. 

A mechanical partner, as we have suggested, must have intelligence. Customarily, computer-aided design studies and intelligent automata studies have been antipodal efforts. On the one hand, we are told to render unto each their respective design functions and talents: man (intelligent) thinks and the machine (dumb, fast) calculates. On the other hand, we are told that, 'Anything you can do, a machine can do better'. The two outlooks are not necessarily contradictory. There is a real issue whether machine intelligence can be independent of human intelligence. In computer-aided design, only the combination of mechanical amplification and mechanical imitation will validate the dialogue. The dialogue will evolve an intelligence, this intelligence will stimulate a more profound dialogue, which in turn will promote further intelligence, and so on. Furthermore, the concurrence of 'extended designer' and 'artificial designer will force a design redundancy and an overlapping of tasks which are necessary for the understanding of intricate design couplings. Perpetual cross-examination of ideas by both the man and the machine will encourage creative thought that would otherwise be extinguished by the lack of an antagonistic (thus challenging) environment. Computer-aided design concerns an ecology of mutual design complementation, augmentation and substitution. 

VIS-À-VIS MACHINE INTELLIGENCE 
Intelligence is a particularly difficult behaviour to emulate in machines because of its extreme dependence on context time, locality, culture, etc. A sophisticated set of sensors, effectors and processors is needed to view the real world (directly or indirectly) and to discern changes in meaning brought about by changes in context - in other words, to be intelligent. 

For example, the meaning of a metaphor (in the physical environment, in a story, in a painting) is conveyed through context, and the assessment of such meaning is an example of an intelligent act. (Note that a literary metaphor characterises the era and the culture in which it was written.) One might judge a machine's intelligence (not necessarily its maturity, wisdom or knowledge) by its ability to appreciate a joke – a joke being a funny story with a punch line that is an about-face in context. As humans we exhibit an intelligence by tracing back through previous metaphors of the tale and deriving pleasure from the new meanings revealed and brought on by a shift in context. (Note that people of different cultures have difficulty in understanding each other's jokes.) 

Some architects might propose that machines cannot design unless they can think, cannot think unless they want, and cannot want unless they have bodies and, since they do not have bodies, they therefore cannot want, thus cannot think, thus cannot design - quod erat demonstrandum. The argument, however, is usually an emotional issue rather than a logical conclusion. Nonetheless, the reader must recognise (if he is a 'machine-intelligence' enthusiast) that theories on machine intelligence can, at this time, best be supported with such examples as computers playing a superb game of checkers and a mediocre game of chess. And furthermore, architecture, unlike checkers (with fixed rules and a fixed number of pieces) and much like a joke (determined by context), is the croquet game in Alice in Wonderland, where the Queen of Hearts (society, technology, economics) keeps changing the rules. 

Let us not be misled. We are not interested in a machine that will simply parrot a human designer, nor are we interested in a machine that will have an autonomous existence by which to mimic and replace an architect. An Architecture Machine will feature a dependence. An artificial intelligence is in fact an interdependence. 

You might summarise the proposition here as being: for machines to contribute to an environmental humanism, they must have a 'natural dialogue with a human designer, natural because they need his metaphors and natural because they need his ideas unmutated. The dialogue, in turn, must reside within a computer-aided (not computerised) system. The system, consequently, must include a 'resident processor, some real-world sensors and effectors, and an intelligence. 

For further reading on computer processes in design and architecture see Nicholas Negroponte (ed), Reflections on Computer Aids to Design and Architecture, Petrocelli/Charter, New York, 1975. 
Nicholas Negroponte, 'Towards a Humanism Through Machines', Architectural Design, September issue no 7/6, John Wiley & Sons Ltd (London), 1969, pp 511-12. Originally published by Technology Review Inc in Technology Review, vol 71, no 6, April 1969. Technology review by Nicholas Negroponte Copyright 1969. Reproduced with permission of MIT Technology Review in the format Other book via Copyright Clearance Center. © 2002 MIT Technology Review. 
A NEW AGENDA FOR COMPUTER-AIDED DESIGN 
William ] Mitchell 

In this extract from the introduction of the book The Electronic Design Studio (1990), William Mitchell establishes a succession from earlier concepts of computational design - the primary break focusing on the moment at which information arises within a computational process. Historically, processes had been executed to exact a solution from a specific and complex set of initial inputs. In this instance, the information exists a priori. As Mitchell points out, this works in a disjunctive manner with a design paradigm which uses search and iteration to establish specificity, rather than preset determinism. Vitality in computation depends upon malleable functions and operations which deduce information rather than simply dispense it. Mitchell cites this as a critical characteristic for any computational procedure. This also reflects into Mitchell's consideration of the means for terminating a computational procedure. Mitchell addresses the contrast between a purely analytical approach and the more circuitous nature which underlies architectural design. With either case, the procedures of a computational process, the design system, respond to the shifting, evolving, contradicting and coinciding nature of architecture, the designed system, as dynamic in shape, material and function. 

Design is the computation of shape information that is needed to guide fabrication or construction of an artefact. This information normally specifies artefact topology (connections of vertices, edges, surfaces and closed volumes), dimensions, angles and tolerances on dimensions and angles. There may also be associations of symbols with subshapes to specify material and other such properties. 

The process of design takes different forms in different contexts, but the most usual computational operations are transformations (unary operations) and combinations (binary operations) of shapes in a two-dimensional drawing or a three-dimensional geometric model. An initial vocabulary of shapes, together with a repertoire of shape transformation and combination operators, establishes the shape algebra within which the computation takes place. 

The computation terminates successfully when it can be shown that certain predicates are satisfied by a shape produced by recursively applying the transformation and combination operators to the initial vocabulary. These predicates are usually stated in symbolic (verbal or numerical) form. Thus determination of whether a predicate is satisfied usually involves producing a numerical or verbal interpretation of a drawing, then deriving inferences from this interpretation by applying rules or formulae. 

This definition may seem provocatively reductionist and to leave little room for creativity. But I shall argue, in this paper, that taking a computational view of design can reveal precisely where creativity enters and why we intuitively take it to be characteristic of all but the most trivial design processes. In particular, I shall focus on the roles of ambiguity and discontinuity in shape interpretation, instability in the rules for carrying out shape computations, and nonmonotinicity in critical reasoning to determine whether or not a design proposal is complete and satisfactory. My examples will be taken from the sphere of architecture, but I shall provide a general theoretical treatment that applies to many other areas of design as well. 

FORMALISING DESIGN RULES 
The earliest idea about computational treatment of design rules was that they could be expressed as procedures which would accept design requirements as input and produce appropriate shape information as output. Such a procedure might, for example, accept a list of rooms together with area and adjacency requirements and produce a plan that satisfied those requirements. This was consistent with the dogma of early Modernism that architectural design was a matter of satisfying an established set of requirements as closely and efficiently as possible, and it was also consistent with early, procedurally oriented approaches to programming in languages like Fortran. 

Much useful research was done within this paradigm, and sometimes this resulted in programs that actually worked. In the mid-1970s, for example, Philip Steadman, Robin Liggett and I published an efficient, rigorous procedure for producing small rectangular floor plans that satisfied adjacency, area and dimensional constraints, and that minimised total floor area (Mitchell, Steadman, and Liggett 1976; Steadman 1983). 

A fundamental limitation of this approach, however, is that it requires design rules to be expressed in a very cumbersome and artificial way - strictly in terms of the constructs provided by procedural programming languages. A clever programmer can certainly do this, but the architectural content of the resulting code is very difficult to comprehend, and this makes it difficult to subject the rule system to critical scrutiny. Furthermore, the rules are inextricably intertwined with information that specifies strategy for applying them, so it is usually very difficult to isolate and modify them. 

But human designers learn. They see the work of others, become sensitive to new issues, become aware of new possibilities, respond to criticism and constantly modify the rules that they apply. We usually call this stylistic evolution. It may take the form of sweeping stylistic change, or that of refinement and elaboration of an established style. In any case it is an essential component of creative design, and we must provide for it in CAD systems that are seriously intended to support creative design. Further evidence for this need is provided by the experience of those who have attempted to implement rule- based design systems, and have typically found that they must spend a great deal of time tinkering with the rules to get them to produce the right sorts of results. 

Adoption of a more modular and declarative style of programming is an important step in the right direction. If rules are expressed as productions, for example, they are decoupled from control information, and it becomes convenient to add rules, delete rules and modify rules. It is possible to adapt and tune a rule system in incremental, experimental fashion. Some sort of general inference engine can be used to apply the rule system in whatever form it currently exists. 

There are now many ways to set up production systems that encode design rules. You can build them in Lisp, or you can take advantage of Prolog's high-level facilities for processing rules expressed in the format of first-order logic. Numerous shells for knowledge-based systems provide facilities for expression of rules in if... then format. A shape grammar is also a production system, with the additional advantage that it expresses rules directly in terms of shapes rather than in terms of some symbolic calculus. The following rule, for example, shows (opposite) one thing you can do with a square - one possible compositional move. 

By sketching you can rapidly discover other things to do with squares some of them interesting and some of them not. By remembering the interesting ones you can establish a grammar for composing squares, then you can go on to explore the language specified by that grammar. If you think of some more interesting things you can add corresponding rules to the grammar, and thus structure a new terrain for design exploration. (See Knight (1986) for a detailed discussion of the way that changing the rules changes a designer's output.) 

Shape rules can be thought of as stereotyped responses to stereotyped situations, and therefore as constraints on the free play of the imagination. This was certainly the position of the Romantics, who opposed the hegemony of classical rules. But the imagination needs something to play with. Though the consequence of applying a given rule to a given shape is plain to see, the consequences of recursively applying a rule system to that shape can be very surprising. The rules of a grammar are not limiting prescriptions, but tools for constructing a path from the known to the unknown - tools that can be changed if they do not seem to get you to the right place. 

What sorts of shape rules should a designer use to structure paths into the unknown? Useful and interesting sets of shape rules are, I think, those that yield a lot for a little. Any set of designs can be generated in an uninteresting way by a grammar that has a rule to produce each of the designs in the set, but it is no advantage to a designer to know this sort of grammar. It is advantageous, however, to know a concise grammar, with few but powerful rules, that specifies an extensive and interesting set of designs. It has been shown that such concise yet powerful grammars can be provided for interesting and important bodies of architectural design work (Stiny and Mitchell 1978; Mitchell 1989). 

Graphically expressed shape rules are much easier for a designer to understand and criticise than, say, pages of Prolog, so there is good reason to suggest that shells for building knowledge-based design systems should take the form of shape grammar interpreters that can be programmed graphically, and that allow quick and easy modification of rules. Some interesting prototype systems of this sort have been developed (Mitchell, Liggett, and Tan 1989; Nagakura 1989; Tan 1989). 

The interface details of systems for building and applying sets of design rules are less important, however, than the general point that we should focus on the expression of design rules in declarative, modular, easily understood and easily modifiable format. In other words, we should think of design systems as open, flexible, constantly evolving knowledge-capture devices rather than static collections of familiar tools and dispensers of established wisdom. When we can do this I think we will see the emergence of design systems that do not just mechanically assemble banalities, but that have real style and flair. 

Reprinted by permission of MIT Press. MIT Press. 
KNOWING WHEN A DESIGN IS FINISHED 
There is a famous paradox in Plato's Meno which seems to suggest the impossibility of truly creative thought. The protagonist asks Socrates how it is ever possible to attain new knowledge. If you know what you are looking for, he suggests, it will not really be new to you when you find it. But if you don't know what you are looking for, if you can't put forth something definite as the subject of inquiry, you will have absolutely no way of ever knowing that you have found it, and the search will never terminate. Turing provided us with a modern resolution. We can establish a process for generating information to consider (propositions, or numbers, or shapes), and we can specify a test which determines whether a given piece of information is that for which we are looking - the solution to our problem. When we have generated some information that passes the test we have solved the problem, and the process terminates. More specifically, in the context of design, we can set forth the requirements that a shape must satisfy, then attempt to instantiate a shape that does so. The design is successfully completed when we can show that we have a shape which satisfies the requirements. We may never have seen this shape before we make it explicit for consideration: it may be genuinely new and surprising to us. 

In the early days of CAD this was seen as a matter of stating programmatic and technical requirements, then applying analysis procedures to a database to determine whether or not these requirements were satisfied (Maver 1988). A floor plan layout program, for example, might incorporate procedures to test a proposed plan for compliance with given room adjacency and area requirements. This approach was a very direct extension of the traditions of engineering analysis that had begun with Galileo's analysis of designs, for a cantilever beam to determine whether they would prove strong enough. 

Clearly this approach was, and continues to be, very useful. But analysis procedures certainly have not supplanted human critics. Why? 

One reason is that architectural designs (as opposed to some sorts of engineering designs) often have extraordinarily complex entailments. Buildings perform subtle economic, social and cultural roles, and these can only be understood adequately by reasoning about them in the light of extensive economic, social and cultural knowledge. It might be possible, in principle, to build artificial critics that could apply such broad- based knowledge to produce detailed, useful evaluations of design proposals, but no such systems have emerged so far. The evaluations and appraisals that CAD systems do successfully produce are extremely narrowly focused. 

A second problem may be thought of as a special case of the well-known 'frame problem' in artificial intelligence (Dennett 1984). How do you know what possible consequences of a design proposal should be given attention and explored? There are indefinitely many ways that a design does not fail, but most of these are irrelevant. Why doesn't a critic spend time carefully demonstrating, to everybody's satisfaction, that painting the walls yellow won't cause the bathtub to explode? This is a perfectly valid line of inference, but not one that is worthwhile to pursue. Clearly the critic has some way of knowing which parts of the indefinitely extensive entailment of a design are worth attention and which are not, but precisely how the critic knows this is a very difficult (and I think unsolved) problem. 

It might seem that the issue could be settled by setting forth an a priori agenda of relevant issues for consideration. In well-defined problems, such as chess problems and theorem-proving problems, it is fairly straightforward to provide a complete specification of what is required. But in creative design, characteristically, this is impossible: issues and solution criteria are evoked contextually as the design takes shape (Reitman 1965; Akin 1986). You do not necessarily know what you want until you see what you can have. Nor do you know what to avoid until you have seen some failures. So the critical agenda may evolve and change as the design possibilities structured by shape rules are made explicit, and may not, in fact, stabilise until the point of termination has almost been reached. The most original and incisive criticism is often so because it departs from established agendas, establishes new issues for consideration, and reshapes the discourse. 

Furthermore, different presumptions about what a thing is meant to be, or needs to be, or just might be, lead to different diagnoses of what it lacks. If you see that a shape might be a duck you can, from the rule All ducks have feathers, derive the criticism that the proposal lacks feathers, and suggest that feathers must be added to yield a complete and satisfactory design. But if you are more interested in its potential rabbithood you can, from the rule No rabbits have feathers derive the critical conclusion that the addition of feathers would not be an appropriate way to complete the design. In general, a designer's sketch is an incomplete, ambiguous and possibly inconsistent depiction of a possible artefact. Thus it allows contradictory observations to be made and contradictory critical inferences to be drawn from these. Design development is, in large part, a matter of identifying and resolving these inconsistencies: hence the frequent critical comment that a design in its early stages is still 'unresolved'. It is only in the endgame of design, when the final details are being worked out within a well- established overall framework, that there is value in the mechanisms for automatically maintaining semantic integrity of designs that have so frequently been proposed for CAD systems (Borkin 1986; Eastman 1978; Kalay 1989). 

A final problem, and perhaps the most important one, is that analysis programs work within a framework of strictly monotonic reasoning. They draw critical conclusions from observations of the design together with some fairly stable, consistent body of facts and rules about the world. Adding facts and rules, or new observations about the design, should never invalidate critical conclusions that have already been drawn. But interesting critical discussion, as heard at architectural juries for example, is not really much like that. In fact, it exhibits the classic hallmarks of nonmonotonic discourse (Ginsberg 1987). Conclusions are frequently modified or retracted when critics notice things about the design that they had not noticed before, or evoke knowledge of the world that had not previously seemed relevant, or revise their beliefs in the face of argument. From the observation that a room has no windows a critic might, for example, conclude that it would be intolerably stuffy. A second critic might challenge this conclusion by observing that the room could easily be served by air conditioning. A third critic might respond that air conditioning would be too expensive, and so on. All these critical conclusions hold in the absence of information to the contrary - but such information might be on the tip of another critic's tongue. 

In sum, it is unrealistic to assume that a definitive, consistent set of design requirements can always be established ahead of time in order to provide an ironclad test for a solution, A creative design proposal may, in fact, provide a challenge to established beliefs and critical agendas. We should recognise that architectural designs are tested against a rich and complex interpretational discourse that develops in parallel with the processes of establishing design rules and making design possibilities explicit. 

CONCLUSION 
Ivan Sutherland's idea of structured design representation in computer memory, the Galilean tradition of design validation by analysis for compliance with predefined criteria, and faith in stable, universal design rules, provided the foundation on which computer- aided architectural design was initially built. But we should not remain prisoners of these ideas. Close consideration of the phenomenology of design exploration and the epistemology of criticism suggests that we must embrace the possibilities of designs that have ambiguous and unstable structural descriptions, of constructive rule systems that are provisional, fluid and mutable as we discover what they can produce, and of critical reasoning that is not bound by assumptions of monotonicity. These issues are not, I suggest, ones that arise under anomalous conditions that can safely be ignored in mainstream, 'practical' CAD systems. On the contrary, their centrality is characteristic of creative design processes. 

I do not see the emergence of these complexities as cause for pessimism. The great achievement of pioneering work in CAD has been to construct a sufficiently rigorous and comprehensive theoretical framework to allow clear identification of these issues and appreciation of their importance. The challenge now is to build a new generation of CAD systems that responds to them in sophisticated ways. The language games that architects play are subtle, and require commensurately subtle instruments. 

REFERENCES 
Akin, Omer, Psychology of Architectural Design, Pion (London), 1986. 

Borkin, Harold, 'Spatial and Nonspatial Consistency in Design Systems', Environment and Planning B 13, 1986, pp 207-22. 

Dennett, Daniel, 'Cognitive Wheels: The Frame Problem of Al', in Christopher Hookway (ed), Minds, Machines and Evolution, Cambridge University Press (Cambridge), 1984. 

Eastman, Charles M, 'The Representation of Design Problems and Maintenance of their Structure', in Jean-Claude Latombe (ed), Artificial Intelligence and Pattern Recognition in Computer-Aided Design, North-Holland (Amsterdam), 1978. 

Ginsberg, Mathew L (ed), Readings in Nonmonotonic Reasoning, Morgan Kaufmann (Los Altos, CA), 1987. 

Kalay, Yehuda E, Modeling Objects and Environments, John Wiley (New York), 1989. 

Knight, Terry Weissman, Transformations of Languages of Designs, PhD dissertation, Graduate School of Architecture and Urban Planning, University of California (LA), 1986. 

Maver, Thomas W, 'Software Tools for the Technical Evaluation of Design Alternatives', in Thomas W Maver and Harry Wagter (eds), CAAD Futures 87, Elsevier (Amsterdam), 1988. 

Mitchell, William J, The Logic of Architecture, MIT Press (Cambridge, MA), 1989. 

Mitchell, William J, Robin S Liggett, and Milton Tan, 'Top-Down Knowledge-Based Design', The Electronic Design Studio, 1989. 

Mitchell, William J, Philip Steadman and Robin S Liggett. 'Synthesis and Optimization of Small Rectangular Floor Plans', Environment and Planning B3, no I, 1976, pp 37-70. Nagakura, Takehiko, 'Shape Recognition and Transformation - A Script-Based Approach', The Electronic Design Studio, 1989. 

Reitman, Walter R, 'Creative Problem Solving: Notes from the Autobiography of a Fugue', in Cognition and Though, John Wiley (New York), 1965. 

Steadman, Philip, Architectural Morphology, Pion (London), 1983. 

Stiny, George, and William ] Mitchell, 'The Palladian Grammar', Environment and Planning B 5, no 1, 1978, pp 5-18. 

Stiny, George, and William J Mitchell, 'Counting Palladian Plans', Environment and Planning B 5, no 2, 1978, pp 189-98. 

Tan, Milton, 'Saying What it Is by What it Is Like', The Electronic Design Studio, 1989. 

From Malcolm McCullough, William J Mitchell and Patrick Purcell, eds, The Electronic Design Studio: Architectural Education in the Computer Era, 3400 word excerpt from pages 1-2, 6-8, 11-13, 13-15. © 1990 Massachusetts Institute of Technology, by permission of The MIT Press. © MIT Press. 
ALGORITHMIC FORM 
Kostas Terzidis 

Critical to the understanding of computation as a mode of thinking is its distinction from computerisation. In his writings, Kostas Terzidis has elaborated on the expanse of digitally based processes, in the realms of practice and theory, describing the mechanisms, functionality and relations to the balance between intention and emergence. In this text, from the book Expressive Form (2003), he clarifies the distinction between computerised and computational processes in terms of the relationship between authorship, abstraction and specificity. Terzidis explains, in computerised methods specificity exists as a precursor, where the action of design is a matter of filtering, reorganising and recompiling information to present different fields of outcomes. Computerisation mainly mimics the actions of designing in rationalising the information of a predictable outcome. Computation, on the other hand, involves the action of deducing specificity, that which cannot be captured given the initial dataset of properties and conditions. It is argued that this, in fact, does mime the logic of design, as a pursuit of evolving precision from initially abstract instructions. Terzidis poses a valuable point in describing computation as not vitally bound to the digital. This frees it from the whole of geometric and mathematically generated styles and particular modes of software engagement (the interface). Computation serves as the means to which an explorative 'search' can be undertaken. 

An algorithm is a computational procedure for addressing a problem in a finite number of steps. It involves deduction, induction, abstraction, generalisation and structured logic. It is the systematic extraction of logical principles and the development of a generic solution plan. Algorithmic strategies utilise the search for repetitive patterns, universal principles, interchangeable modules and inductive links. The intellectual power of an algorithm lies in its ability to infer new knowledge and to extend certain limits of the human intellect. 

While many algorithms have been invented and implemented for architectural design in space allocation and planning problems,' 'their implementation in aesthetics and formal theories has been, generally, limited. Most of the theories related to form pertain mainly to subjective interpretation and perception. In contrast, algorithmic logic involves a deterministic approach to form and its shaping forces; it suggests rationality, consistency, coherency, organisation and systemisation. What makes algorithmic logic so problematic for architects is that they have maintained an ethos of artistic sensibility and intuitive playfulness in their practice. In contrast, because of its mechanistic nature, an algorithm is perceived as a non human creation and therefore is considered distant and remote. Traditionally, the dominant mode for discussing creativity in architecture has always been that of intuition and talent, where stylistic ideas are pervaded by an individual, a ‘star, or a group of talented partners within the practice. In contrast, an algorithm is a procedure, the result of which is not necessarily credited to its creator. Algorithms are understood as abstract and universal mathematical operations that can be applied to almost any kind or any quantity of elements. For instance, an algorithm in computational geometry is not about the person who invented it but rather about its efficiency, speed and generality. 

Consequently, the use of algorithms to address formal problems is regarded suspiciously by some2 as an attempt to overlook human sensitivity and creativity and give credit instead to an anonymous, mechanistic and automated procedure.3 

While most algorithms are tailored to automate tedious manual methods, there is a certain category of algorithms that are not aimed at predictable results. Their inductive strategy is to explore generative processes or to simulate complex phenomena. Such inductive algorithms can be regarded as extensions to human thinking and therefore may allow one to leap into areas of unpredictable, unimaginable and often inconceivable potential. For instance, artificial neural networks are systems of algorithms that simulate the human brain's functions. They are being used in classification, forecasting and modelling applications. In making determinations, neural networks use several principles, including gradient-based training, fuzzy logic, genetic algorithms and Bayesian methods. What distinguishes these algorithmic processes from common algorithms is that their behaviour is often non predictable and that frequently they produce patterns of thought and results that amaze even their own creators. Similarly in design, shape grammars, mathematical models, topological properties, genetic systems, mappings and morphisms are algorithmic processes aimed at exploring uncommon, unpredictable and uncharted formal properties and behaviours.* 

Two opposing human activities that are central to algorithmic composition as a mode of thought are invention and discovery. Invention is defined as the act of causing something to exist by the use of ingenuity or imagination; it is an artificial human creation. In contrast, discovery is the act of encountering, for the first time, something that already existed. Both invention and discovery are about the origin of ideas and their existence in the context of human understanding. These two intellectual mechanisms result from a logic which tends to argue whether the existence of certain ideas or processes is one of the following: either a human creation or simply a glimpse of an already existing universe regardless of the presence of humans. The most paradigmatic example of this polemic is that of geometry itself: the existence of geometry can be regarded as either a descriptive revelation of properties, measurements and relationships of existing forms or as an arbitrary, postulate-based mental structure that exists only in the human mind. For instance, Euclidean geometry was developed originally to measure distances on the surface of earth and yet, in Euclidean geometry, Platonic primitive shapes, such as squares, circles and triangles, do not exist per se in nature though they represent idealised approximations of natural objects. Likewise, architecture can be regarded as either a simulation of the laws and structure of nature or as a world of fantasy and imagination. In any case, algorithms are encapsulations of processes or systems of processes that allow one to leap and venture into the world of the unknown, whether natural or artificial. They are not the end product, but rather a vehicle for exploration. 

Computation is a term that differs from, but is often confused with, computerisation. While computation is the procedure of calculating, ie, determining something by mathematical or logical methods, computerisation is the act of entering, processing or storing information in a computer or a computer system. Computerisation is about automation, mechanisation, digitisation and conversion. Generally, it involves the digitisation of entities or processes that are preconceived, predetermined and well defined. In contrast, computation is about the exploration of indeterminate, vague, unclear and often ill-defined processes; because of its exploratory nature, computation aims at emulating or extending the human intellect. It is about rationalisation, reasoning, logic, algorithm, deduction, induction, extrapolation, exploration and estimation. In its manifold implications, it involves problem solving, mental structures, cognition, simulation and rule-based intelligence, to name but a few. 

Digitisation is the conversion of analog information into digital information. Computerisation, by definition, involves digitisation. This is not necessarily the case with computation. Because many mental processes can be analysed, codified, systematised or even synthesised without the use of computers, computational methods do not have to involve digitisation. Nonetheless, their implementation on a computer system allows explorations of complexities that extend the limits of human prediction. For instance, even though the works of Gaston Julia' in the 1920s and, consequently, Benoît Mandelbrot in the 1970s were conceived and expressed on paper, they would have never been visualised, understood and explored further without the use of computational graphics. 

Computing is a term used to denote the act of making a mathematical calculation or a computation. Computing is often equated with computation since both employ the same methods. Essentially, both terms share the same meaning. Grammatically, the term computation involves the suffix "-tion' that denotes a state, condition or quality of a procedure. Similarly, the term computing employs the suffix -ing' that implies an action of implementing a procedure. While the two terms are linked through a state-action relationship, the noun 'computation' implies a set of theories and methods, whereas the noun 'computing' suggests an active investigation within the sphere of computation. In any case, these two terms are entirely different, distinguished and set apart from the terms computerisation or digitisation. 

The dominant mode of utilising computers in architecture today is that of computerisation; entities or processes that are already conceptualised in the designer's mind are entered, manipulated or stored in a computer system. In contrast, computation or computing, as a computer-based design tool, is generally limited. The problem with this situation is that designers do not take advantage of the computational power of the computer. Instead some venture into manipulations or criticisms of computer models as if they were products of computation. While research and development of software involves extensive computational techniques, mouse-based manipulations of three-dimensional computer models are not necessarily acts of computation. For instance, it appears, from the current discourse, that mouse-based manipulations of control points on nonuniform rational B-spline (NURBS)-based surfaces are considered by some theorists to be acts of computing. While the mathematical concept and software implementation of NURBS as surfaces is a product of applied numerical computation, the rearrangement of their control points through commercial software is simply an affine transformation, ie, a translation. 

An alternative choice is being formulated that may escape these dialectically opposed strategies: algorithmic design. It involves the designation of software programs to generate space and form from the rule-based logic inherent in architectural programs, typologies, building code and language itself. Instead of direct programming, the codification of design intention using scripting languages available in three-dimensional packages (ie, Maya Embedded Language (MEL), 3dMaxScript and FormZ) can build consistency, structure, coherency, traceability and intelligence into computerised three-dimensional form. By using scripting languages designers can go beyond the mouse, transcending the factory-set limitations of current three-dimensional software. Algorithmic design does not eradicate differences but incorporates both computational complexity and creative use of computers. For architects, algorithmic design enables the role of the designer to shift from architecture programming to programming architecture. Rather than investing in arrested conflicts, computational terms might be better exploited by this alternative choice. For the first time perhaps, architectural design might be aligned with neither formalism nor rationalism but with intelligent form and traceable creativity. 

There are still some misconceptions about the role of the computer in the process of design. Design, like many other mental processes, at the information-processing level has nothing specifically neural about it. The functional equivalence between brains and computers does not imply any structural equivalence at an anatomical level (eg, equivalence of neurons with circuits). Theories of information processes are not equivalent to theories of neural or electronic mechanisms for information processing." Even though, physically, computers may appear to be a set of mindless connections, at the information level they are the materialisation of mathematical and syllogistic procedures."1 

The word 'tool' is often used to describe the synergistic interaction of designers with computers. A tool is defined as an instrument used in the performance of an operation. The connotative notion of a tool implies control, power, dominance, skill and artistry. A pen, for instance, is a device that allows one to perform or facilitate the manual or mechanical work of writing or drawing. The capabilities, potency and limitations of a tool are known or estimated in advance. This is not the case with computers performing inductive algorithmic computations. Neither is their capacity or potency understood, nor can their limitations be pre-estimated. Indeed, designers are frequently amazed by processes performed by algorithmic procedures, over which they have no control and of which they often have no prior knowledge. 

Since the mid-1970s, beginning with shape grammars and computational geometry and continuing through topological properties and morphism, designers and theorists have been concerned with the use of algorithms as a mechanism for exploring formal compositions. These theories have attempted either to automate and enhance existing manual techniques or to explore new uncharted territories of formal behaviour. Various methods have been employed in the search for new forms: formal analysis involves the investigation of the properties that describe an architectural subject. Composition, geometrical attributes and morphological properties obeying Galilean, Newtonian and, lately, molecular and organic principles are extracted from figural appearances of an object. In contrast, structural analysis deals with the derivation of the motivations and propensities which are implicit within form and which may be used to extract their generative processes. Morphism employs algorithmic processes for the interconnection between seemingly disparate entities and the evolution from one design to another. 

Unlike computerisation or digitisation, the extraction of algorithmic processes is an act of high-level abstraction. It is often equated with rationalism, determinism or formalism, but more importantly these resources are ultimately in the service of transcendency. Transcendency is the quality of lying beyond the ordinary range of perception. It is the quality of being above and beyond in response to timelessness and spacelessness. Algorithmic structures represent abstract patterns that are not necessarily associated with experience or perception. Furthermore, the observable outputs of algorithms should not be equated with the processes that created them. Marcos Novak makes a distinction between topology and curved surfaces. Topology, he points out, 'means simply the study of those relations that remain invariant under transformations and deformations. A notion of continuity is indeed implied in this definition, but the continuity is abstract."2 Similarly, in Architectonics of Humanism Lionel March 'seeks an order beyond appearances' as he attempts to uncover the "many modes of numbering" and 'looks for the "warring and opposing elements", which go to make an original microcosm echoing universal harmony." 13 Algorithmic processes result from events that are often neither observable nor predictable and seem to be highly intuitive. These events are made possible by abstraction and ingenuity. For instance, the discovery (or invention) of 'hyperspace' resulted from an algorithmic inductive process of projections that map three-dimensional points into four-dimensional ones, yet both the projections and the results are neither predictable nor observable. In this sense, algorithmic processes become a vehicle for exploration that extends beyond the limits of perception. 

When comparing contemporary practising architects such as Thom Mayne, Frank Gehry and Peter Eisenman it is necessary to overlook many significant and distinguishing differences in order to identify at least one common theme: the use of the computer as an exploratory formal tool and the increasing dependency of their work on computational methods. The most paradigmatic examples of the last 10 years invest in computationally generated parts and diagrams. Through computation, architecture transcends itself beyond the common and predictable. In contrast, computerisation provokes Whorfian effects: through the use of commercial applications and the dependency on their design possibilities, the designer's work is at risk of being dictated by the language-tools they use. By unknowingly converting to the constraints of a particular computer application's style, one runs the risk of being associated not with the cutting-edge research, but with a mannerism of 'high-tech' style. 

In Diagram Diaries,1 Peter Eisenman's concept of an architectural diagram as an explanatory, analytical, generative or representational device is directly linked to the principles of human understanding and interpretation. This human-centric approach is implicit within the sphere of subjective phenomena and personal interpretations. Within that realm, any logic that deals with the evaluation or production of form must be, by default, both understandable and open to interpretation. The problem with this approach is that it does not allow thoughts to transcend beyond the sphere of human understanding. In fact, while it praises and celebrates the uniqueness and complexity of the human mind, it also becomes resistant to theories that point out the potential limitations of the human mind. In contrast, algorithmic form is not about perception or interpretation but rather about the process of exploration, codification and extension of the human mind. Both the algorithmic input and the computer's output are inseparable within a computational system of complementary sources. In this sense, a diagram becomes the embodiment of a process obtainable through a logic of mutual contributions: that of the human mind and that of the machine's extendibility. 

Similarly, Euclidean geometry was understood as an extension of human perception. The divinity of its nature can be ultimately linked to its ability to infer abstract concepts that appeal to the mind rather than the eye. Like religion, it was the revelation of an abstract system of relationships that transcended above and beyond the material objects it represented. Similarly, algorithmic form is an extension of human understanding. The mere existence of the term 'unimaginable' can be linked to the ability of algorithms to surpass the sphere of human control and prediction. Like meta-structures, 15 algorithmic forms are manifestations of inductive processes that describe, extend and often surpass the designer's intellect. 

There is often confusion about ownership of algorithmic forms. Intellectual property is defined as the ownership of ideas and control over the tangible or virtual representation of those ideas. Traditionally, designers maintain full intellectual property rights over their designs or manifestations thereof, based on the assumption that they own and control their ideas. This is not always the case with algorithmic forms. While the hints, clues or suggestions for an algorithm may be intellectual property of the designer-programmer, the resulting tangible or virtual representations of those ideas are not necessarily under the control of their author. Algorithms employ induction, regression, randomness, recursion, cellular automata, probability, Markov chains or quantum computation, to name a few, the outcomes of which are unknown, unpredictable and unimaginable. If there is an intellectual root to these processes it must be sought in a world that extends beyond human understanding.16 Both the notions of 'unknown' and 'unimaginable' escape from human understanding since both negate two of the last resorts of human intellect, those of knowledge and imagination. In fact, as Novak points out, while the clause 'if-then' is a syllogistic structure that leads on to new knowledge, the clause 'if-then-else" involves the alternative 'else' that may point to the opposite of knowledge, that is, to 'that which does not follow from its roots, or, indeed, that whose roots can no longer be traced, or have become irrelevant, or are unknown, or follow from principles outside previous understanding'." 

A paradigm shift is defined as a gradual change in the collective way of thinking. It is the change of basic assumptions, values, goals, beliefs, expectations, theories and knowledge. It is about transformation, transcendence, advancement, evolution and transition. While paradigm shift is closely related to scientific advancements, its true effect is in the collective realisation that a new theory or model requires understanding traditional concepts in new ways, rejects old assumptions and replaces them with new. For TS Kuhn, 18 scientific revolutions occur during those periods when at least two paradigms coexist, one traditional and at least one new. The paradigms are incommensurable, as are the concepts used to understand and explain basic facts and beliefs. The two live in different worlds. The movement from the old to a new paradigm is called a paradigm shift. 

Traditionally, the dominant paradigm for discussing and producing architecture has been that of human intuition and ingenuity. For the first time perhaps, a paradigm shift is being formulated that outweighs previous ones. 19 Algorithmic design employs methods and devices that have no precedent. If architecture is to embark into the alien world of algorithmic form, its design methods should also incorporate computational processes. If there is a form beyond comprehension it will lie within the algorithmic domain. While human intuition and ingenuity may be the starting point, the computational and combinatorial capabilities of computers must also be integrated. 
NOTES 
1 The work in this area is as old as computer-aided design. An early attempt was MIT's BUILD system, which would take a building program, indicating dimensions and connections for each space. The computer then arranged the spaces, thus solving the problem. This approach has since been used extensively for solving complex design problems that are related to arranging parameters in optimum ways. These approaches focus on the functionality of the design scheme and do not take into account aesthetic or artistic parameters. In areas such as the design of computer chips, nuclear plants or hospitals, automatic spatial allocation plays a very important role, even today. See A Dietz, Dwelling House Construction, MIT Press (Cambridge, MA), 1974, and C Yessios, B Parent, W Brown and C Terzidis, 'CKAAD Expert: A Computer and Knowledge Aided Architectural Design Expert', in Design Theory 88, NSF Grantee Workshop on Design Theory and Methodology, Rensselaer Polytechnic Institute (Troy, NY), 1988, pp 1-8. 

2 Greg Lynn reveals that: 'because of the stigma and fear of releasing control of the design process to software, few architects have attempted to use the computer as a schematic, organizing and generative medium for design'. See G Lynn, Animate Form, Princeton Architectural Press (New York), 1999, p 19. 

3 In response to this discrepancy the ancient Greeks devised a 'fair' method of acknowledgement of authorship. The Pythagorean theorem, the spiral of Archimedes and Euclidean geometry are attempts to give proper credit to the authors regardless of the status of their subjects as inventions or discoveries. 

4 The term algorithmic is often connected with complexity. While the objective or result of an algorithm may be complex, the strategy itself does not necessarily follow that complexity. For instance, chaos is the study of how simple systems can generate complicated behaviour. 

5 Claude Perrault, the architect of the peristyle of the Louvre, argued that architecture is a fantastic art of pure invention. He asserted that architecture really exists in the mind of the designer and that there is no connection to the natural world. In addition, architecture as an imaginative art obeys its own rules which are internal and personal to each designer, and that is why most creators are vaguely aware of the rules of nature and yet produce excellent pieces of art. A similar point was also argued by Giovanni Battista Vico. In his work The New Science (1744), Vico argued that one can know only by imagining. The twisting of language and meaning can lead one to discover new worlds of fantasy. He argued that one can know only what one makes. Only God can understand nature, because it is his creation. Humans, on the other hand, can understand civilisation, because they made it. The world of civil society has certainly been made by men, and its principles are therefore to be found within the modification of our own human mind.' 

6 In its colloquial sense, computerisation refers to the process of furnishing with a computer or a computer system. 

7 Gaston Julia, 'Memoire sur l'ltération des fonctions rationnelles', Journal de Math. Pure et Appl, vol 8, 1918, pp 47-245. 

8 Benoît B Mandelbrot, The Fractal Geometry of Nature, WH Freeman (New York), 1983. 

9 See Dana Cuff, 'One Educator's Thoughts on Design Software's Profound Effects on Design Thinking and Teaching', Architectural Record, vol 9, 2001, pp 200-06. In this article Cuff considers that computing is 'one of the most important transformations of the contemporary profession' and that today 'computing has become a populist skill'. 

10 The works of Herbert Simon and Allen Newell in the 1960s and 1970s are undoubtedly some of the best examples of the study of artificial intelligence. A Newell, Unified Theories of Cognition, Harvard University Press (Cambridge, MA), 1990 and HA Simon, The Sciences of the Artificial, MIT Press (Cambridge, MA), 1981. 

11 Glynn, in Animate Form, op cit, p 19, describes machine intelligence 'as that of mindless connections'. 

12 Novak continues: A cube is not less topological than a blob. However, when working algorithmically, what remains invariant is the algorithm, so that a new notion of topology, 'variable topology' is introduced. While the variations in the space of the parameters and control structures that implement the algorithm may be continuous, the product of the algorithm may be to show tears and discontinuities and ever fracture into a cloud of particles or an explosion of shards. See Marcos Novak, Transarchitectures and Hypersurfaces', in Architecture and Science, G Di Cristina (ed), Wiley-Academy (London), 2001, pp 153–7. 

13 See Lionel March, Architectonics of Humanism: Essays on Number in Architecture, Academy Editions (Chichester), 1998, pix. 

14 See Peter Eisenman, Diagram Diaries, Universe Publishing (New York), 1999, p 27. 

15 The prefix meta- indicates one level of description higher. If X is some concept, then meta-X is data about, or processes operating on, X. For example, meta-syntax is a syntax for specifying syntax, meta- language is a language used to discuss language, meta-data is data about data, and meta-reasoning is reasoning about reasoning (definition taken from the Free On-line Dictionary of Computing). 

16 Karl Popper argued that the world as a whole consists of three interconnected worlds. World One is the world of physical objects and their properties - with their energies, forces and motions; World Two is the subjective world of states of consciousness, or of mental states - with intentions, feelings, thoughts, dreams, memories and so on, in individual minds. World Three is the world of objective contents of thought, especially of scientific and poetic thoughts and of works of art. World Three is a human product, a human creation, which creates in turn theoretical systems with their own domains of autonomy. See Karl R Popper, The Logic of Scientific Discovery, Harper & Row (New York), 1968. 17 See Marcos Novak, 'Alien Space: The Shock of the View', article reproduced from Art + Technology, supplement of CIRCA, vol 90, pp 12-13. 

18 See Thomas S Kuhn, The Structure of Scientific Revolutions, third edition, University of Chicago Press (Chicago, IL), 1996. 

19 Peter Eisenman referred to the idea of an electronic paradigm shift in architecture in 1992. He wrote: During the fifty years since the Second World War, a paradigm shift has taken place that should have profoundly affected architecture: this was the shift from the mechanical paradigm to the electronic one. This change can be simply understood by comparing the impact of the role of the human subject on such primary modes of reproduction as the photograph and the fax; the photograph within the mechanical paradigm, the fax within the electronic one. Eisenman, "Visions" Unfolding: Architecture in the Age of Electronic Media', Domus, vol 734, 1992, pp 17-24. 

From Kostas Terzidis, 'Algorithmic Form', Expressive Form, Spon Press (Abingdon) - Taylor & Francis Group, 2003, pp 65-73. Reproduced by permission of Kostas Terzidis and Taylor & Francis. © Taylor & Francis. 


ARCHITECTURE AND PRACTICAL DESIGN COMPUTATION 
Mark Burry 

Mark Burry's work in deciphering the complex geometries for the construction of Gaudi's Sagrada Família has established an ongoing legacy for the relevance of computation in the realm of design. In this text, he tracks the development of specific geometric and procedural methods driven by the critical nature of connecting computational form with material form. Burry first describes this challenge in identifying how particular mathematical means for generating geometry may not align with rules which relate to scalar issues of structure, materiality and assembly. In his second example, Burry describes the lineage of processes developed for extrapolating forms from Gaudi's underlying geometric principles. Through these deductions, Burry projects the concept of agreed principles - computation being the realm in which they are captured. While computation in itself implies operation, it is really the selection, alignment and coordination of the operators which allows for knowledge to be inherited into the process and specificity relevant to material and context to be realised. Burry identifies this as a capacity of the architect, while being the casual observer, who is still integral in the coordination of the ‘observations'. 

Introduced to a sophisticated role for geometry within the design process a decade or more before computers became an option within the studio, I have had three decades now to reflect on the role of computation in design thinking. During this time computational design has transposed from analogue to digital practice. Taking 'compute' as to estimate or determine by arithmetical or mathematical reckoning; to calculate, reckon, count,' | shall avoid the temptation to critique the philosophical limitations of such a definition and stick to this literal interpretation, because the issues raised for the education and practice of an architect remain complex enough without having to step towards the esoteric. Before I leap in I should clarify what I mean by mathematics too, as I am relying on the older definition of mathematics being the collective name for geometry, arithmetic, and certain physical sciences (as astronomy and optics) involving geometrical reasoning? which, I believe, is closer to the lay view of the work of the mathematician than the stricter modern usage definition of it being an abstract science which investigates deductively the conclusions implicit in the elementary conceptions of spatial and numerical relations." 

Of course other premises further constrain my reflection. The first is while the majority of architects are not mathematicians, they nevertheless compute within their practice to a certain level. The second premise is that very few qualified mathematicians have subsequently moved to professional roles as architectural practitioners. There have been a few exceptional individuals who were blessed with mastery of both disciplines (Brunelleschi, Wren, da Vinci, Alexander, products of a time), but their exceptionality is as much in their slim numbers as their being truly exceptional as deeply creative thinkers. My third premise asserts that with the extraordinary opportunities offered by the computer to assist the architect in computation, we have entered a new world of opportunity. So, in considering the topic 'computation and design thinking' who is our constituency? Should future architects become more familiar with mathematics, more skilled in the world of numbers? I am going to argue here that there is a place for the architect at least within the 'computationally learned, and that this is a position of design strength if not actual professional necessity. I shall do this by looking at two case studies. Both examples have practical exigencies as the prime motivator for using computation to assist the resolution of design thinking and design making; that tricky issue of locating an effective representation of ideas on the path towards achieving successful built artefacts. The first case study considers the problem of the flattened arch and the second looks at Gaudi's dedication to second order geometry during his last 12 years. 

THE 'N' CENTRED ARCH 
Arches are at their most efficient when they accommodate the forces to which they are subjected towards a vertical gravitational alignment. The catenary curve describes the force path within the uniformly loaded arch, essential knowledge emerging during the Renaissance. The catenary curve echoes the parabola most closely visually (although not mathematically), and is derived from a hanging chain spanning between two points whose self-weight acted on by gravity is an analogue calculator providing critical information on potential performance. Wren's triumph with the dome of St Paul's with the scientific contribution of Robert Hooke follows on from Brunelleschi's largely intuitive discoveries when building the Florence Duomo dome. Both offer conspicuous evidence of a shift to greater architectural sophistication through the use of design computation. In considering appropriate geometries for the arch - especially the bridge - the ellipse is a more favourable alternative to the Roman's semicircular arc, for it can be far wider than it is tall, and it more or less visually if not functionally directs forces on to a vertical trajectory at its two horizontal extremes (Figure 1). With the right proportions an ellipse can approximate a catenary and parabola far more closely than a semicircular Roman arch. How likely was it that the Renaissance masters were familiar with the mathematical equation for an ellipse and its practical application? 

Most likely the 'string loop with two nails' description would have sufficiently fascinated their enquiring minds and would have been more accessible to most architects of that time. The two nails are set apart by a distance /. A string whose length £ is greater in length than 2/ is made into a loop and fitted around the two nails positioned at p1 and p2 (Figure 2). If an inscribing device is inserted into the loop at d, and pulled away from both nails until the string is taut, it can move around the two nails along a path that ensures that the loop is kept taut. The trajectory for the resulting inscription is elliptical. Logically the following conditions apply: pl to p2 is fixed as distance ; pld + p2d is a constant. String loop length L = pld + p2d + 1. 

Figure 1. Ferdinand Meldahl, elliptical arch, Bygningen blev tegnet og opført som grovsmedie (former naval forge), Copenhagen, Denmark, 1861. © Mark Burry. 
Figure 2. Mark Burry, constructing an ellipse using the 'string loop and two nails' method. © Mark Burry. 
Figure 3. Mark Burry, geometry of the five-centred arch (based on diagram by Paul L Rosin and Michael LV Pitteway). © Mark Burry, 
This method for constructing an ellipse is effective, haptic and correspondingly more within the designers' instinct than its defining maths, one might argue, but it is not accurate. Nor is it convenient for the stonemason working at full scale for an arch many metres wide. A different construction of the ellipse and alternative to an equation approach is that of the two circles method with radii equivalent to the ellipse's major and minor a and b axes from which the geometry may be approximated. Even if an ellipse could be accurately derived and laid out at full size using this method, it implies unique templates for every piece of masonry from arch side to arch centre the left- hand templates obviously being mirrored for the right side or vice versa. But what sheet material existed then to template at this scale, using an approach that we can envisage so easily for our time? This was presumably a root cause for the need to simplify the problem of describing an ellipse using connected cotangent arcs. In this way a 'three centred arch' has a flatter arc in the top of the ellipse brought round more steeply at either side by an arc with a tighter radius, intersecting at the lateral extreme by an arc with tighter radius still. For a more accurate result a 'five centred arch' could be composed whereby five connected cotangent arcs form the half ellipse (Figure 3). Where to position the arcs is the challenge here, and there is a choice of orthodoxies. Some are better than others, and the test is making an exaggeratedly flat arch whereby the least successful method reveals its relative deficiencies. For the stonemason, working with a reduced set of templates through relatively easily inscribed arc curves is easier to countenance than the gradated curve of the ellipse. 

It may seem that I am wandering more into an account of descriptive geometry and stereotomy than design computation but that is not my intention at all there is a thin line between computing for a designed material outcome and the application of geometry as a set of transferable skills between master and apprentice. My argument is that despite such orthodoxies for constructing the 'five centred arch' that emerge over time, even within a relatively reduced set of accomplishments such as flattening an arch there is still opportunity for choice; and a rich one too. A good eye is guiding the best combination of parameters such that the optimum might emerge as much from human judgement as from a set of agreed principles. But it is clear from the account of others that considerable computational effort was also applied to seek a mathematically perfected outcome for the best arrangement of 'n' centred arches, as Rosin and Pitteway's account so painstakingly reveals (referred to in previous endnote). Here the computation is generative in terms of a practical efficacy rather than being restricted to a purely computational device. 

As an architect I have the capacity to understand the dilemmas posed by the flattened arch and to involve myself in the computation leading to the best possible outcome. If I were to busy myself with someone else's software to compute the most efficient archway using the least material, extraordinary levels of computation would have been accessed but I could not claim the territory from a mathematical perspective, as I am merely the such operative. The computer has done the computing - how can I, as architect, make any claim? I therefore wonder at which level architects work knowledgeably as part of the computationally learned, or falsely as the dilettante mathematician, for regardless of level they are not working mathematically at all. Even in the era of digital computing we do not have a failsafe that allows us to be blissfully ignorant of the maths hidden in the black box; vigilance is still required on the part of the designer. The stained-glass rose window for the Passion Facade at Gaudi's Sagrada Família Church, for example, is a giant ellipse measuring 4 by 2 metres and was completed in 2001. The masonry that accommodates the window was described using high-end aeronautical design software. The metal frame for the glass was templated using software dominating the architectural sector at that time. It was not producing a 'true ellipse' we discovered, when we came to attach the frame to the masonry only to find that despite being the correct height it was too wide for the opening by several centimetres. 

COMPUTATIONAL GAUDÍ 
Gaudí moved away from free form towards a strict geometrical schema following completion of the Casa Milà apartments (approximately 1905 to 1912). For some these apartments symbolise his creative zenith but arguably they were also his professional nadir: a rapid descent from his status as conspicuous and successful architect fêted by the city's well-to-do and revered by the citizens of Barcelona where most of his work was built.5 From 1914 until his death in 1926 there are three broad characteristics that distinguish him from all that had gone before in a long and illustrious career. First, he took on no further secular commissions. Second, he devoted himself entirely to the completion of the Sagrada Família Church design, which up to that point he had worked on for 31 years. The third characteristic of this period is his surprising migration from a free form predilection to one of committed application of three second order (doubly ruled) surfaces, all formulaically described using relatively simple equations: the hyperbolic paraboloid, the helicoid and the hyperboloid of revolution of one sheet (Figure 4).6 

At the beginning of this final period, the building was only a few percentage points built, while the design for what remained to be completed was hardly much further advanced. He needed to push the design way forward as the implications of his advancing age became more apparent to him. 

There are highly original computationally generative aspects to his decision to apply second order geometry, and the aesthetic, philosophical and practical consequences of choosing them have only become better known in recent years, many decades following his death. In the context of computational design thinking I am describing the revelations that came from unravelling his approach very succinctly, a kind of reverse engineering motivated by the expressed wish of the Sagrada Família's commissioning body to continue the building faithfully following the surviving fragments of Gaudi's 1:10 and 1:25 scale plaster models, his principal design tool (Figure 5). To compute the best possible outcome for completing this project without Gaudi's direct authorial vision we had to reveal his computational strategy by unravelling the secrets contained within the models. In using these geometries - mathematical surfaces was Gaudí acting both as geometer and architect? 
Figure 4. Mark Burry, hyperboloid of revolution of one sheet. © Mark Burry. 
Figure 5. Antoni Gaudí, Sagrada Família Church, 
+Barcelona, circa 1920. Original photograph of 1:10 scale model of Gaudi's clerestory window of the Sagrada Família Church, Barcelona, taken prior to the model's destruction during the Spanish Civil War (1936-9). © Mark Burry. 
Figure 6. Antoni Gaudí, Sagrada Família Church model, Barcelona, circa 1920. Restored 1:10 scale model of Gaudi's clerestory window for the Sagrada Família Church, Barcelona. Mark Burry. 
 

ANALOGUE DESIGN. COMPUTATION 
First let me describe his approach to computational design. The 1:10 funicular (hanging) model was Gaudi's interactive analogue parametric design computer' for his uncompleted chapel at Colònia Güell on Barcelona's outskirts. The hanging model itself was a network of catenary systems. He adopted a similarly haptic approach to modelling the Sagrada Família Church in the city itself, albeit using a different medium and approach: plaster of Paris. Gaudi was working in a hands-on manner with his model-makers composing walls, columns, domes, towers and vaulted ceilings predominantly by intersecting hyperboloids of revolution. These intersected conglomerates were decoratively embellished by articulating the surface unions with triangular planes or hyperbolic paraboloids, the directrices of the hyperbolic paraboloids being designed to coincide with selected generatrices from the hyperboloids (Figure 6). 
We know exactly how Gaudi worked as there has been an almost uninterrupted apprenticeship from the generations who assisted him then to the current model- makers on site today. His approach is sublimely convincing especially when it is seen in action, but in itself seems not to have been based on calculation beyond what was at least an ineffable intuition informed by actual mathematical insight. The approach that he took for major wall and ceiling vault elements, for example, is a rich one as there are nine parameters that govern the relationship between any two adjacent surfaces and the character of the intersection: in other words there is infinite choice with nine dimensions of possibility. In summary, the first three parameters are the x, y and z coordinates for the hyperboloid centroid. A second cluster of three parameters comprises the x, y and z axes of rotation. Parameters 7 and 8 are respectively a and b, the major and minor axes to the hyperboloid collar ellipse (or circle when a = b), and the ninth is c, the asymptote determining the slope of the surface. Because Gaudí went to the trouble of first devising then applying this geometrical codex, we presume to aid his successors in capturing his vision for the building in totality, the challenge for us, his post mortem successors, has always been to extract the corresponding data accurately, interpret it as second order geometry, and apply it such that the constructed outcome responds unequivocally to his intentions, albeit so subtly expressed in his models but precisely redacted for the purposes of building. 

Gaudí died at the age of 74 in a street accident and he had probably held off fully describing how his design approach would be taken up by his successors, so this has had to be undertaken forensically. The first efforts to engage seriously with untangling Gaudi's geometries and applying them to the construction did not begin in earnest until half a century after his death with the Spanish Civil War of 1936-9 further complicating the issue. The digital revolution was taking place at the same time as our research into the surviving models, which has therefore taken place in a series of shifts in level of sophistication, always using the best computational assistance available, and not necessarily coming from the architecture sector. 

DESIGN COMPUTATION THROUGH DRAWING 
If Gaudi's composition made through his computational plaster modelling process was level 1 then its later transfer without his direct input can be seen as level 2: there was no possibility of simply extrapolating from where he left off because not only had he not written down his theory at all, he had never applied ruled surfaces to masonry construction in the way proposed by his scale models from which his followers might have learned. This is also to say that Gaudí himself never saw any of this second order geometry applied to the built fabric as his models for the Sagrada Familia Church determined, despite his earlier experiments with hyperbolic paraboloids at Colònia Güell chapel. Furthermore, if (and it is quite a big if) his sketched geometries and other graphical musings contained the secrets to his computational design approach, they were destroyed when vandals sacked his workshop based on site during the Civil War. 

My first effort to undertake the necessary analysis of the surviving model fragments was through projective drawing. I had not been sufficiently schooled in descriptive geometry at architecture school to find this an easy challenge, nor was there anyone on site who knew how to approach the problem. My two senior colleagues were both nearing 90 years in age, Senyors Bonet Garí and Puig i Boada, and while they had followed Gaudí, coinciding their youthful enthusiastic architectural apprenticeship with frequent visits to the atelier on site during his last years, if Gaudi had himself known how he would approach the translation of ruled surfaces from scale model to a construction method, he had not shared this with his younger acolytes. It was therefore a matter of working with first principles. 

Essentially the problem was intersecting adjacent hyperbolic surfaces and plotting the resulting fourth order curves of intersection. More crucially, it was ascertaining the intersection between any three or more of these curves - the triple points - that drove the investigation, because only when all the triple points matched the data measured from Gaudi's model would we be able to claim a faithful rendition of the models to built work. As Gaudi's models were effectively 3D sketches, it was not unreasonable that a certain amount of approximation had taken place, but for this system to work at full scale it was essential that the various imperfections were ironed out. 

The skills of the geographer and surveyor were called upon; unsurprisingly, a late Modern Movement education had provided me with few of these for mapping complex surfaces. Once I had made the necessary conceptual leap and started reading the triple points as the equivalent of geographical mountain peaks and the surface intersections as ridges, contours could be computed by referring simultaneously to three separate orthographic projections each the size of tablecloths. It seemed an odd departure to be using concurrently a scientific calculator (recently invented at that time) to find points in space in order to draw them with my Rotring pen using sets of railway curves. This was 1979, and the projective drawing approach, while less labour intensive than experimental modelling with setting plaster, was intensive all the same. Once a combination of surfaces had been graphically tested painstakingly for fit, a plaster model would then be made for verification purposes, each time suggesting further design refinement. This was a process in which each iteration took many months (Figure 7). 

Figure 7. Mark Burry, sections of the hyperboloids projected graphically, Sagrada Família Church, Barcelona, 1980. Mark Burry. 
DIGITAL INCURSION 
With the arrival of accessible architectural CAD in the late 1980s, the cavalry had seemed to arrive, but early CAD at best offered only latent computational design potential. Intersecting two surfaces in space could only be undertaken using a variation of the graphical method described above. Surely two surfaces passing through each other would be trimmed with a single click of the mouse one assumed - it was not so. It took three years to get access to extraordinarily expensive aeronautical design software that boasted this trimming facility - one that only became core to affordable design software a decade later, or more. In this interim period a number of approaches were applied as a third level to our investigation. The one pertinent to this essay was an effort to trace representative hyperbolas extracted from the model and match them to curves calculated by mathematical equation. Copies of plaster models of Gaudi's surfaces were interrogated by cutting them normal to the hyperboloid centres along the x and y axes and tracing the hyperbolas as 2D profiles on paper, and digitising them using a ruler and set square. The intention then was to build 3D hyperboloids digitally derived from the traced hyperbolas, and this was especially ambitious since nearly all the hyperboloids of revolution that make up the clerestory window elements, for instance, are elliptical. 

I discovered at that time that there was no native hyperbola curve function in any of the most widely used CAD packages, thus the only way to draw the mathematically correct curve was to calculate the position of sufficient points and interpolate them with a fitted polyline. This was a trial and error approach that seemed at odds with the efficiency espoused by the emerging software proselytisers whose software was designed to tackle the inefficiencies of manual drafting. Looking under the hood of various architectural software packages I discovered the world of the macro and scripting (Level 3). I was surprised by its potential to enrich design decision-making and became engaged immediately. For the decade that followed I marvelled at the lack of general take-up of such powerful design adjuncts, but i conceded that we had emerged as a design profession displaced by several generations from the time when geometrical engagement with design had far greater significance.® 

PARAMETRIC DESIGN 
The measuring by hand method preceded easy access to computer-linked measuring devices such as surface scanners or point digitisers, so it was slow and quite awkward. In 1993 parametric design made its appearance in our digital toolbox via software called CADDS5 from ComputervisionTM. Level 4 of the quest for the ruled surfaces now fully interpreted in the digital era was the leap in functionality provided by such high-end engineering design software available then only on UNIX-based workstations. Amazingly, from my point of view, there is not a great deal more that can be done on our software today, almost two decades later. It seems that the cost of such sophistication, and its inaccessibility through being so different in structure and use for the design studio, denied architects the opportunity to work to this level of digital design computation until the emergence of cheap software for use on everyday machines so many years later. This might account for the unembarrassed heralding and sudden rapid awareness and take-up so many years later than the period in the early 1990s that I describe, although the actual use of parametric design has not become any easier in the intervening years. Indeed 'parametricism" could be argued as a phenomenon that has burgeoned despite its innate unfamiliarity to a majority of designers. Is it a case of a tail wagging a dog? (Figure 8). 

Level 5 of the quest was a realisation that we needed to move from 2D analysis of curves approximating a hyperbola to 3D analysis. From matching a mathematically derived hyperbola with those we had carefully traced from Gaudi's surviving model fragments we moved to matching points on the surface with a best-fit optimisation approach, which brought home to me the difference between architects dabbling in computation at this level and the refinement that applied mathematicians could bring to solution-seeking of this nature. 

BEST-FIT OPTIMISATION 
In 1993 my architectural colleagues at Universitat Politècnica de Catalunya wrote a program in BASIC to home in on best-fit surfaces. We measured 3D points from the surfaces for which we were seeking geometrically defined hyperboloids. Working with a set of minima and maxima values, the program would iteratively run through potential solutions such that all of the points in our set were tested for conformation to each hypothetical solution. We gradually reduced the maxima and minima until finally there would be only one result - the 'best fit". In some cases the program would run for several days before yielding a candidate. We experienced the frisson that comes from not knowing when to give up, as, for instance, when the maxima and/or the minima were set too tight to the extent that there might not be a result at all and we would be waiting for days in vain - fest a result suddenly emerge, as we experienced. I described this process to an engineer and programmer colleague, Peter Wood, and he introduced our team to not only genetic algorithms, but bespoke UNIX-based tools with which he wrote a program that he christened Xhyper, taking advantage of UNIX's user-friendly GUI windows - a novelty at that time. This was Level 6 in our development.10 This program could home in on the best-fit result from a given dataset comprised of measured points in a matter of seconds. This meant that we could entirely avoid the informed trial and error methods that characterised earlier efforts of levels 1-5 described above. 

Again this task was far harder to do in the early 1990s than it would be now. In 1992 we had to measure all 550 relevant points in space on the interior and exterior of the surviving parts of the 1:10 plaster model of the clerestory. We built an apparatus to help us to be as accurate as possible, but within a few years medical measuring technology filtered through first to the animation industry and then on to projects such as our own.

Figure 8. Mark Burry, parametrically driven digital sculpting through Boolean subtraction, Sagrada Família Church, Barcelona, 1994. Mark Burry. 
Gehry Partners in LA were leading the vanguard in this regard. Our measured points were weighted for crucial and less crucial relevance. Triple points and points on the hyperboloid collar were the most important. Points on the curve of intersection came second, and points on the surface that were neither intersections nor on the collar were deemed less important. Gaudi's method of making the models was very accurate in itself, but various rasping marks on the surface suggested that when the outcome was inconvenient the geometries were honed into shape meeting the needs of the eye but less visibly losing their geometrical credentials. The interior and exterior points from each surface were combined as a single .dat file, and read into Xhyper for processing. We were seeking concrete results for up to nine parameters, but in some cases the values of several parameters were known such as the x, y and z coordinates of the hyperboloid that Gaudí had governed using a proportional system."1 

Depending on the number of unknown parameters and richness of the data we could choose interactively between the Simplex and Hill Climbing optimisation algorithms. Generally a was known through Gaudi's proportional system so we were down to seeking values for five unknowns; rotation about x, y and z, and constants b and c. Xhyper allowed us to constrain and relax parameters interactively, and finally generate a script that would automatically build the hyperboloid in our chosen parametric solid modeller (Figures 9, 10). I am going to emphasise an aspect of this story to make my point about the nexus between the 'architect as architect' and 'mathematician as mathematician' as one condition, and the architect as being computationally learned as the other. 

Gaudi's approach to this surface modulation is extraordinarily subtle. The general condition for his approach to composition at the Sagrada Família Church in his final 12 years was for each hyperbolic surface to intersect with two others. One component surface has only to tilt by a fraction of a degree for the character of the composition to change markedly (Figure 11). Therefore despite Xhyper zooming in on a best-fit solution conforming to our captured points, and a script that would recompose an entire parametric assembly such as the nave clerestory window in less than an hour, we had to override the constraints automatically generated by Xhyper 96 times until we had a solution that visually matched Gaudi's, even if apparently the final tweak had to conflict somewhat with the data. But this is the essential point: Gaudí did not have access to these tools. In our quest to unravel his geometries, which were slightly approximate due to the modelling process itself, changes to the plaster in time and humidity effects, we had inadvertently built him a design tool. For armed with our digital tools he could have modelled his surfaces roughly using clay, for instance, digitised this approximate model with a scanner, and sought appropriate ruled surfaces using Xhyper. The fact that he did not have this advantage makes his method no less prescient in many ways, and neatly frames the tension between what designers might do for themselves using computation, and what they might request of others more expert than the necessarily junior status of the architect as mathematical dilettante. 
Figures 9 and 10. Mark Burry (programming Peter Wood), Xhyper, Sagrada Família Church, Barcelona, 1994. Xhyper is a standalone software that takes surface point measurements as input and calculates the best- fit hyperboloid of revolution to suit. The software offers a choice between Simplex and Hill Climbing optimisation algorithms. Mark Burry. 
Figure 11. Mark Burry, overlay of digital and physical data, Sagrada Família Church, Barcelona, 1994. Changing the parameters even by small degrees has profound effects on the outcome: here the darker shaded surfaces are seeking a match with Gaudi's original model (pale grey). © Mark Burry. 
BEYOND DESIGN COMPUTATION 
My first example looked at a situation where the designer uses computation in his/ her role as a solution seeker, in this case for building elliptical flattened arches. In the second example, computation is engaged as a design generator as in the case of the later Gaudí. One is a deceptively simple 2D profile challenge; the other is a 3D compositional strategy with communication being the key driver. The architect was hardly remote from the computation involved in either example. What is clear is that even a great designer will not know what to ask for when collaborating with a mathematician if they have no working knowledge of computation, and designing with the computer is not necessarily computational design in this regard. 

Up until the end of the 19th century, descriptive geometry was an essential component of an architectural education for most schools of architecture. The arrival of new materials and techniques following the Industrial Revolution, a century or more earlier, led to the gradual erosion of the inevitability of masonry construction for architects. Until the introduction of tensile structures and poured compressive materials, computation of statics and the intersection of volumes and surfaces through projection presented a tidy delimitation of the architect's computational demands: geometry, arithmetic and basic algebra were sufficient. Once the age of steel and concrete came to dominate material selections for major construction efforts, the different demands for expert calculations of these new structures resulted in greater computational sophistication, beyond the skills of most conventionally trained architects."2 In many countries this signalled the arrival of the engineer who would assert a new role as the designer's problem solver. Perhaps this division of labour is as responsible for the separation of the designer from mathematics as the arrival of non-Euclidean geometry and the great mathematical theorems and abstractions, especially during the 19th century? 

As the computer assists architects through making design computation so much more accessible, this facility also acts as a major stimulus to look beyond the richness of creative and practical problem solving. This facility can be argued as potentially freeing designers up to re-engage with mathematics at a more philosophical level than we have been doing for much of the last 100 years. Indeed, as my earlier quote from Theo van Doesburg in 1924 posits (see note 8), this is not exactly a new call, but it is nevertheless a reminder that spatial designers are not uniquely positioned in this regard, and that mathematicians have been engaging in n-dimensional thinking for considerably longer. The educational implications of what seems like a growing distance between design and mathematics have always struck me as being profound, especially when so few schools seem game enough to rise to the challenge, yet the opportunities for computation enriching (digital) design have never seemed greater than they were at the end of the 19th century. Coping with the practicalities of design resolution through computation might well act as the essential precursor to thinking more deeply about mathematics and the opportunities it offers for rethinking some fundamental aspects of design. 

NOTES 
1 'Compute.' Def. 1, The Oxford English Dictionary, second edition, 1989, CD-ROM. 

2 'Originally, the collective name for geometry, arithmetic, and certain physical sciences (as astronomy and optics) involving geometrical reasoning. In modern use applied, (a) in a strict sense, to the abstract science which investigates deductively the conclusions implicit in the elementary conceptions of spatial and numerical relations, and which includes as its main divisions geometry, arithmetic, and algebra; and (b) in a wider sense, so as to include those branches of physical or other research which consist in the application of this abstract science to concrete data. When the word is used in its wider sense, the abstract science is distinguished as pure mathematics, and its concrete applications (for example, in astronomy, various branches of physics, the theory of probabilities) as applied or mixed mathematics.' 'Mathematics.' Def. 1, The Oxford English Dictionary, second edition, 1989, CD-ROM. 

3 Ibid. 

4 Paul L Rosin and Michael LV Pitteway, 'The Ellipse and the Five-Centred Arch', The Mathematical Gazette, vol 85, no 502 (Mar 2001), pp 13-24. Published by The Mathematical Association. 

5 Joan Bassegoda Nonell, El gran Gaudí, Ausa (Barcelona), 1989, pp 511-28. 

6 MC Burry, The Expiatory Church of the Sagrada Família, Phaidon Press (London), 1993, Figs 46-50. 

7 Gaudi's education included descriptive geometry - approximately one-sixth of his university studies. His textbook was the French classic by CFA Leroy, which includes projects of hyperboloids of revolution: CFA Leroy, Traité de géométrie descriptive: suivi de la méthode des plans cotés et de la théorie des engrenages cylindriques et coniques, avec une collection d'épures composée de 69 planches, editions du Mallet-Bachelier, Gendre et Successeur de Bachelier, editeurs (Paris), 1855. 

8 'In architecture's next phase of development the ground-plan must disappear completely. The two-dimensional spatial composition fixed in a ground-plan will be replaced by an exact constructional calculation - a calculation by means of which the supporting capacity is restricted to the simplest but strongest supporting points. For this purpose Euclidean mathematics will be of no further use- but with the aid of calculation that is non-Euclidean and takes into account the four dimensions everything will be very easy.' Proposition 9: '1924 Theo van Doesburg: Towards a Plastic Architecture". Extracted from: Ulrich Conrads, Programs and Manifestoes on 20th-century Architecture, MIT Press (Cambridge, MA), 1970, p 79. 

9 http://www.patrikschumacher.com/Texts/Parametricism%20-%20A%20New%20Global%20 Style%20for%20Architecture%20and%20Urban%20Design.html 

10 The programming and interface design described here was undertaken by Peter Wood at Victoria University of Wellington, New Zealand, where we were colleagues at the time. 

11 The current Architect Director for the Sagrada Família Church, Dr Jordi Bonet, has provided a full account of Gaudi's use of proportion applied to his design for the church: in J Bonet. The Essential Gaudi, Editorial Pòrtic (Barcelona), May 2001 (MC Burry, translated from the Catalan original L'Ultim Gaudi.) 

12 Some countries, including Spain, still have structural calculation as part of the architect's core competencies. 

© 2011 John Wiley & Sons Ltd. 
AN INTRODUCTION TO CREATIVE EVOLUTIONARY SYSTEMS 
Peter] Bentley and David W Corne 

In this text, from the book Creative Evolutionary Systems (2002), Peter Bentley and David Corne define the components of a design process which has been configured to function in a manner akin to evolution and natural selection. Applying this in the context of creativity in design, evolution serves as an exploratory engine in producing variety within genotypic parameters, and natural selection acts as a search for effective functioning of phenotypic results. In relaying the steps from genotype to embryogeny and phenotype to evaluation, Bentley and Corne describe an iterative process for realising specific solutions from generalised information. As a necessary capacity, specificity is relevant to the means and context of evaluation. There exists an indirectness between the initial parameters and a specific outcome, but the expression of the phenotype demands a direct relevance to the measures by which it is being analysed. Where Bentley and Corne call these 'transformations, they are alterations in the phenotype for means of evaluation which lead inevitably to the genotype becoming less effective in moving the evolution forward. As a specific set of procedures, evolutionary algorithms (EAs) operate through recursive steps of constant reconfiguration of initial generative parameters through means of evaluation. In a conceptual manner, this describes a functioning design space, a combinatorial matrix of potential solutions developed through examination and selection. 

INTRODUCTION 
In many ways, using the word 'creativity' in a field of computer science opens up a Pandora's box of controversy. Convincing people that our creativity can be enhanced using computer software is sometimes difficult. Arguing with people that the techniques themselves might be capable of creativity is often futile. But there are an increasing number of researchers prepared to try. 

These are the researchers who pioneer creative evolutionary systems. Their programs enable artists to evolve stunning pieces of art, or allow musicians to create new sounds and new compositions. By guiding evolution as it happens, the users are able to explore new ideas as they emerge through the mechanisms of evolution. Many find this a highly rewarding activity - exploring the space of possible images or sculptures or compositions, and being constantly surprised as possibilities are revealed to them. Other creative evolutionary systems take this approach one step further. Guidance is provided by automatic software routines that judge evolving solutions without the need for human input. Designs are evolved from random blobs to fully functional forms. Novel circuits, boat hulls, architectural forms, aircraft manoeuvres, even chemical structures are now routinely evolved by computers. This automatic generation of innovation by creative evolutionary systems allows our designers to consider more solutions faster. These systems allow us to sidestep limitations of 'conventional wisdom' and 'design fixation'. Creative evolutionary systems can even suggest entirely new methods and principles that we can then exploit in our own designs. 

The list of applications of these approaches is long and increasing at a ferocious rate. But we are still only taking the first steps along this road. We are still learning how to enable evolutionary computation to provide us with this kind of assistance. There are as many problems yet to overcome as there are debates about the topic. The chapters in this book are our best answers so far. 
Thankfully we have a very good mentor to inspire us and help us when we run into difficulties. Natural evolution is a very creative problem solver, and the solutions of nature are ever present to remind us just what evolution is capable of. Can we make a machine 3 millimetres long that is capable of flying under its own power? What about giving it sight, or the ability to keep itself functioning by converting chemicals into energy, or even the ability to make copies of itself? We simply do not have the know-how to achieve these marvels. But this is what nature has achieved in a creature as simple as a fly. Other examples abound. For example, would we have thought of turning flippers designed to propel fish through the water into arms and highly dextrous hands? Would we have thought of using hair to make the horn of a rhinoceros? Despite the constraints nature is faced with - only a small number of materials available, the fact that life must grow from a single cell and maintain itself - the creativity of nature is overwhelming. We do not have to wait for some science fiction scenario where aliens from other worlds give us superior technology. The technology of the natural world is already all around and within us - and it is vastly superior to our technology. 

A FRAMEWORK FOR CREATIVE EVOLUTION 
From this investigation of creative evolutionary systems, we can construct a framework that contains five elements (see Figure 1): 

+An evolutionary algorithm 
+A genetic representation 
+An embryogeny using components 
+A phenotype representation 
+Fitness function(s) and/or processing of user input 

To summarise, a creative evolutionary system requires some kind of evolutionary algorithm to generate new solutions. The algorithm modifies genotypes defined by the genetic representation, which should be designed to minimise disruption caused by the genetic operators. An embryogeny (or mapping process) should decode the genotype and, using some kind of components, should construct the phenotype. The phenotype representation should be designed such that it permits quick and efficient evaluation by the fitness function(s). It is likely that the evolutionary algorithm, the genetic representation, and, to some extent, the embryogeny will be generic and suitable for reuse for most problems without modification. The phenotype representation and fitness functions will usually be specific to the current application of the system. The fitness functions may also be designed to process human input rather than actually evaluate the solutions directly. The following sections explore the elements of the framework for explorative evolutionary systems in more detail. 

Figure 1. The five elements of the framework for creative evolutionary systems. Peter Bentley and David Corne. Image from original publication. Reprinted by permission of Elsevier Books. © Elsevier Books. 

EVOLUTIONARY ALGORITHM 
The evolutionary algorithm forms the core of any evolutionary system. ... [T]here are four main EAs in use today: the genetic algorithm, genetic programming, evolutionary strategies and evolutionary programming. Sadly, only the GA and GP are commonly used for the exploration of creative solutions. The reason for this can perhaps be found in the way these algorithms work. The genetic algorithm maintains genotypes and phenotypes, with a mapping between the two. [T]his distinction has helped to encourage some GA researchers to use component-based genotype representations that map on to the phenotype representations, thus allowing explorative evolution to begin. In the same way, genetic programming also makes use of genotypes (this time with tree structures) that are mapped onto phenotypes such as programs, images or circuits. GP has the advantage that its genetic representation requires the use of smaller components (in the function and terminal sets), so all applications of GP demonstrate the explorative power of evolution. This may explain why the first notion of 'invention machine' came from John Koza, the inventor of GP - his algorithm ensures that explorative evolution will always take place. In contrast, algorithms such as evolutionary strategies and evolutionary programming make no distinction between genotype and phenotype. By directly modifying the solution and with no provision for mapping to new representations, these approaches make the use of components to construct solutions difficult to implement and hence almost unheard of in this field. Nevertheless, there is nothing inherent in any evolutionary algorithm that prevents its use in a creative evolutionary system. 

Within any evolutionary algorithm there are other issues that must be tackled. Handling multiple objectives, multimodality, noise, premature convergence and fuzzy or changing fitness functions must all be considered. Solutions to all of these problems, using ideas such as Pareto optimality, region identification, speciation, variable or directed mutation rates and steady-state GAs are now emerging in evolutionary computation (Bentley 1999; Parmee 1999; Vavak and Fogarty 1996). These issues, although important, are not the most significant consideration for creative evolution. Indeed, even the choice of evolutionary algorithm (or indeed any other search algorithm) is secondary to the representations, for it is the representations that permit evolution to explore. 

GENOTYPE REPRESENTATION 
The genotype representation defines the search space of the algorithm. A poor representation may enumerate the space such that very dissimilar solutions are close to each other, making search for better solutions harder. For creative evolutionary computation, where genes will represent (directly or indirectly) a variable number of components, the search space is typically of variable dimensionality, thus making its design even harder (Gero and Kazakov 1996). 

There are also other problems. Because of the use of components to represent solutions, the likelihood of epistasis dramatically increases. Not all component-based representations will have this effect (eg, a one-to-one mapping with a voxel representation allows both exploration and zero epistasis). However, most components are inherently linked to their companions for the solution to work as a whole. A circuit relies on the links between its components, a melody relies on links between notes, a house relies on links between walls, a program relies on links between commands. These linkages all mean that corresponding genes become epistatically linked, resulting in potentially serious problems for evolution. With polygeny so prevalent in these problems, great care must be taken in the design of the genotype representation and corresponding genetic operators to minimise the disruption of inheritance. 

Practitioners of GP have long been aware of these problems, and many solutions are now in existence. Modifications can be made to the genetic representation to increase functionality and decrease disruption, for example, encapsulation, ADFS, ADIS, ADLs and so on (Koza et al 1999). New genetic operators that enforce typing help ensure that genetic trees are not shuffled too drastically during the production of offspring (Page, Poli, and Langdon 1999). 

GAs do not require the use of tree-structured genotypes, so genetic representation- based problems are often less prevalent. GAs can be used to evolve variable-length genotypes and structured genotypes, typically with operators designed to perform crossover only at points of similarity between two parent genotypes (for example, Bentley and Wakefield 1996; Harvey 1992). Advanced GAS designed to minimise damage caused by disruption of epistatic links between genes have also been demonstrated (Goldberg 1999). In addition, GAs do not suffer from the classic GP problem of bloat, where genotypes tend to increase in size, with redundant genetic material becoming ever greater in solutions. It is clear that the creation of suitable genetic representations and corresponding operators is a considerable problem in its own right. Furthermore, recent research indicates that significant benefits may be gained from using less complex genetic representations and operators, instead making use of embryogenies of greater complexity (Bentley and Kumar 1999). 

EMBRYOGENY 
An embryogeny is a special kind of mapping process from genotype to phenotype. Within the process, the genotype is now regarded as a set of 'growing instructions' recipe that defines how the phenotype will develop. Polygeny - phenotypic traits being produced by multiple genes acting in combination is common. Research in this area has revealed some of the potential of these advanced mapping processes. Advantages include reduction of the search space, better enumeration of the search space, the evolution of more complex solutions and adaptability (Bentley 1999). 

Embryogenies are widely used for explorative evolutionary systems because they provide the mechanism for constructing whole solutions from components. Three types of embryogeny are used today. The first and most common is external, where a programmer writes the software that performs the mapping, and the process cannot be evolved - it is external to the genotype (Bentley 1999). More recently, explicit embryogenies have become popular, with every step of the growth process explicitly held as part of the genotype and evolved (Bentley 1999). Examples include cellular encoding, used by Koza and team for the evolution of analog circuits (Koza et al 1999); Lindenmayer systems, used by Coates (1997) for the evolution of architectural forms; and shape grammars, used by Gero for the evolution of floor plans (Schnier and Gero 1996). Despite the considerable success of these embryogenies, they often require complex additions to genetic representations and operators to allow evolution to work. The third type of growth process, the implicit embryogeny, has shown the most exciting results and greatest potential in recent work. Instead of evolving the mapping as a set of explicit steps in the genotype, an implicit embryogeny uses a set of rules, typically encoded as binary strings in a GA genotype. For each solution, a 'seed' component is created, and then the rules are iteratively applied. Over many iterations, with rules activating and suppressing each other, the growth, position and type of new components are built up, finally resulting in the development of a complete solution. This emergent growth process shows remarkable properties of scalability, with the genotype describing solutions of increasing complexity without any increase in the number of rules needed; the rule-directed growth process is simply allowed to run longer. This is in contrast to all other approaches, which require significantly larger genotypes to define the increased growth of more complex solutions (Bentley and Kumar 1999). 

The process of mapping from genotypes to phenotypes is clearly of importance to the investigation of explorative evolution. Issues of scalability, evolvability and biases induced in search have yet to be considered by researchers in any great depth. Increased understanding in this area would benefit both computer science and developmental biology. 

PHENOTYPE REPRESENTATION 
Once constructed by the embryogeny, the resulting solution is defined by the phenotype representation. Typically this representation is specific to the current application: if we are evolving circuits, the representation might define networks of connected components; if we are evolving buildings, the representation might define exterior shapes and/or interior walls, floors and stairs. An important criterion is evaluation. Typically the phenotype representation will be designed to allow direct evaluation by fitness functions, without intermediate transformations or calculations. A poor choice will detrimentally affect processing times and solution accuracies. 

The distinction between genotype, embryogeny and phenotype representations is often blurred in this field. Some GP practitioners regard all three to be the same. Others, such as Jakobi's work on evolving neutral networks (Jakobi 1996) or Taura's work on evolutionary configuration design (Taura and Nagasaka 1999), use different and distinct representations for each stage. It is still unclear whether the component growing process of the embryogeny should use the same representation as the phenotype. Should the phenotype be represented by blocks, or should the blocks be merged into a single, whole description of the solution? For example, should an evolved musical composition be represented by a series of notes or by a single, complex waveform in the phenotype? The answer is likely to depend on the fitness functions. 

FITNESS FUNCTIONS AND/OR PROCESSING OF USER INPUT 
The fitness functions must provide an evaluation score for every solution. For creative evolution computation, this is often a little harder. Typically the use of components rather than a parameterised solution means that early results can be chaotic, to say the least. A design of a car may resemble a shoe; a melody may sound like a burglar alarm. Before evolution has had time to improve these initially random solutions, they can be nothing at all like the desired result. And yet the fitness functions must always be able to provide a fitness score that makes sense. The task is made even harder when unknowns or approximations must be incorporated into the evaluation, or when constraints and objectives are varied to aid exploration. Potential solutions include the use of custom-designed modular functions (Bentley 1996) and fuzzy logic (Parmee 1999) to cope with such problems. 

As discussed earlier, many creative systems use human input to help guide evolution. Artists can completely take over the role of a fitness function (Sims 1991; Todd and Latham 1999), and more recent work has investigated the use of these techniques for evolving photorealistic images of faces - potentially useful for the identification of criminals (Hancock and Frowd 1999). These applications raise numerous human- computer interfacing issues. Will a creative system detrimentally affect the style of an artist or the memory of a crime victim? These software tools have been shown to aid imagination and creativity, but what is the best method to let the user inform evolution of his/her preferences, and what is the best method for the computer to report the structure and contents of the space being explored? Clearly, further research is required to address these issues. 

EXAMPLE CREATIVE APPLICATION 
To illustrate the potential of creative evolutionary systems, we will now explore a simple application - the generation of shapes that, when constructed out of paper, fall as slowly as possible to the ground. In this case we will not use human interaction; this will be the second type of creative evolutionary system described earlier. 

We can construct a simple creative evolutionary system, following the framework provided above. A 'simple GA can be used as the evolutionary algorithm (Goldberg 1989). The genotype representation can be a flat chromosome of 16 binary coded genes. A very basic mapping process or embryogeny can be used to derive eight (x, y) parameter values from each chromosome, join each vector to its successor by an edge, join the last vector to the first with an edge, and fill the resulting shape. An easy way to perform this process is to execute PostScript instructions output by the system, print the shapes, and use scissors to cut out the shapes (see Figure 2). The resulting paper phenotypes ('represented' by reality) can then be tested by releasing them from a height of 150 millimetres three times in succession. Each phenotype can be allocated their fitness score by calculating 1/(time, + time, + time,), ensuring that evolution would attempt to generate phenotypes with increased 'falling times'. 
Figure 2. The paper fall application uses eight vertices as its components. These are extracted from the genotype and transformed into real paper shapes by the use of a 'connect the dots and fill the shape' embryogeny (and someone to print and cut out the shape). Peter Bentley and David Come. Image from original publication. Reprinted by permission of Elsevier Books. Elsevier Books. 
Figure 3. Two paper shapes evolved to fall slowly through the air. Both are members of the final generation, and both use a smaller 'arm' or flap to cause the shape to rotate as it falls, like a sycamore seed. (Not shown at scale used for testing.) Peter Bentley and David Come. Image from original publication. Reprinted by permission of Elsevier Books. Elsevier Books. 
This application illustrates the use of an extremely basic component-based embryogeny representation. As Figure 2 illustrates, the components are simply eight vertices with (x, y) positions. Despite these components having no size and no type, the unconstrained freedom of position of each vertex relative to all other vertices means that this component-based representation allows the definition of a vast number of different shapes. Clearly this is a knowledge-lean representation (no information about which shapes are best is provided). It should also be clear that the representation allows evolution to explore the solution space in an unconstrained manner and that there will be many millions of good and bad solutions for this problem. 

Predictably, the use of real-life testing is time-consuming and laborious (especially if you make the mistake of performing the experiment yourself instead of enlisting the services of a student). Consequently, for the purposes of this illustration, population sizes of 10 individuals were used in an evolutionary run of 10 generations. 

The results were interesting. Despite these excessively low values, evolution was able to make significant improvements on the time taken for the shapes to reach the ground. In the experiments, times taken for initially random shapes to fall 150 millimetres varied from 0.7 seconds to 1.8 seconds. By the 10th generation, all shapes took, on average, more than two seconds to fall the same distance. Figure 3 shows two of the solutions in the final population. Convergence has begun to occur, with most shapes using the same technique of having a smaller flap that causes the shapes to rotate as they fall. The main benefit of this solution appears to be the way rotation stabilises the motion of the shape, preventing it from slipping sideways in the air and plummeting to the ground at tremendous speed. Even with the very limited resources evolution was given, a 'creative' solution to the problem was found. 

FROM CREATIVE EVOLUTIONARY SYSTEMS TO CREATIVITY 
This section of the introduction has introduced the ideas behind creative evolutionary systems. Such systems are used to aid the creativity of users by helping them to explore ideas during evolution, or help show users new ideas and concepts by generating innovative new solutions to problems previously thought to be only solvable by creative people. A brief background of the field was given, showing how difficult applications such as architecture, programming, circuit design, and art and music composition triggered the development of a new approach. By using human interaction we can enable the evolution of aesthetically pleasing and other difficult-to-evaluate solutions. By employing knowledge-lean component-based representations, we remove constraints to search in the representations and allow evolution to assemble new solutions. Using such methods we have turned evolution into an explorer of what is possible, instead of an optimiser of what is already there. [...] 

REFERENCES 
Bentley, PJ, 'An Introduction to Evolutionary Design by Computers', in PJ Bentley (ed), Evolutionary 

Design by Computers, Morgan Kaufmann (San Francisco), 1999, pp 1-73. 

Bentley, PJ, Generic Evolutionary Design of Solid Objects Using a Genetic Algorithm, PhD Thesis, 

Division of Computing Control Systems, Department of Engineering, University of Huddersfield, 1996. 

Bentley, PJ and S Kumar, "Three Ways to Grow Designs: A Comparison of Embryogenies for an 

Evolutionary Design Problem', in Genetic and Evolutionary Computation Conference (GECCO '99), 14-17 July 1999, Orlando, Florida, 1999, pp 35-43. 

Bentley, PJ and JP Wakefield, Hierarchical Crossover in Genetic Algorithms', in Proceedings of the Ist 

Online Workshop on Soft Computing (WSCI), Nagoya University, Japan, pp 37-42, 1996. Coates, P 'Using Genetic Programming and L-Systems to Explore 3D Design Worlds', in R Junge (ed), 

CAADFutures '97, Kluwer Academic (Munich), 1997. 

Gero, JS and V Kazakov, 'An Exploration-Based Evolutionary Model of Generative Design Process', 

Microcomputers in Civil Engineering, 11: 209–16, 1996. 

Goldberg, DE, Genetic Algorithms in Search, Optimization and Machine Learning, Addison-Wesley 

(Reading, MA), 1989. 

Goldberg, DE, "The Race, the Hurdle, and the Sweet Spot: Lessons from Genetic Algorithms for the 

Automation of Design Innovation and Creativity', in PJ Bentley (ed), Evolutionary Design by Computers, Morgan Kaufmann (San Francisco), 1999. 

Hancock, P and C Frowd, 'Evolutionary Generation of Faces', in PJ Bentley and DW Come (eds), 

Proceedings of the AISB'99 Symposium on Creative Evolutionary Systems (CES), Society for the Study of Artificial Intelligence and Simulation of Behaviour (AISB), 1999. 

Harvey, I, 'The SAGA Cross: The Mechanics of Recombination for Species with Variable-Length 

Genotypes', in R Manner and B Manderick (eds), Parallel Problem Solving from Nature II, North- Holland (Amsterdam), 1992, pp 269–78. 

Jakobi, N, 'Harnessing Morphogenesis', in Proceedings of the International Conference of Information 

Processing in Cell and Tissue, 1996. 

Koza, JR, F Bennett, D Andre and MA Keane, Genetic Programming III: Darwinian Invention and Problem 

Solving, Morgan Kaufmann (San Francisco), 1999. 

Page, J, R Poli and W Langdon, 'Smooth Uniform Crossover with Smooth Point Mutation in Genetic 

128 Computational Design Thinking 

129 An Introduction to Creative Evolutionary Systems 

Programming: A Preliminary Study', in R Poli, P Nordin, WB Langdon and T Fogarty (eds), Proceedings of the Second European Workshop on Genetic Programming-EuroGP'99, Gothenburg, 26-27 May, 1999, Springer Verlag (Berlin), 1999. 

Parmee, I, 'Exploring the Design Potential of Evolutionary Search, Exploration and Optimization', in PJ Bentley (ed), Evolutionary Design by Computers, Morgan Kaufmann (San Francisco), 1999. Schnier, T, and JS Gero, 'Learning Genetic Representations as an Alternative to Hand-Coded Shape 

Grammars', in J Gero and F Sudweeks (eds), Artificial Intelligence in Design '96, Kluwer (Dordrecht), 1996, pp 39-57. 

Sims, K, 'Artificial Evolution for Computer Graphics', Computer Graphics 25(4): 319-28, 1991. Taura, T and I Nagasaka, 'Adaptive Growth Type Representation for 3D Configuration Design', in P} 

Bentley (ed), First Special Issue on Evolutionary Design, Artificial Intelligence for Engineering Design, Analysis, and Manufacturing (AIEDAM) 13(3): 171–84, 1999. 

Todd, S, and W Latham, "The Mutation and Growth of Art by Computers', in PJ Bentley (ed), Evolutionary 

Design by Computers, Morgan Kaufmann (San Francisco), 1999. 

Vavak, F and T Fogarty, 'Comparison of Steady State and Generational Gas for Use in Nonstationary 

Environments', in Proceedings of the IEEE 3rd International Conference on Evolutionary Computation ICEC'96, Nagoya, Japan, 1996. 

From Peter Bentley and David Come, 'An Introduction to Creative Evolutionary Systems', in PJ Bentley and DW Corne, Creative Evolutionary Systems, Academic Press (San Diego), 2002. Reproduced by permission of Elsevier Books, Elsevier Books. 



CONSTRAINED GENERATING PROCEDURES 
John H Holland 

Emergence is a process by which complex phenomena arise from the interaction of simple conditions, where the possibilities of such complex behaviours cannot be identified within the individual initial assertions. This is inherent to any procedural process when compared with a discrete, directed one. John Holland, a scientist who established in the 1970s the concepts and techniques for genetic algorithms, expands upon the computational mechanisms underlying emergent systems in this text. In the book Emergence: From Chaos to Order (1998), Holland explains in this chapter the properties of a 'model' in which systems containing emergent characteristics can be produced. The use of the term 'model' is unique as it describes the collection of procedures and operators which interact to produce a set of possibilities; the model is not defined as the finite product of a select set of individual operations. Holland describes a key feature of these procedures as the 'transition function', which is a mapping of the possible states of a system that can arise from this function. Such an operational framework is expressed further by Holland in the use of cellular automata. Fundamentally, this asserts the capacity of simple agents (or functions) to develop complex emergent behaviours but only when engaged into a combinatorial network - where the conditions of interaction are as pivotal as the properties of the agents themselves. 

The Greeks already knew the benefits of describing diverse objects (machines) in terms of elementary mechanisms (the lever, the screw, and so on). This chapter develops a similar setting for describing the diverse systems that exhibit emergence. The Greek approach to machines also provides a helpful, intuitive guideline for developing this setting. It suggests looking at emergence in terms of mechanisms and procedures for combining them. To make this work, we have to extend the idea of 'mechanism'. The generalisation used here comes close to the physicist's notion of an elementary particle (say a photon) as a mechanism for mediating interactions (causing an electron to change its orbit around an atom). We can use this extended notion of mechanism to provide precise descriptions of the elements, rules and interactions (for example, agents) that generate emergent phenomena. And we can compare quite different systems, increasing our chances of finding common rules or laws that describe systems exhibiting emergence. 

[...] We'll be mimicking the path that scientists often follow in moving from intuition to precision. The result is an explicit definition of a broad class of models I'll call Constrained Generating Procedures (CGPs). The models that result are dynamic, hence procedures; the mechanisms that underpin the model generate the dynamic behaviour; and the allowed interactions between the mechanisms constrain the possibilities, in the way that the rules of a game constrain the possible board configurations, All the systems we've looked at so far can be described as CGPS. Potentially, any constrained generating procedure can exhibit emergent properties. 

In outline, the path to constrained generating procedures involves the following steps. 

1. First, we'll translate the notion of rule (for instance, the rule for jumping in checkers, or Hebb's rule) into the notion of mechanism. As with rules for games, or laws for a physical system, mechanisms will be used as the defining elements of the system. 

Roughly, a mechanism responds to actions (or information), processing that input to produce resultant actions (or information) as output. The simple lever, one of humankind's earliest discoveries, provides an easy example (see Figure 1): when you pull down on one end of the lever (the input), you get a force at the other end (the output) that is multiplied by the ratio of the two lever arms (the processing). More complicated mechanisms may have several inputs and may produce several different outputs - think of a mechanism that sorts coins. The word 'mechanism has variant meanings in common usage, but I'll use a definition and notation that restricts the word to a single, unambiguous meaning. 

2. Next, we'll define ways of linking mechanisms to form networks; these networks are the constrained generating procedures, the CGPS. Because most models involve more than one kind of mechanism (as the different elementary particles in physics), the setting must make evident how the actions of one mechanism can affect others. 

Within this setting, it is the interaction of the mechanisms that generates complex, organised behaviour. Typically, the mechanisms allowed will be few in kind and simple to describe, enforcing the deletion of many details. Still, the interactions provide possibilities not easily anticipated by inspection of the individual mechanisms. The complexity increases when multiple copies of the basic mechanisms are allowed, as seen in the case of ant colonies and neural networks. 

We'll first look at CGPs in which linkage captures the constraints imposed by the fixed landscape of a board game, or the geometry of a physical system. Later we'll look into a generalisation whereby links can be made and broken from within the CGP, changing the underlying geometry. This facilitates models based on mobile agents. 

3. Once the mechanisms are linked, we come to the counterpart of the game tree the set of possibilities generated by the constrained interactions of the mechanisms. The examples already examined show that the tree of states is a handy way of modelling the possible courses of action the strategies and dynamics of games. Now we want to extend that notion to systems in general. 

The transition function is derived from the “conservation of moments” law for levers, a step along the way to the powerful “conservation of momentum” law of modern physics. 
Figure 1. A simple mechanism the lever. Reprinted by permissions of Basic Books, a member of the Perseus Books Group, and John Holland. © Brockman/Holland. 
To accomplish this objective, we have to define the state of a whole CGP in terms of the states of the component mechanisms. We have to be able to do what we did for the checkers player and neural nets: condense everything about the CGP that is relevant to future possibilities into a single entity called its global state. Then we go on to describe the legal ways of making transitions from state to state... we'll see that the possibilities for state changes are described precisely by a transition function. 

4 Finally, we want to provide a specific procedure in CGPs for defining hierarchies of subassemblies, using more complex mechanisms built up from the basic mechanisms. Advantages in explanation will accrue, along the lines noted by Simon in 1969, and the resulting organisation parallels the hierarchical nature of most systems exhibiting emergence. 

The major advantage here is again the one the Greeks perceived. It should be possible to treat a whole CGP as a mechanism that can be used to build still more complex CGPs. This treatment parallels the use of subroutines, themselves composed of elementary instructions, as elements of a computer program. By making such definition a part of the CGP apparatus, we can capture the hierarchies that are major features of systems exhibiting emergence, like the molecule, organelle, cell, organ, organism, ... hierarchy in biology. 

MECHANISMS 
The term 'mechanism', as used here, is defined by means of a transition function. ... For a mechanism, we must first define the state of the mechanism before we can define the transition function. 

As a simple example of the state of a mechanism, consider the state of a watch. The watch has (a) a wound spring that transmits power (b) through a ratchet that advances (c) to produce the rotary motion of an indicator (hand). The state of the mechanism includes the tightness of the spring and the position of the ratchet, as well as the position of the indicator. 

When we look at the watch as a mechanism, we see a succession of states as the hands move around the face of the watch. Internally, this movement of hands is caused by movement of the ratchets and gears, unwinding of the mainspring, and so on all components of the state. This state trajectory, the sequence indicated by the movement of the hands, is the dynamic of this mechanism. The watch is somewhat atypical of mechanisms because its input is intermittent. The trajectory of states proceeds autonomously between windings of the mainspring, whereas mechanisms more typically are under the control of a steady stream of inputs (similar to the succession of decisions that determine the unfolding of a game). Still, the watch provides an example of the kind of dynamic we can expect from mechanisms, and illustrates the way in which simpler mechanisms (the mainspring, ratchets, and so forth) can be combined to yield a more complex mechanism, Much as the combination of rules defines a game, combinations of complex mechanisms built from simple ones have a major role in CGPs. 

We have not yet said what outputs a mechanism produces in response to an input sequence. It simplifies the notation, without changing the range of rule-governed systems that can be modelled, if we simply take the state of the mechanism to be its output, as if we could observe everything or anything that takes place in the mechanism. Of course, in realistic cases we may only be able to observe a part of the state, as when we can see the hands of a watch but not the internal works. That complication is easily handled when we provide for the interaction of mechanisms as we'll see in the next section. With this simplification the transition function f completely specifies the mechanism's behaviour. In what is called an abuse of notation, the symbol ƒ will be used as the mechanism's name, in addition to designating its transition function. 

INTERACTIONS AND LINKAGE 
Our basic concern in defining constrained generating procedures is to provide a setting in which we can study the complexities, and examples of emergence, that arise when rule-governed entities interact. I've adopted mechanisms, with their defining transition functions, as the formal counterparts of rules. Emergence, in this setting, is closely connected to the generated complexity that arises when multiple copies of like mechanisms interact. Our concern now is to provide for the interaction of copies of a set of elementary mechanisms. 

A little more carefully, the definition of a constrained generating procedure starts with the selection of a set of mechanisms F, called primitives, from which everything else will be constructed. The mechanisms in F are the counterparts of the elementary mechanisms the Greeks used to construct all machines. When the CGP is used to model a game, neural net, cellular automaton, or other system exhibiting emergence, the primitives are the elements that are connected to form the model. 

In this precise setting, one mechanism is connected to another when the state sequence of the first mechanism determines a sequence of values for one of the second mechanism's inputs. Once F is selected, we connect copies of mechanisms in F to form a network of interacting mechanisms, as when we connect neurons to form a neural network. In short, we obtain a particular CGP when we select a set of primitives F and then interconnect them (see Figure 2). 

*** 

Once each mechanism in the CGP is assigned an index, we can develop a kind of lattice to illustrate the interconnections and neighbourhoods in the CGP, a lattice reminiscent of the tree that showed how moves are interconnected in a game. Node i in the lattice corresponds to the mechanism with index i. If mechanism i is connected to mechanism j, then an arrow runs from i to j in the lattice. For example, the mechanisms might be connected in a regular square array, like a checkerboard. Then each node would have four neighbours to which it is connected, and the whole array could be laid out as a square tiling pattern. However, any kind of connection pattern is allowable; it need not be regular in any way. We can even allow the number of nodes (copies of mechanisms) to become infinite, yielding an infinite array such as a checkerboard extending indefinitely in two directions. 

Figure 2. Constructing CGPs from a set of primitive mechanisms. Reprinted by permissions of Basic Books, a member of the Perseus Books Group, and John Holland. © Brockman/Holland. 

CELLULAR AUTOMATA AS CGPS 
We can make an early test of the ability of CGPs to model complex systems by looking at cellular automata. Cellular automata are the brainchildren of two of the most renowned mathematical physicists of the 20th century, Stanislaw Ulam and John von Neumann. Ulam's idea (see the 1974 collection of his papers) was to construct a mathematically defined model of the physical universe within which one could build a wide range of 'machines'. This model physics would retain the essence of a real physics - it would have a geometry and a set of locally defined laws (a transition function) that held at every point in that geometry. Ulam saw that you could use a checkerboard geometry with an identical finite set of states assigned to each square (see Figure 3 for a simple example, Conway's automaton). Ulam completed the construct by specifying a transition function that determines the way the states change over time. Von Neumann then used this construct to design the self-reproducing machine... Cellular automata have proved to be useful tools for investigating complex systems, so they constitute an appropriate test for CGPs. 

Each square, or cell, in the cellular automaton, with its associated states and rules, becomes a mechanism in the CGP framework. That is, the states of the cell become the mechanism's states, and the rules of the cellular automaton define the mechanism's transition function. In these terms, the cellular automaton is characterised as a CGP employing copies of a single mechanism, connected to form the regular square array described in the previous section. By picking an appropriate pattern of states in this array, we can provide a kind of active blueprint for any conceivable machine. 

A SIMPLE CELLULAR AUTOMATON 
One of the simplest cellular automata was designed and named by John Conway. It is called 'Life'. Life, despite its simplicity, offers some fascinating examples of emergence. The automaton will be fully defined here, and we will discuss one of the examples of emergence therein in some detail, but the reader should see Gardner's 1983 book to gain an idea of the extensive research that has been devoted to this automaton. In CGP terms, Life is formed from copies of a single mechanism that has just two states, represented by 1 and 0. For descriptive purposes, we can think of a cell as occupied by a particle if the corresponding mechanism is in state 1; otherwise it is empty. The lattice of connections is a square array, as described earlier, but now all eight squares that surround a given square are connected to that square. That is, each node has eight immediate neighbours (see Figure 3). 

The transition function is also easy to describe. If a cell is empty (state 0), and exactly three of its eight immediate neighbours are occupied (state 1), then that cell becomes occupied on the next time-step (its state changes to state 1); otherwise, it stays empty. If a cell is occupied, and either two or three of its immediate eight neighbours are occupied, the rest being empty, then on the next time-step the cell remains occupied; otherwise, it becomes empty. That encompasses the full range of possibilities. 

Basic mechanism for Conway's automaton: 8 inputs, 2 states {1, 0} 
Transition function: If the state is 0 and exactly three neighbors are in state 1, 
then the state becomes 1; otherwise it remains 0. If the state is 1, and either two or three neighbors are in state 1, then the state remains; otherwise it becomes 0. 
Basic mechanism connected to its immediate neighbors: 
One-step transitions for some simple state patterns: 
Figure 3. Conway's automaton. Reprinted by permissions of Basic Books, a member of the Perseus Books Group, and John Holland. © Brockman/Holland. 
Such a simple CGP would seem to offer little that is instructive concerning emergence, but that is not the case. The first surprise is that you can embed a general-purpose (fully programmable) computer in this simple space. Specifically, you can design a pattern of occupied cells in this space that interact so as to form a general-purpose computer. In consequence, any process that can be modelled on a computer can be imitated in the 'physics' of Conway's cellular automaton. I will not attempt a description of the complicated 'general-purpose computer pattern here, but will instead describe a much simpler example of emergence in Conway's automaton. 

GLIDERS 
Conway's automaton can contain a simple, mobile, self-perpetuating pattern called a glider. It consists of a pattern of five occupied cells, surrounded by empty cells (see Figure 4). The transition function just described produces a sequence of changes in the pattern over successive time-steps. Though five occupied cells are always involved, the pattern changes shape in a regular way and it moves ('glides') diagonally across the space. At intervals of four time-steps, the pattern recurs, now shifted one cell diagonally, say down and to the right, as in Figure 4. If you continue on in time, the pattern continues to go through the same sequence of changes, gliding downward across the lattice, as long as it does not encounter another pattern of occupied cells. The transition is so easy that you can try it on a piece of ruled paper. 

Simple though the glider is, it tells us much about emergence. First of all, the glider is not a fixed set of particles bound together and moving on a trajectory through the space. Rather, particles are continually being created and deleted to produce the glider. This persistence of pattern, despite a continual turnover of constituents, recalls the persistence of the standing wave that forms in front of a rock in a rapidly flowing stream. The glider is more complicated than the standing wave because it changes form as it moves, and it does not stay in one location. Still, the glider and its glide are a consequence of the simple laws of this universe. 

The possibility of such a spatially coherent moving pattern is not something easily determined by direct inspection of the laws of Conway's universe. The possibility only exists because of the strongly nonlinear interactions of the particles (states) in adjacent cells. Even if we restrict attention to the five-by-five array of cells sufficient to contain a glider, along with the immediate surround of empty neighbours, no extant analytical technique will predict the existence of a glider pattern. We can only discover the glider by observation, watching the laws play themselves out in different configurations. A five- by-five array in Life has 225~32,000,000 distinct configurations, so the task is not easy. 

If we undertake an empirical exploration, we soon find out that many configurations are ephemeral, lasting only a few time-steps before dissolving to a set of empty cells. Other configurations remain fixed in place, either retaining a fixed form or 'blinking' through a recurrent sequence of changes. Still other configurations migrate over long periods, assuming a multiplicity of forms, only to dissolve or wind up as a fixed configuration. The glider fits none of these categories, because it persists indefinitely while moving through empty space. 

Even though we may infer the persistence of the glider from extended observation, such inferences can be treacherous. For instance, there are patterns in Life that go through extremely long sequences of changes before ultimately dissolving. We can only be sure of the glider's persistence if we can prove this property within a theory founded on the laws that define Life. In this, Conway's artificial universe is not so different from the universe in which we live. Both require a combination of experiment and theory to discover and explain regularities. 

Once we establish the glider's persistence, we can think about it as a component of larger, more complicated patterns. In fact, this step was taken in showing that a general- purpose computer could be embedded in Life. In an embedded computer, the gliders serve as signalling devices, conveying information from one part of the pattern to another. Additional research showed that there are configurations that emit gliders, and others that respond to a collision with an incoming glider. These are the basic elements for the creation, transmission and reception of information. Once information can be transferred from point to point, it takes only some simple 'bit flipping" and storage schemes to provide computation. Thus, the existence of the glider encouraged a search for a way of constructing a general-purpose computer within Conway's universe. The ensuing search, and proofs, were relatively straightforward once conceived, but it is unlikely that they would have been pursued with the necessary intensity without this encouragement. 

LESSONS ABOUT EMERGENCE 
What lessons about emergence can we draw from this example? 

We see that: 

1 Rules (transition functions) that are almost absurdly simple can generate coherent, emergent phenomena. 

2 Emergence centres on interactions that are more than a summing of independent activities (imposed by the nonlinear rules in this case). 

3 Persistent emergent phenomena can serve as components of more complex emergent phenomena. 

Figure 4. Successive transitions of the glider state pattern in Conway's automaton. Reprinted by permissions of Basic Books, a member of the Perseus Books Group, and John Holland. 

All these points have been made earlier, but here they appear in a context so stark that nothing lies hidden in the complexity, nor is there any room for mysterious, unexplained activity. 

From John Holland, 'Constrained Generating Procedures', Emergence: From Chaos to Order, Perseus Books (Reading, MA), 1998, pp 125-42. Copyright © 1999 John H Holland. Reprinted by permission of Basic Books, a member of the Perseus Books Group. 


REAL VIRTUALITY 
Manuel DeLanda 

In this text, Manuel DeLanda details the relationship of physics and mathematics in the context of computational design. Starting with the long- established definition of 'virtual reality', DeLanda flips its connotation, and literally its syntax, distinguishing the difference between computing a particular result versus computing the processes which lead to a field of possible yet specific results. In addressing processes, the effort lies within mechanisms that describe potential, or as DeLanda describes - 'capacities'. The means which define the subsequent 'state space' are preoccupied with behaviour actions which entities exert or have had exerted upon them rather than properties, the static material description. The parameters of the process (variables and equations) do not hold the definition of a specific material entity - rather the processes need to be executed to unfold the nature and magnitude of physical interaction whose repercussion is a particular organisation of material properties. DeLanda cites certain historical benchmarks in physics and mathematics to delineate the framework for such a computational approach. He broadens this framework by providing a meta- logic by which process is defined in the number of interacting parameters – or 'degrees of freedom' - rather than being individual constructs based upon specific mechanisms that represent specific physical laws. 

Algorithms are the soul of software. They are mechanical recipes for the performance of tasks like calculating, searching or sorting, and are invaluable because computers lack the judgement necessary to use procedures in which every step is not specified unambiguously. Like other procedural entities algorithms are defined by what they do rather than by what they mean. One may be able to figure out what an algorithm does just by looking at the lines of code that implement it, and this activity may display some similarities with that of interpreting a piece of written text, but there is no substitute for actually running the algorithm and observing what it does. On the other hand, algorithms are older than computers so their evolution owes as much to mathematics as to information technology. Mathematics itself may be viewed, somewhat metaphorically, as a kind of software. This helps to eliminate some of the mystical connotations that usually accompany the term, since software is simply a type of machinery, 'soft machinery' as it were. But given the large diversity of fields that make up mathematics, the analogy has its limits. Even if we focused only on mathematical functions, algebraic or differential functions, for example, we would still have to modify the analogy. Functions, like algorithms, are defined by what they do, mapping one set of numbers into another set of numbers, for instance, but they are more abstract than algorithms: one and the same function may be given several algorithmic forms. 

This essay is concerned with the use of these procedural entities to model physical processes. It is precisely because these formal entities are defined by what they do, that is, because they exhibit a certain behaviour, that they can be used to simulate the behaviour of physical entities. The term 'virtual reality' nicely captures the idea that both functions and algorithms can be used to create simplified worlds in which the workings of the material world may be studied. The term, of course, is already in use to refer to a narrow class of computer simulations, those in which only the optical behaviour of physical objects is modelled: the behaviour of physical surfaces as they interact with light and the behaviour of light as it interacts with an observer. In other words, these simulations mimic not only the shading of objects or the casting of shadows, but also the perspective projection that such objects and shadows create in the plane of vision of a moving subject. Yet, despite the fact that the term has already acquired currency in relation to these simulations, it is also appropriate to designate any kind of simulated world created by mathematicians or computer scientists, even if these worlds do not have a visual representation. In other words, the term is also appropriate to refer, for instance, to the worlds created by classical physicists when they modelled the behaviour of gravity almost four centuries ago. 

It is in this extended sense that the term 'virtual reality will be used in this essay. What about 'real virtuality'? This term will be used to refer to the immanent patterns of becoming that physicists and other scientists have discovered in the world, partly through their use of formal models and computer simulations, partly through causal interventions performed in laboratories. We normally use the term 'law' to refer to these patterns, but it may be argued that this term was coined when scientists were still deeply religious and thought of themselves as revealing the secret rules that a legislating god had issued to govern his creation. Today's scientists are not nearly as devout as they used to be but the word 'law' has survived their change of allegiances. For purely practical purposes, of course, there is nothing wrong with keeping the word, but from a philosophical point of view its theological connotations are problematic. On the other hand, nothing would be achieved if we merely switched labels, from 'law' to something else, as if merely changing the way we talk about immanent patterns made any difference. Talk, as everyone knows, is cheap, and so is relabelling. So we must start by analysing a concrete case of a natural law, the law of gravity, and discover what in this case could properly be called 'real virtuality'. 

In his analysis of the character of physical law, the late physicist Richard Feynman argues that the law of gravity has three completely different versions. There is the familiar one in terms of forces and accelerations, the more recent one of using fields, and the least well-known version in terms of singularities, such as the minimum or maximum values of some parameter. As Feynman argues, the three different versions allow physicists to make the exact same predictions so they cannot be told apart by their consequences. Feynman, like many other famous physicists before him, subscribes to a philosophy of physics referred to as 'positivism'. A positivist believes in the mind-independent reality of objects and processes that are directly observable by us, all other entities being only useful theoretical constructs. To put it differently, Feynman believes that the task of physics is not to explain the inner workings of the world but only to produce compact descriptions that are useful to make predictions and that increase the degree of control we have of processes in the laboratory. But since the three versions of the law of gravity make the same predictions, it is useless to speculate which one of the three 'really explains gravitational processes. Are there really forces which act as causes to change the velocity of celestial bodies? Or does reality really contain gravitational fields? Or, even more strangely, is it all a matter of singularities? For Feynman there is no answer to these questions.' Moreover, the very question 'what is a law?' is not strictly speaking answerable, since the immanent patterns of becoming that laws refer to are not directly observable. This is why when positivists use the word 'law' they typically use it to refer to the functions or equations that express a law, equations being directly observable when written on a piece of paper. 

Realist philosophers - those who believe in the mind-independent reality of hydrogen and oxygen, viruses and bacteria, or even electrons and protons, none of which is directly observable - do not have to abide by positivist proscriptions. So when it comes to laws they can take the reality of immanent patterns seriously, even if it means confronting the embarrassment of riches offered by the multiplicity of versions of one and the same law. The first two versions of the law of gravity offer no problem if we think that many physical entities behave both like discrete particles (the kinds of entities to which forces can be applied) as well as continuous fields. In other words, the divergence in our models tracks an objective divergence in reality. But what to make of the third version, what would singularities be? The simple answer is that singularities form the structure of a space of possibilities, and it is this structure that constitutes a real virtuality. To see what this definition implies we need to explore, however briefly, the history of this version of classical mechanics, the so-called 'variational version. 

In one of its forms the variational version is, indeed, well known. In 1662, Pierre de Fermat proposed that light propagates between two points so as to minimise travel time. His basic insight can be explained this way: if we knew the start and end points of a light ray, and if we could form the set of all possible paths joining these two points (straight paths, crooked paths, wavy paths), we could find out which of these possibilities is the one that light actualises by selecting the one that takes the least amount of time. In the centuries that followed, other 'least principles were added to Fermat's (least action, least effort, least resistance, least potential energy). But the real breakthrough was the development in the 18th century of a way to extend this insight into the world of differential functions, the basic technology underlying the virtual realities (the formal models) created by classical physics. This was the calculus of variations created by the mathematician Leonard Euler. Before Euler, the main problem was to find a way to specify the set of possible paths so that it was maximally inclusive, that is, so that it contained all possibilities. This was done by 'parameterising' the paths, that is, by generating the paths through the variation of a single parameter.2 But there are many physical problems in which the possibilities cannot be parameterised by a discrete set of variables. Euler's method solves this problem by tapping into the resources of the differential calculus. Without going into technical details, these resources allow one to rigorously specify the space of possibilities and to locate the minimum, maximum and inflection points (that is, all the singularities) of the functions that join the start and end points.3 

By the mid-19th century, all the different processes studied by classical physics (optical, gravitational, mechanical) had been given a variational form and were unified under a single least principle: the tendency to minimise the difference between kinetic and potential energy. In other words, it was discovered that a simple singularity (a minimum) structured all the possibility spaces of classical processes. But this does not answer the question of what singularities are. More technically, what is the ontological status of these singular points? When a classical process is actually taking place one can discern specific causal mechanisms producing specific effects, and these mechanisms vary from one type of process to another: the mechanisms through which a ray of light finds the shortest path have nothing in common with those through which flooding water seeks the lowest point of altitude. But the singularity itself is mechanism-independent. If the ontological status of causes may be said to be actual, that is, if causes must actually exist in order to produce their effects, the status of the singularity is not that of an actual entity but of a virtual one. It is nevertheless real, every 

bit as real as are the causes and the effects: singularities define a real virtuality. The next milestone in this history came in the last decades of the 19th century with the work of another great mathematician, Henri Poincaré. As appropriate to the study of possibility spaces, the breakthrough was linked to new resources that had become available for the geometrical exploration of spatial questions. A hundred years earlier performing this exploration meant studying metric spaces, that is, spaces in which the notions of length, area and volume are fundamental, like those of Euclidean geometry. A mathematical space is basically a set of points, so basic concerns for mathematicians are the means available to correctly identify a given point in a space. The metric solution to this is to give each point an "address", that is, to locate it relative to a set of fixed coordinates or axes. One determines the distance that each point has from these axes and the set of such rigid distances becomes its address. But in the 19th century, mathematicians invented other ways of achieving this identification. One could, for example, determine the rate at which the curvature of a space changes at every point, and use this instantaneous rate of change to identify a point. Such a space ceases to be a set of X, Y, Z addresses and becomes a field of rapidities and slownesses, the rapidity or slowness with which curvature varies at each point. Differential geometry uses this approach, as does the even less metric geometry called 'topology'. Poincaré's genius was to bring topological resources to bear on the study of the possibility spaces defined by differential functions. 

Tapping into these resources implies that the mathematical model of a physical process can be given a spatial form. How is this achieved? To create the original model one needs to enumerate all the relevant ways in which the process in question is free to change. For example, a given physical process may be characterised by two changing properties, such as temperature and pressure. These are called its 'degrees of freedom'. Of course, the process may also change in an infinite number of irrelevant ways, the art of modelling being based in large part on the ability to tell the significant from the insignificant. Once a process's degrees of freedom are discovered, the model can be given a spatial form by assigning each. of them to a dimension of a topological space. In the physical process being modelled, any combination of values for temperature and pressure fully specifies the state in which the process finds itself at any moment of time. When these degrees of freedom become the dimensions of a space, each combination of values becomes point in that space. That is, each point represents an instantaneous state for the process, the set of points as a whole becoming a space of possible states. This possibility space is called 'state space' or 'phase space". As a physical process changes in time following a sequence of states, these temporal sequences appear in state space as a series of points, that is, as a curve or a trajectory. 

If this was all that topologists could offer, their contribution would amount to having discovered a way to visualise the behaviour of differential functions, or more exactly, of the solutions to those functions. This would be a valuable tool but not a fundamental advance full of ontological consequences. But their real achievement was to move to the mechanism-independent level. In other words, topologists did not study the singularities structuring the possibility space of a process free to change in its temperature and its pressure, but of all processes with two degrees of freedom, whatever these ways of changing were. And similarly for processes with any number of degrees of freedom, that is, for spaces with any number of dimensions. Poincaré, for example, was able to show that, in the two-dimensional case, there was a well defined repertoire of singularities, a repertoire shared by any process that can change in only two ways, whatever the causal mechanisms involved in those changes. He found four different types of point singularity distinguished from each other by the form of the flow of nearby trajectories: nodes, saddle points, foci and centres. He also discovered a line singularity closed on itself forming a loop (a 'limit cycle')." These exhausted the ways of structuring two-dimensional possibility spaces. 

Later on one of his followers showed that, in the three-dimensional case, the four point singularities were still there but now line singularities came in three different forms: stable, unstable and saddle-shaped loops. In fact, the state space for processes with three degrees of freedom also contains another line singularity, one that can be pictured as a loop that has been repeatedly stretched and folded. Poincaré got a glimpse of these strange singularities but they did not get a name until much later: deterministic chaos. Finally, other topologists discovered that not only singularities come in well-defined repertoires but also that sudden (or catastrophic) switches from one to another singularity have an equally limited way of happening. In catastrophe theory, for example, any process driven by a potential that has four degrees of freedom can have only seven different ways of switching, all of which have been classified. These seven catastrophes, as they are called, do not depend on the nature of the degrees of freedom, only their number, or on the nature of the potential (gravitational, mechanical, chemical), only on its existence.? 

From a philosophical point of view it is this conceptual movement towards the mechanism- independent level, or rather the direction of the movement, that is highly significant, because it points towards a real virtuality. Unlike the trajectories which always point in the direction of material reality, since they are possible sequences of states for the process being modelled, the number of dimensions of the space, and the repertoire of singularities available to it, do not refer to any aspect of the process. They are topological invariants, properties of the space that remain unchanged regardless of how it is folded, stretched or otherwise transformed. That these invariants do not point in the direction of anything actual is shown by the fact that those singularities on which trajectories converge are done so asymptotically, that is, the trajectories get for ever close to them without ever reaching them. In other words, the states the singularities represent are never actualised, remaining for ever virtual. 

At this point it may be objected that this argument applies only to mathematical models but has little relevance to material reality. What would correspond, for example, to singularities in the real world? To answer this question we must distinguish between the properties that define a real entity- an atom, a molecule, a living cell - from the entity's tendencies and capacities. The distinction between properties and capacities can be illustrated with a simple example: a kitchen knife. The knife is characterised by certain properties such as its weight, its length and whether it is sharp or blunt. Sharpness, as a property, is always actual: at any given instant, the knife either possesses this property or it does not. But the knife also has capacities, such as its capacity to cut things, that may never be actual if they do not happen to be exercised, that is, if the knife is never used. Also, when the capacity does become actual it is not as a state, like the state of being sharp, but as an event: to cut/to be cut. The event itself is always 'double' because the knife's capacity to affect is contingent on the existence of things that have the capacity to be affected by it. Thus, while properties can be specified without reference to anything else, capacities to affect must always be thought in relation to capacities to be affected. 

Something that is real but not necessarily actual, like an unexercised capacity, is an example of real virtuality in the material world. Another example is: tendencies. A knife has the property of solidity within a wide range of temperatures, but in environments that exceed that range the knife may be forced to manifest its tendency to liquefy. Like capacities, tendencies are real even when they are not actually manifested, and when they do become actual it is as events: to melt or to solidify. The main difference between tendencies and capacities is that the former are typically finite but the latter need not be. We can enumerate, for example, the possible states in which a material entity will tend to be (solid, liquid, gas, plasma), but capacities need not be finite because they depend on the capacities to be affected of innumerable other entities: a knife has the capacity to cut when it interacts with something that has the capacity to be cut; but it also has the capacity to kill in interaction with large organisms that have the capacity to be killed. 

To return to the main argument, the singularities structuring mathematical possibility spaces allow us to study tendencies, such as the tendency of a process to minimise some property, or its tendency to be attracted to an oscillatory state. But capacities cannot be modelled the same way because they are only exercised in interactions, interactions between an entity that can affect and another that can be affected. On the other hand, other forms of virtual reality can be used for this purpose, since in computer simulations we can stage interactions between simulated entities, vary the parameters under which the interactions take place, and use the results as a guide to map the structure of the space of possibilities. An interesting illustration is provided by cellular automata, simulations in which a population of finite state automata (machines that can carry computations without using memory) interacts and generates collective patterns of activity. 

In the well-known Game of Life, a cellular automaton in which the automata can be in only one of two states, the interactions between neighbours generate collective patterns of states that can be steady-state (blocks), periodic (blinkers) or mobile (gliders, shuttles). Using these emergent patterns as elementary building blocks, we can construct more complex patterns (glider guns) and using these, in turn, we can build into the space of the Game of Life an entire computer (Turing Machine). The interactions between the finite- state automata are defined by rules, so to explain the capacity of the automaton population to give rise to such a complex set of emergent patterns, we need to study the space of all possible rules. This space is divided into four areas (defining four different classes of rules) with different capacities: the first class of rules leads to fixed, homogenous patterns; the second class gives rise to periodic patterns; the third class leads to relatively random configurations; and finally, the fourth class (or 'class IV) produces true complexity, including localised structures capable of motion, like the gliders and other mobile patterns of Life. The results of this analysis give us a sense of what a singularity would be in the possibility spaces associated with capacities: class IV rules are singular or special not only because they lead to a maximum capacity for pattern generation but also because they are relatively rare, constituting a small area of the possibility space sandwiched between class II and class III.10 Other simulations give us the means to explore other aspects of real virtuality: genetic algorithms, for example, are very good at searching a space of possible solutions to a problem, allowing us to conceive of biological evolution as a search in the space of possible animals or plants; neural nets can map similarities in the patterns used to train them into relations of proximity in the space of possible strength patterns of their interconnections, allowing us to picture the mind as a kind of infolding of real virtuality, and multiagent systems let us stage interactions between agents of different degrees of complexity to discover combinations of mutually stabilising behavioural strategies that give us a glimpse into social possibility spaces." As memory and computing power decrease in price and our desktop computers acquire the computational capacity that supercomputers had a decade before, they will become not only machines in which virtual realities are deployed as tools, such as 3D modelling and rendering tools, but also a privileged place for the scientific and philosophical exploration of real virtuality. 

NOTES 
1 Richard Feynman, The Character of Physical Law, MIT Press (Cambridge, MA), 1997, pp 50-3. 

2 Don S Lemons, Perfect Form: Variational Principles, Methods, and Applications in Elementary Physics, Princeton University Press (Princeton, NJ), 1997, p 7. 

3 Ibid, pp 17-27. 

4 A more detailed presentation of the relation between metric and nonmetric spaces in the spirit 

of Felix Klein's Erlangen Program is done in: Manuel DeLanda, Intensive Science and Virtual Philosophy, Continuum Press (London), 2002, pp 23-6. 

5 June Barrow-Green, Poincaré and the Three Body Problem, American Mathematical Society/ 

London Mathematical Society, 1997, pp 32-3. 

6 Ian Stewart, Does God Play Dice: The Mathematics of Chaos, Basil Blackwell (Oxford), 1989, p 107. 

7 Alexander Woodcock and Monte Davis, Catastrophe Theory, EP Dutton (New York), 1978, p 42. 

8 The implementation of a Turing Machine in the Game of Life was performed in the year 2000 by Paul Rendell, and can be seen in operation at his website. 

9 Stephen Wolfram, 'Universality and Complexity in Cellular Automata', in Cellular Automata and Complexity, Addison-Wesley (Reading, MA), 1994, pp 140-55. 

10 Christopher G Langton, 'Life at the Edge of Chaos', in Artificial Life II, edited by Christopher G Langton, Charles Taylor, J Doyne Farmer, Steen Rasmussen, Addison-Wesley (Redwood City, CA), 1992, p 44. 

11 These simulations are discussed in detail in: Manuel DeLanda, Philosophy and Simulation: The 

Emergence of Synthetic Reason, Continuum (London), 2011. 

©2011 John Wiley & Sons Ltd. 


A NATURAL MODEL FOR ARCHITECTURE 
John Frazer 

*** 

In John Frazer's seminal book An Evolutionary Architecture (1995), from which this essay is extracted, a fundamental approach is established for how natural systems can unfold mechanisms for negotiating the complex design space inherent in architectural systems. In this essay, which forms a critical part of the book, Frazer draws both correlations and distinctions from natural processes as emulated in design processes and form as active manifestations within natural systems. Form is seen as an evolving agent generated via the rules of descriptive genetic coding, functioning as a part of a metabolic environment. Frazer's process-model establishes the realm in which computation must manoeuvre to produce a valid solution space, including the operations of self-organisation, complexity and emergent behaviour. Addressing design as an authored practice, he extends the transference of 'creativity' from the explicit impression into form, to the investment of thought, organisation and strategy in the computational processes which produce form. Frazer's text concentrates astutely on the practising of the evolutionary paradigm, the output of which postulates an architecture born of the relationships to dynamic environmental and socio- economic contexts, and realised through morphogenetic materialisation. 

THE NATURE OF THE ANALOGY 
Architecture has frequently drawn inspiration from nature - from its forms and structures, and, most recently, from the inner logic of its morphological processes. It is therefore necessary to be clear as to where architecture is literally considered as part of nature, where there are analogies or metaphors, and where nature is a source of inspiration.' 

We can say that architecture is literally part of nature in the sense that the man- made environment is now a major part of the global ecosystem, and man and nature share the same resources for building. In turn, our description of an architectural concept in coded form is analogous to the genetic codescript of nature. Analogies, particularly biological, bedevil architectural writing. Sullivan, Wright and Le Corbusier all employed biological analogies, and the concept of the organic is central to the 20th century. In our case, the primary inspiration comes from the fundamental formative processes and information systems of nature.2 

THE PROBLEM OF THE BLUEPRINT 
Architecture's use of biological analogies has a counterpart in the use of architectural analogies in science. There is an abundance of books with titles like The Cosmic Blueprint (by Paul Davies) or Blueprint for a Cell (by the Nobel Laureate Christian de Duve). These, however, are quick to point out that an architect's blueprint is a specific one-off set of plans whereas the 'blueprint' in nature is a set of instructions which are dependent on a particular environmental context for their interpretation. Our present search to go beyond the 'blueprint' in architecture and to formulate a coded set of responsive instructions (what we call a 'genetic language of architecture') may yield a more appropriate metaphor. It may provide at least a model of how such a form- generating process might work, even if it is not a direct parallel of the way in which nature itself generates form. 

Examples of the architectural analogy turn up in every field. For botanists, too, the concept of the architectural model has provided a powerful tool for studying plant form and structure: 'The architectural model is an inherent growth strategy which defines both the manner in which the plant elaborates its form and the resulting architecture."3 It is ironic that architectural theory actually lacks the methodological incisiveness which these reverse analogies imply. 

INTENTIONALITY 
Although evolution operates without preknowledge of what is to come - that is, without design the seemingly purposeful construction of living things makes it tempting to apply the idea of "design' to natural forms. On this basis, theologian William Paley argued that since the existence of something as complex as a watch implied the existence of a watchmaker, the infinitely greater complexity of nature had necessarily to imply the existence of a creator. Richard Dawkins.countered with the 'Blind Watchmaker argument, contending that the blind forces of Darwinian natural selection alone were sufficient explanation for the complexity of natural forms. But Dawkins in turn has resorted to anthropomorphism, attributing 'selfish' tendencies to genes.

The tendency to invest nature with vitalist forces is common in both science and poetry. In the framework of our analogy, we will sometimes apply the word 'design' to nature for convenience of expression: we will also apply it to our new model with purposeful intent. To us the connotations of the term 'design' are very different from the norm: when we 'design', we are clear in our intentions, but perhaps 'blind' to the eventual outcome of the process that we are creating. This 'blindness' can cause concern to those with traditional design values who relish total control. It can alarm those who feel that what we are proposing might get out of control like a computer virus. However we remain convinced that the harnessing of some of the qualities of the natural design process could bring about a real improvement in the built environment. 

SOURCES OF INSPIRATION 
A clear distinction is intended between sources of inspiration and sources of explanation. When natural science is used for explanation or illustration, it is essential that the science is correct and that the analogy is valid. But when it is used for inspiration and as a take-off point for thought experiments, it matters less, and misunderstood or even heretical ideas can provide much imaginative stimulus. 

It is important to differentiate between the nature of different kinds of theories. Lionel March has written: 'Logic has interests in abstract forms. Science investigates extant forms. Design initiates novel forms. A scientific hypothesis is not the same thing as a design hypothesis. A logical proposition is not to be mistaken for a design proposal." In this context, ours is not a theory of explanation, but a theory of generation. 

THE INSPIRATION OF NATURE 
The perfection and variety of natural forms is the result of the relentless experimentation of evolution. By means of profligate prototyping and the ruthless rejection of flawed experiments, nature has evolved a rich biodiversity of interdependent species of plants and animals that are in metabolic balance with their environment. While vernacular architecture might occasionally share some of these characteristics, the vast majority of buildings in our contemporary environment most certainly do not. 

Our analogy of evolutionary architecture should not be taken just to imply a form of development through natural selection. Other aspects of evolution, such as the tendency to self-organisation, are equally or even more significant. Natural processes such as metabolism and the operation of the laws of thermodynamics are central to our enquiry, as are the general principles of morphology, morphogenetics and symmetry-breaking. 

Charles Darwin established a new world which broke away from the Newtonian paradigm of stability – a world in a continuous process of evolution and change. Modern physics now describes a world of instability. Ilya Prigogine has discovered new properties of matter in conditions that are far from equilibrium, revealing the prevalence of instability which is expressed by the phenomenon of small changes in initial conditions leading to large amplifications of the effects of the changes." 

NATURAL AND ARTIFICIAL MODELS 
The modelling of these complex natural processes requires computers, and it is no coincidence that the development of computing has been significantly shaped by the building of computer models for simulating natural processes. Alan Turing, who played a key role in the development of the concept of the computer (the Turing Machine), was interested in morphology and the simulation of morphological processes by computer-based mathematical models. The Church- Turing hypothesis stated that the Turing Machine could duplicate not only the functions of mathematical machines but also the functions of nature. Von Neumann, the other key figure in the development of computing, set out explicitly to create a theory which would encompass both natural and artificial biologies, starting from the premise that the basis of life was information. 

A significant example of this dual approach in terms of our genetic model is John Holland's Adaptation in Natural and Artificial Systems. Holland starts by looking for commonality between different problems of optimisation involving complexity and uncertainty. His examples of natural and artificial systems range from 'How does evolution produce increasingly fit organisms in highly unstable environments?' to 'What kind of economic plan can upgrade an economy's performance in spite of the fact that relevant economic data and utility measures must be obtained as the economy develops? 

Although Holland suggests that such problems have no collective name, they seem to share a common concern with questions of adaptation. They occur at critical points in fields as diverse as evolution, ecology, psychology, economic planning, control, artificial intelligence, computational mathematics, sampling and inference. To this list we must now add architecture. 

ARTIFICIAL LIFE 
In nature it is only the genetically coded information of form which evolves, but selection is based on the expression of this coded information in the outward form of an organism. The codes are manufacturing instructions, but their precise expression is environmentally dependent. Our architectural model, considered as a form of artificial life, also contains coded manufacturing instructions which are environmentally dependent, but as in the real world model it is only the codescript which evolves. 

GENERATIVE SYSTEMS 
An essential part of this evolutionary model is some form of generative technique. Again this is an area charged with problems and controversy. The history of generative systems is summarised by William Mitchell, who maps out a line from Aristotle to Lull through the parodies of Swift and Borges. After tracing back the use of generative systems in architectural design to Leonardo's study of centrally planned churches and Durand's Précis des Leçons d'Architecture, he outlines the concept of 'shape grammars', or elemental combinatorial systems.9 

From our point of view, there are several problems with this approach. All of these generative systems are essentially combinatorial or configurational, a problem which seems to stem from Aristotle's description of nature in terms of a kit of parts that can be combined to furnish as many varieties of animals as there are combinations of parts. Fortunately, nature is not actually constrained by the limitations implied by Aristotle. 

Mitchell regards architectural design as a special kind of problem-solving process. This approach has limitations which he recognises in principle. First, it assumes we can construct some kind of a representation which can achieve different states that can be searched through for permutations corresponding to some specified criterion (the criterion of the problem). Unfortunately for this goal-directed approach, it is notoriously difficult to describe architecture in these terms, except in the very limited sense of an architectural brief to which there are endless potential solutions. The other problem is that any serious system will generate an almost unmanageable quantity of permutations. 

PROBLEMS WITH INDUSTRIALISATION 
A problem arose in the 1960s when architecture started toying with new design processes, taking up a particular form of component-based rationalisation and methodology which embraced a generic approach, modular coordination and a perception of construction as a kit of parts. For despite rhetoric to the contrary, the architectural profession ~ and the construction industry as a whole - have failed to learn from developments in the aircraft, automotive and shipbuilding industries. Construction remains labour intensive: it has never made the transition to a capital-intensive industry with adequate research and development capabilities. It has been left largely to individual architects to take the risk of performing experimental and innovative prototyping in an uncoordinated and romantic or heroic manner. The ensuing (inevitable) failures have been catastrophic for both society and the architectural profession." The nostalgic inclinations of some to return to a previous era are an equally inadequate response, at every level, to the current predicament. The archetypes of the past do not reflect the changing demands of society, the realities of the construction industry, or the pressing need for environmentally responsible buildings. 

POST-INDUSTRIALISATION 
Architecture is not the only field to be concerned with these problems. In industrial design the all-embracing concept of mass production for a homogeneous international market has given way to a search for a new flexibility in design and manufacture. The distinguishing characteristic of this approach is that it focuses on the dynamic processes of user-experience rather than on physical form.12 Or, in John Thackara's words, design is now 'beyond the object'.13 

Industrial production used to be associated with high tooling costs and very large production runs. This is now changing because the computer has paved the way for what i have called 'the electronic craftsman'. The direct relationship between the designer at the computer console and the computer-controlled means of production potentially means not just a dramatic reduction in the production costs of the tools for mass production, and thus shorter economic runs, but a one-to-one control of production and assembly equipment. This is effectively a return to one-off craft technology, but with all the capability of the precision machine tool. 

As Charles Jencks describes it: 'The new technologies stemming from the computer have made possible a new facility for production. This emergent type is much more geared to change and individuality than the relatively stereotyped productive processes. of the First Industrial Revolution. And mass production, mass repetition, was of course one of the unshakable foundations of Modern Architecture. This has cracked apart, if not crumbled. For computer modelling, automated production, and the sophisticated techniques of market research and prediction now allow us to mass produce a variety of styles and almost personalized products. The results are closer to nineteenth-century handicraft than the regimented superblocks of 1965.14 15 

THE ENVIRONMENTAL CASE 
Natural ecosystems have complex biological structures: they recycle their materials, permit change and adaptation, and make efficient use of ambient energy. By contrast, most man-made and built environments have incomplete and simple structures: they do not recycle their materials, are not adaptable and they waste energy. An ecological approach to architecture does not necessarily imply replicating natural ecosystems, but the general principles of interaction with the environment are directly applicable. 

The construction industry is a significant consumer of raw materials and of the energy required to process, transport and assemble them. Buildings in use are even more significant consumers of energy for heating and cooling. An ecological approach would drastically reduce construction energy and materials costs and allow most buildings in use to export energy rather than consume it. 

While there is a growing awareness of the importance of environmentally and ecologically sound design among both architects and enlightened clients, there is still no comprehensive design theory and few built examples of an ecological architecture. The solution to our environmental problems may lie in relating architecture to the new holistic understanding of the structure of nature. 

RESPONSIVE ENVIRONMENTS ... 
Another important issue for our model of an evolutionary architecture is that it should be responsive to evolving in not just a virtual but a real environment. In his article, 'The Design of Intelligent Environments, Warren Brodey proposed an evolutionary, self- organising, complex, predictive, purposeful, active environment. He asked: can we teach our environments first complex, then self-organising intelligence which we can ultimately refine into being evolutionary? These issues preoccupy us too. 

…AND SOFT ARCHITECTURE 
Brodey went on to describe in enthusiastic terms some of the hypothetical implications of intelligent environments, and to introduce the concept of 'soft architecture".16 

The idea of a soft, responsive architecture also preoccupied Nicholas Negroponte, author of The Architecture Machine. He suggested that the design process, considered as evolutionary, could be presented to a machine, also considered as evolutionary, to give a mutual training resilience and growth." Negroponte placed high expectations first on computer hardware, then on software through artificial intelligence. Neither delivered any answers. 

THE ROLE OF THE COMPUTER 
Christopher Alexander dismissed the use of computers as a design aid: 'A digital computer is, essentially, the same as a huge army of clerks, equipped with rule books, pencil and paper, all stupid and entirely without initiative, but able to follow exactly millions of precisely defined operations... In asking how the computer might be applied to architectural design, we must, therefore, ask ourselves what problems we know of in design that could be solved by such an army of clerks... At the moment, there are very few such problems. 

18 

Our evolutionary approach is exactly the sort of problem that could be given to an army of clerks - the difficulty lies in handing over the rule book. Much of this text concerns the nature of these rules and the possibilities of developing them in such a way that they do not prescribe the result or the process by which they evolve. 

THE ELECTRONIC MUSE 
I see computers not as an army of tedious clerks who will thwart all creativity with their demands for precise information, but as slaves of infinite power and patience. However computers are not without their dangers. If used unimaginatively, they have a tendency to: dull critical faculties, induce a false sense of having optimised a design which may be fundamentally ill conceived, produce an atmosphere where any utterance from the computer is regarded as having divine significance, distort the design process to fit the limitations of the most easily available program, distort criticism to the end product rather than to an examination of process, and concentrate criticism and feedback on aspects of a problem which can be easily quantified. 

'Imaginative use' in our case means using the computer like the genie in the bottle to compress evolutionary space and time so that complexity and emergent architectural form are able to develop. The computers of our imagination are also a source of inspiration an electronic muse.1 

THE ROLE OF HUMAN CREATIVITY 
19 

One of the world's leading exponents of neural networks, Igor Aleksander, is also very conscious of the unique capabilities of the human brain. He reminds us that it is extraordinarily good at making guesses based on experience, at retrieving knowledge from memory without the need for exhaustive searches, at perceiving analogies and forming associations between seemingly unrelated items.20 These aspects of intuition, perception and imagination are the traditional creative engines for architectural ideas. While the model of architectural creativity proposed by this text departs in many ways from the traditional model, it still relies on human skills for the essential first step of forming the concept." The prototyping, modelling, testing, evaluation and evolution all use the formidable power of the computer, but the initial spark comes from human creativity.22 

PROBLEMS OF COMPLEXITY 
The 'sheer imponderable complexity of organisms' overwhelms us now as surely as it did Darwin in his time. The developmental processes of nature inevitably lead to complexity, but they work with simple building blocks and an economy of means to achieve complexity in a hierarchical manner. The coding of all natural forms in DNA is achieved with just four nucleotides, which in turn use just 20 triplets to specify the amino-acids that manufacture protein. The hierarchical structure of living systems is analysed by James Miller on seven levels: cell, organ, organism, group, organisation, society and supranational system. These are broken down into 19 critical subsystems which appear at all levels. This hierarchical and self-similar description applies also to organisms and social systems, which are seen as part of the same continuum.23 

Even very simple local rules can generate emergent properties and behaviour in a way apparently unpredicated by the rules. Collections of small actions ripple upwards, combining with other small actions, until a recognisable pattern of global behaviour emerges. When a certain critical mass of complexity is reached, objects can self-organise and self-reproduce in an open-ended fashion, not only creating their equals but also parenting more complicated objects than themselves. Von Neumann recognised that life depends upon reaching this critical level of complexity. Life indeed exists on the edge of chaos, and this is the point of departure for our new model of architecture. 

THE NEW MODEL OF ARCHITECTURE 
There is so far no general developed science of morphology, although the generation of form is fundamental to the creation of all natural and all designed artefacts. Science is still searching for a theory of explanation, architecture for a theory of generation and it is just possible that the latter will be advanced before the former. In other words, form-generating models developed for architectural purposes (or based on unorthodox or incorrect scientific views) may be valuable if they model a phenomenon that scientists are seeking to explain. 

In a modest way some contribution has already been made. Many papers relating to this work (particularly on computing and computer graphics) have been enthusiastically received at international conferences and cited in further publications. Many of the resulting graphics techniques are now an integral part of the computer programs used in architectural offices. If taken much further, this could contribute to the understanding of more fundamental form-generating processes, thus repaying some of the debt that architecture owes the scientific field. Perhaps before the turn of the century there will be a new branch of science concerned with creative morphology and intentionality. 

NOTES 
1 As early as 1969 Charles Jencks was predicting that biology would become the major metaphor for the 1990s and the source of the most significant architectural movement this century - the Biomorphic School. See Charles Jencks, Architecture 2000: Predictions and Methods, Studio Vista (London), 1971. 

2 A critical overview of biological analogies is given in Philip Steadman, The Evolution of Designs: Biological Analogy in Architecture and the Applied Arts, Cambridge University Press (Cambridge; New York), 1979. 

3 D Barthélémy et al, 'Architectural Concepts for Tropical Trees' in LB Holm-Nielsen et al (eds), Tropical Forests, Academic Press (London), 1989. 

4 William Paley, Natural Theology, or Evidences of the Existence and Attributes of the Deity Collected from the Appearance of Nature (Oxford), 1802. 

5 Richard Dawkins, The Blind Watchmaker, Longman (London), 1986, and The Selfish Gene, Oxford University Press (London), 1976. 

6 Lionel March (ed), The Architecture of Form, Cambridge University Press (Cambridge), 1976. 

7 Grégoire Nicolis and Illya Prigogine, Exploring Complexity, Freeman (New York), 1993. 

8 John H Holland, Adaptation in Natural and Artificial Systems, University of Michigan Press (Ann Arbor), 1975. 

9 William J Mitchell, The Logic of Architecture Design, Computation, and Cognition, MIT Press (Cambridge, MA; London), 1990. 

10 These difficulties stem largely from the views of Herbert A Simon as expressed in The Sciences of the Artificial, MIT Press (Cambridge, MA; London), 1969, an exploration of the problem of complexity with particular reference to artificial (man-made) systems. While Simon's views on the inevitability of a hierarchical strategy in nature have been influential in the formulation of our own theories, his scientific method does not recognise the need for a generating concept when approaching design, and as a consequence design has come to be misunderstood as a problem- solving activity. 

11 Martin Pawley, Theory and Design in the Second Machine Age, Blackwell (Oxford), 1990. 

12 C Thomas Mitchell, Redefining Designing: From Form to Experience, Van Nostrand Reinhold (New York), 1993. 

13 John Thackara, Design after Modernism, Thames and Hudson (London), 1988, 

14 Charles Jencks, The Language of Post-Modem Architecture, Academy Editions (London), 1987. 

15 In 1972 Alex Pike and I made a presentation to the Royal Institute of British Architects Annual Conference, Designing for Survival, which emphasised the need for a responsible approach to energy and materials. Although there was some genuine interest in resource problems it was clear that it was the survival of the profession, not of the planet, which preoccupied most delegates. It is ironic that if architects had seized the initiative at this time and formulated a comprehensive energy policy, they might have ensured a future role for the profession. Instead the chance was wasted. with a 'Long life, loose fit, low energy' campaign motivated more by politics than by a serious attempt to address the issue. See John H Frazer and Alex Pike, 'Simple Societies and Complex Technologies' in Designing for Survival - RIBA Annual Conference (Lancaster 1972) and RIBA Journal, September 1972, pp 376-7. 

16 Warren M Brodey, "The Design of Intelligent Environments: Soft Architecture', Landscape, Autumn 1967, pp 8-12. 

17 Nicholas Negroponte, The Architecture Machine, MIT Press (Cambridge, MA), 1970. 

18 Christopher Alexander, "The Question of Computers in Design', Landscape, Autumn 1967, pp 8-12. 

19 See John H Frazer, The Use of Computer-Aided Design to Extend Creativity' in H Freeman and B Allison (eds), National Conference Art and Design in Education, electronic proceedings, NSEAD (Brighton), 1993. 

20 Igor Aleksander and Helen Morton, An Introduction to Neural Computing, Chapman and Hall 

(London), 1990. 

21 This discussion is expanded in JH Frazer and JM Frazer, 'Design Thinking: Creativity in Three Dimensions' in Creative Thinking: A Multifaceted Approach, conference proceedings, Malta University Press, 1994, and John H Frazer, "The Architectural Relevance of Cybernetics', Systerns Research, vol 10, no 3, pp 43-4. 

22 Stuart A Kauffman, The Origins of Order: Self-Organization and Selection in Evolution, Oxford University Press (New York), 1993. 

23 James Grier Miller, Living Systems, McGraw-Hill (New York), 1978. 
From John Frazer, ‘A Natural Model for Architecture', An Evolutionary Architecture, Architectural Association Publications (London), 1995, pp 9-21. Reproduced by permission of John Frazer and the Architectural Association Publications. Architectural Association Publications. 
MORPHOGENESIS AND THE MATHEMATICS OF EMERGENCE 
Michael Weinstock 

Michael Weinstock has contributed significantly to establishing the theoretical and practical foundations for employing design computation in generating architectural systems based on the logics and mechanisms of natural models. In this important text, from the Emergence: Morphogenetic Design Strategies issue (2004) of Architectural Design, he culls concepts and techniques from the fields of mathematics and biology to present an approach where emergence, within a computational environment, is a valuable and necessary component for the generation of robust architectural systems. Weinstock lays out the principles of organisation, differentiation and interaction of natural systems, along with their correlating mechanisms in mathematics, cybernetics and systems theory to propose a computationally based paradigm for design. Across these domains, the characterisation of pattern becomes essential. As Weinstock explains, pattern is always an outcome, born of local relationships, and not realised through global imposition or projection. Pattern is an image of growth, not of finality. The repercussion of this is that if architecture is modelled on natural systems then it must be modelled as itself a dynamic system, and within a larger evolving 'ecology'. Weinstock succinctly establishes the foundation for this approach and a metabolistic envisioning of architectural and cultural systems. 

Emergence is a concept that appears in the literature of many disciplines, and is strongly correlated to evolutionary biology, artificial intelligence, complexity theory, cybernetics and general systems theory. It is a word that is increasingly common in architectural discourse, where too often it is used to conjure complexity but without the attendant concepts and mathematical instruments of science. In the simplest commonly used definition, emergence is said to be the properties of a system that cannot be deduced from its components, something more than the sum of its parts. This description is perhaps true in a very general sense, but rather too vague to be useful for the purpose of design research in architecture. One can truthfully say, for example, that every higher- level physical property can be described as a consequence of lower-level properties. In the sciences, the term refers to the production of forms and behaviour by natural systems that have an irreducible complexity, and also to the mathematical approach necessary to model such processes in computational environments. 

The task for architecture is to delineate a working concept of emergence and to outline the mathematics and processes that can make it useful to us as designers. This means we must search for the principles and dynamics of organisation and interaction, for the mathematical laws that natural systems obey and that can be utilised by artificially constructed systems. We should start by asking: what is it that emerges, what does it emerge from, and how is emergence produced? 

Mathematics has always played a critical role in architecture, but the character and function of mathematics in relation to the theories and the material objects of architecture have varied so that a definitive account remains elusive. It is evident that there is a pressing need for a more developed mathematical approach in current architecture. First, the liberation of tectonics from the economic straitjacket of orthogonal geometry demands more precision in the interface between architectural definitions of form and the computer-driven fabrication processes of manufacturing constructors. Second, the engineering design for the complex geometries of contemporary tectonics must begin from a definitive mathematical base. And third, there is a lacuna in the theoretical body of architecture, an absence that is marked by the proliferation of design processes that borrow the appearance of scientific methods yet lack their clarity of purpose, mathematical instruments and theoretical integrity. 

There is an intricacy in the interchange of ideas and techniques between the disciplines of biology, physical chemistry and mathematics. They are separate, almost discrete but overlapping, and the boundaries between them are indeterminate. The originating concepts and the subsequent development of emergence are founded in these interchanges. 

PROCESS AND FORM 
Living organisms can be regarded as systems, and these systems acquire their complex forms and patterns of behaviour through the interactions, in space and over time, of their components. The dynamics of the development of biological forms, the accounts of growth and form, of morphogenesis, have become much more central to evolutionary theory than in Darwin's thesis. Darwin argued that just as humans breed living organisms by unnatural selection, organising systematic changes in them, so wild organisms themselves are changed by natural selection. Natural selection is a process involving predators and parasites, and environmental pressures such as food supply, temperature and water. Successful organisms will survive the fierce competition and have greater breeding success and, in turn, their offspring will have greater reproductive success, and so on. Darwin's arguments had a remarkable alignment with the then current theory of competitive struggle in capitalism2 and the concepts of mechanisms in industry. Theories of morphogenesis, the creation of forms that evolve in space and over time, are now inextricably entwined with the mathematics of information theory, with physics and chemistry, and with organisation and geometry. The pattern of alignment with concepts and technologies of economics and industry remains consistent. 

The convergent lines of thought between biology and mathematics were initiated early in the 20th century, particularly in the work of Whitehead and D'Arcy Thompson. D'Arcy Thompson, zoologist and mathematician, regarded the material forms of living things as a diagram of the forces that have acted on them.3 His observations of the homologies between skulls, pelvises and the body plans of different species suggested a new mode of analysis, a mathematisation of biology. Morphological measurements are specific to species and at times to individuals within a species, and so are various, but there are underlying relations that do not vary - the 'homologies'. 

Homology has two distinct but related meanings: to biologists it means organs or body parts that have the same evolutionary origin but quite different functions; and to mathematicians it is a classification of geometric figures according to their properties. Form can be described by mathematical data, mapping points in 3D coordinate space, dimensions, angles and curvature radii. D'Arcy Thompson's comparison of related forms within a genus proceeds by recognising in one form a deformation of another. Forms are related if one can be deformed into another by Cartesian transformation of coordinates. Comparative analysis reveals what is missing in any singular description of a form, no matter how precise, and that is the morphogenetic tendency between forms. 

At around the same time the mathematician and philosopher Whitehead* argued that process rather than substance was the fundamental constituent of the world, and that nature consists of patterns of activity interacting with each other. Organisms are bundles of relationships that maintain themselves by adjusting their own behaviour in anticipation of changes to the patterns of activity all around them. Anticipation and response make up the dynamic of life. 

form and behaviour 

The union of these two groups of ideas is very interesting emerge from process. It is process that produces, elaborates and maintains the form or structure of biological organisms (and nonbiological things), and that process consists of a complex series of exchanges between the organism and its environment. Furthermore, the organism has a capacity for maintaining its continuity and integrity by changing aspects of its behaviour. Forms are related by morphogenetic tendencies, and there is also the suggestion that some, if not all, of these characteristics are amenable to being modelled mathematically. The ideas are particularly relevant to us, as in recent years both architecture and engineering have been preoccupied with processes for generating designs of forms in physical and computational environments. 

Pattern, Behaviour and Self-Organisation 

Form and behaviour have an intricate relationship. The form of an organism affects its behaviour in the environment, and a particular behaviour will produce different results in different environments, or if performed by different forms in the same environment. Behaviour is nonlinear and context specific. Mathematical descriptions of behaviour are found in the elaboration of Whitehead's 'anticipation and response' by Norbert Wiener, who developed the first systematic description of responsive behaviour in machines and animals. Wiener argued that the only significant difference between controlling anti- aircraft fire and biological systems was the degree of complexity. He had developed new programs for ballistics guidance, in which information about the speed and trajectory of a target is input to a control system so that anti-aircraft guns could be aimed at the point where a target would be. The control system could record and analyse the data from a series of such experiences and subsequently modify its movements. 

Cybernetics organises the mathematics of responsive behaviour into a general theory of how machines, organisms and phenomena maintain themselves over time. It uses digital and numerical processes in which pieces of information interact and the transmission of information is optimised. Feedback is understood as a kind of 'steering' device that regulates behaviour, using information from the environment to measure the actual performance against a desired or optimal performance. 

Work in thermodynamics by Prigogine? extended this (and the second law of thermodynamics) by setting up a rigorous and well-grounded study of pattern formation and self-organisation that is still of use in the experimental study and theoretical analysis of biological and nonbiological systems. He argued that all biological organisms and many natural nonliving systems are maintained by the flow of energy through the system. The pattern of energy flow is subject to many small variations, which are adjusted by 'feedback' from the environment to maintain equilibrium, but occasionally there is such an amplification that the system must reorganise or collapse. A new order emerges from the chaos of the system at the point of collapse. The reorganisation creates a more complex structure, with a higher flow of energy through it, and is in turn more susceptible to fluctuations and subsequent collapse or reorganisation. The tendency of 'self-organised' systems to ever-increasing complexity, and of each reorganisation to be produced at the moment of the collapse in the equilibrium of systems extends beyond the energy relations of an organism and its environment. Evolutionary development in general emerges from dynamic systems. 

Geometry and Morphogenesis 

Theoreticians fiercely contest the precise relationship of morphogenesis to genetic coding, but there is an argument that it is not the form of the organism that is genetically encoded but rather the process of self-generation of the form within an environment. Geometry has a subtle role in morphogenesis. It is necessary to think of the geometry of a biological or computational form not only as the description of the fully developed form, but also as the set of boundary constraints that act as a local organising principle for self- organisation during morphogenesis. Pattern and feedback are as significant in the models of morphogenesis as they are in the models of cybernetics and dynamic systems. Alan Turing put forward a hypothesis of geometrical phyllotaxis, the development of form in plants, which offered a general theory of the morphogenesis of cylindrical lattices. These are formed locally rather than globally, node by node, and are further modified by growth. To mathematically model this process, it is necessary to have a global informing geometry, the cylinder, and a set of local rules for lattice nodes. 

Turing had a life-long interest in the morphogenesis of daisies and fir cones, of polygonally symmetrical structures such as starfish, of the frequency of the appearance of the Fibonacci series in the arrangements of leaves on the stem of a plant, and of the formation of patterns such as spots or stripes. His simple early model of morphogenesis1o demonstrates breakdown of symmetry and homogeneity, or the emergence of a pattern, in an originally homogeneous mixture of two substances. Equations describe 

160 Computational Design Thinking 

161 Morphogenesis and the Mathematics of Emergence 

..... 

the nonlinear changes of concentrations of two chemicals (morphogens) over time, as the chemicals react and diffuse into each other. It offers a hypothesis of the generation of pattern from a smooth sheet of cells during development in the formation of buds, skin markings and limbs. Chemicals accumulate until sufficient density is reached, then act as morphogens to generate organs. 

The reaction-diffusion model is still of interest in mathematical biology, in which a great deal of research is concentrated on the interrelated dynamics of pattern and form. Turing's model operates on a single plane, or a flat sheet of cells. Some current research" in the computational modelling of morphogenesis extends the process that Turing outlined on flat sheets to processes in curved sheets. Geometry is inherent in these models of process, incorporating morphological units' that have a dynamic relationship to each other, and to an informing global geometry. 

Spiral organisation of broccoli florets, displaying a Fibonacci mathematical series 

in which each number is the sum of the two previous numbers, evident in many natural patterns. 

George Post/Science 

Photo Library C 

Science Photo Library. 

Cummings argues that the interaction and diffusion of morphogens in cellular layers12 is also affected by the Gaussian and mean curvature of the particular membrane or layer,13 so that changes to any particular curve in a curved membrane will produce reciprocal changes to curvature elsewhere. Successful computational models have recently extended this approach by incorporating the mathematics of curvilinear coordinate meshes used in fluid dynamics to the simulation of the morphogenesis of asymmetrical organs and limbs." Folding and buckling of flat and curved sheets of cells are the basis of morphogenesis in asexual reproduction. 

The lineages of organisms that reproduce asexually exhibit convergent evolution, similar forms and properties emerging without common ancestors. There are generic patterns in natural morphogenesis, and this adds to the set of geometrical operations in these mathematical models. An intricate choreography of geometrical constraints and geometrical processes is fundamental to self-organisation in biological morphogenesis. Computational models of morphogenetic processes can be adapted for architectural research, and self-organisation of material systems is evidenced in physical form-finding processes. 

The Dynamics of Differentiation and Integration 

Feedback is not only important for the maintenance of form in an environment; it is also a useful concept in modelling the relationship of geometrical pattern and form during biological morphogenesis. In pattern-form models, feedback is organised in two loops: from form to pattern and from pattern to form. In these models the unstructured formation of biochemical pattern causes morphogenetic 'movements' and a consequent transformation in geometry. The change in geometry disrupts the pattern and a new pattern emerges, which initiates new morphogenetic movements. The process continues until the distribution of morphogens is in equilibrium with the geometry of the evolving form in the model. The feedback loops, from pattern to form and from form to pattern, construct a mathematical model of morphogenesis15 as a dynamic process from which form emerges. 

Cybernetics, system theory and complexity have a common conceptual basis, as is evidenced by the frequency of the terms 'sciences of complexity' and 'complex adaptive systems' in the extensive literature of thermodynamics, artificial intelligence, neural networks and dynamical systems. Mathematically, too, there are commonalities in the approach to computational modelling and simulations. It is axiomatic in contemporary cybernetics that systems increase in complexity, and that in natural evolution systems emerge in increasing complexity, from cells to multicellular organisms, from humans to society and culture. 

System theory argues that the concepts and principles of organisation in natural systems are independent of the domain of any one particular system, and contemporary research tends to concentrate on 'complex adaptive systems that are self-regulating. What is common to both is the study of organisation, its structure and function. Complexity theory' formalises the mathematical structure of the process of systems from 

162 Computational Design Thinking 

163 Morphogenesis and the Mathematics of Emergence 

which complexity emerges. It focuses on the effects produced by the collective behaviour of many simple units that interact with each other, such as atoms, molecules or cells. The complex is heterogeneous, with many varied parts that have multiple connections between them, and the different parts behave differently, although they are not independent. Complexity increases when the variety (distinction) and dependency (connection) of parts increases. The process of increasing variety is called differentiation, and the process of increasing the number or the strength of connections is called integration. Evolution produces differentiation and integration in many 'scales' that interact with each other, from the formation and structure of an individual organism to species and ecosystems. 

THE GENETICS OF COLLECTIVE BEHAVIOUR 
The collective behaviour of semi-autonomous individual organisms is exhibited in the social or group dynamics of many natural species. Flocks of birds and schools of fish produce what appears to be an overall coherent form or array, without any leader or central directing intelligence. Insects such as bees and termites produce complex built artefacts and highly organised functional specialisations without central planning or instructions. Structured behaviour emerges from the repetition and interaction of simple rules. Mathematical models have been derived from natural phenomena, massively parallel arrays of individual 'agents', or 'cell units' that have very simple processes in each unit, with simple interactions between them. Complex patterns and effects emerge from distributed dynamical models. Wolfram's extensive study of cellular automata" offers a comprehensive account of their characteristics and potential. The study and simulation of co-evolution and co-adaptation is particularly effective in distributed models. 

The current work at the Santa Fe Institute uses heterogeneous networks of interacting agents to pursue studies in self-organisation. The objective is to develop computational simulations of life, involving gene activity, molecular dynamics, morphogenesis and life cycles - virtual organisms with realistic morphologies that have artificial life. 

Adaptive processes in natural systems provided the initiating concepts for the first genetic algorithms, developed by Holland to design artificial systems based on natural ones.18 Genetic algorithms initiate and maintain a population of computational individuals, each of which has a genotype and a phenotype. Sexual reproduction is simulated by random selection of two individuals to provide 'parents' from which 'offspring' are produced. By using crossover (random allocation of genes from the parents' genotype) and mutation, varied offspring are generated until they fill the population. All parents are discarded; and the process is iterated for as many generations as are required to produce a population that has among it a range of suitable individuals to satisfy the fitness criteria. They are widely used today in control and optimisation applications and the modelling of ecological systems. 

Mathematical simulations of genes acting in Boolean networks with varying strengths of connection can produce differentiation of tissues and organs in models, and Kauffman19 argues that the self-organisation produced by such networks is complementary, rather than opposed, to Darwin's selection by fitness to the environment. The solution to the central problem of how to introduce the dynamics of gene processes into Cummings' model of morphogenesis is suggested by Kauffman's work on periodic attractors in genetic networks. 

The concepts and mathematical techniques to produce collective behaviour from simple local responses have the potential to radically change architectural environmental systems. It is evident that the current methods of producing 'smart' buildings with hybrid mechanical systems that are controlled by a remote central computer are inferior conceptually and prone to failure in operation. The self-organising capabilities of distributed dynamic systems have produced intelligent behaviour in natural organisms and in computational simulations, and await architectural applications. 

Models of self-organisation based on distributed variation and selection have been put forward by Heylighen.20 In this argument is the view common to many approaches, in which complex systems, such as organisms and ecologies, are evolved from the interactions of elements that combine into a variety of ‘assemblies'. Some ‘assemblies' survive to go on to form naturally selected wholes, while others collapse to undergo further evolution. The process repeats at higher levels, an emergent whole at one level becoming a component of a system emerging at a higher level. Further, natural evolution is in general not a single system but distributed or parallel, with multiple systems co-evolving with partial autonomy and some interaction. Self-organisation of the ecology as a whole is as critical as the morphogenetic self-organisation of an individual within it. 

A very recent series of models have been developed based on phenotypes rather than genotypes or genetic activity.22 Stewart argues that all self-organising systems are organised not just by themselves, but also by their contexts. The focus in this approach to modelling evolutionary self-organisation is on speciation, fitness landscapes and species selection. The models exhibit nonlinear and collective effects and represent speciation as a symmetry-breaking bifurcation. They are collections of ordinary differential equations arranged in network structures, and the network topology is reflected in the form of the equations. Recombination is preferred to mutation, as it produces substantial diversity in each generation, and selection operates on individual organisms in the context of other organisms and the environment. 

ARCHITECTURE AND EMERGENCE 
In answer to the question: what is it that emerges, what does it emerge from, and how is emergence produced? we can say the following. 

Form and behaviour emerge from the processes of complex systems. Processes produce, elaborate and maintain the form of natural systems, and those processes include dynamic exchanges with the environment. There are generic patterns in the process of self-generation of forms, and in forms themselves. Geometry has both a local and a global role in the interrelated dynamics of pattern and form in self-organised morphogenesis. 

Forms maintain their continuity and integrity by changing aspects of their behaviour and by their iteration over many generations. Forms exist in varied populations, and where communication between forms is effective, collective structured behaviour and intelligence emerges. 

The systems from which form emerges, and the systems within individual complex forms themselves, are maintained by the flow of energy and information through the system. The pattern of flow has constant variations, adjusted to maintain equilibrium by "feedback" from the environment. Natural evolution is not a single system but distributed, with multiple systems co-evolving in partial autonomy and with some interaction. An emergent whole form can be a component of a system emerging at a higher level - and what is 'system' for one process can be 'environment for another. 

Emergence is of momentous importance to architecture, demanding substantial revisions to the way in which we produce designs. We can use the mathematical models outlined above for generating designs, evolving forms and structures in morphogenetic processes within computational environments. Criteria for selection of the 'fittest' can be developed that correspond to architectural requirements of performance, including structural integrity and 'buildability'. Strategies for design are not truly evolutionary unless they include iterations of physical (phenotypic) modelling, incorporating the self- organising material effects of form finding and the industrial logic of production available in CNC and laser-cutting modelling machines. 

The logic of emergence demands that we recognise that buildings have a lifespan, sometimes of many decades, and that throughout that life they have to maintain complex energy and material systems. At the end of their lifespan they must be disassembled and the physical materials recycled. The environmental performance of buildings must also be rethought. The current hybrid mechanical systems with remote central processors limit the potential achievement of 'smart' buildings. Intelligent environmental behaviour of individual buildings and other artefacts can be much more effectively produced and maintained by the collective behaviour of distributed systems. 

We must extend this thinking beyond the response of any single individual building to its environment. Each building is part of the environment of its neighbours, and it follows that 'urban environmental intelligence can be achieved by the extension of data communication between the environmental systems of neighbouring buildings. Urban transport infrastructure must be organised to have similar environmental responsive systems, not only to control internal environments of stations and subways, but also to manage the response to the fluctuating discharge of people on to streets and into buildings. Linking the response of infrastructure systems to groups of environmentally intelligent buildings will allow higher-level behaviour to emerge. 

We are within the horizon of a systemic change, from the design and production of individual 'signature' buildings to an ecology in which evolutionary designs have sufficient intelligence to adapt and to communicate, and from which intelligent cities will emerge. 

NOTES 
I include computation as part of mathematics for the sufficient reason that all computation proceeds from low-level operations on 1 and 0. 

'In an acquisitive hereditary society he stated acquisition and inheritance as the primary means of survival': Geoffrey West, Charles Darwin: A Portrait, Yale University Press (New Haven, CT), 1938, p 334, and 'the application of economics to biology': Oswald Spengler, The Decline of the West, Knopf (New York), 1939, p 373. 

3 D'Arcy Thompson, On Growth and Form, Cambridge University Press, 1961 (first published 1917). 

4 Alfred North Whitehead, The Concept of Nature, Cambridge University Press, 1920. 

5 Norbert Wiener, Cybernetics, or Control and Communication in the Animal and the Machine, MIT Press (Cambridge, MA), 1961. 

CE Shannon and W Weaver, The Mathematical Theory of Communication, fifth edition, University of Illinois Press (Chicago), 1963. 

7 Ilya Prigogine, Introduction to Thermodynamics of Irreversible Processes, John Wiley & Sons (Chichester), 1967. 

8 Any physical system that can be described by mathematical tools or heuristic rules is regarded as a dynamic system. Dynamic system theory classifies systems by the mathematical tool rather than the visible form of a system. 

9 PT Saunders (ed), The Collected Works of AM Turing, Volume 3, Morphogenesis, includes 'The Chemical Basis of Morphogenesis (Philosophical Transactions, 1952), 'A Diffusion Reaction Theory of Morphogenesis in Plants', 'Morphogen Theory of Phyllotaxis: Geometrical and Descriptive Phyllotaxis' and 'Chemical Theory of Morphogenesis'. 

10 Alan Turing, 'The Chemical Basis of Morphogenesis', Philosophical Transactions, 1952. 

11 Christopher J Marzec, 'Mathematical Morphogenesis', Institute of Biomolecular Stereodynamics; 'A Pragmatic Approach to Modelling Morphogenesis', Journal of Biological Systems, vol 7, 2, 1999; "The Morphogenesis of High Symmetry: The Symmetrisation Theorem', Journal of Biological Systems, vol 7, 1, 1999; "The Morphogenesis of High Symmetry: The Warping Theorem', Journal of Biological Systems, vol 7, 2, 1999. 

12 LG Harrison and M Kolar, 'Coupling between Reaction-Diffusion Prepattern and Expressed Morphogenesis', Journal of Theoretical Biology, 130, 1988, and A Hunding, SA Kauffman and BC Goodwin, 'Drosophila Segmentation: Supercomputer Simulation of Prepattern Hierarchy', Journal of Theoretical Biology, 145, 1990. 

13 FW Cummings, 'A Pattern Surface Interactive Model of Morphogenesis', Journal of Theoretical Biology, 116, 1985; 'On Surface Geometry Coupled to Morphogen', Journal of Theoretical Biology. 137, 1989, 1990; 'A Model of Morphogenetic Movement', Journal of Theoretical Biology, 144, 1990. 

14 CH Leung and M Berzins, A Computational Model for Organism Growth Based on Surface Mesh Generation, University of Leeds, 2002. 

15 Alexander V Sprirov, "The Change in Initial Symmetry in the Pattern-Form Interaction Model of Sea Urchin Gastrulation', Journal of Theoretical Biology, 161, 1993. 

16 Warren Weaver, 'Science and Complexity', American Scientist, 36, 536, 1948. 

17 Stephen Wolfram, A New Kind of Science, Wolfram Media (IL), 2002, and Cellular Automata and Complexity: Collected Papers, Addison-Wesley (Reading, MA), 1994. 

18 John H Holland, Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control and Artificial Intelligence, MIT Press (Cambridge, MA), 1992 (first published 1975), and Hidden Order: How Adaptation Builds Complexity, Addison-Wesley (Reading, MA), 1996. 

19 SA Kauffman, 'Antichaos and Adaptation', Scientific American, August 1991; The Origins of Order: Self-Organization and Selection in Evolution, Oxford University Press, 1993; At Home in the Universe: The Search for Laws of Self-Organization and Complexity, Oxford University Press, 1995. 

20 Francis Heylighen, 'Self-Organization, Emergence and the Architecture of Complexity', Proceedings of 1st European Conference on System Science, 1989. 

21 HA Simon, "The Architecture of Complexity', Proceedings of the American Philosophical Society 106, reprinted in The Sciences of the Artificial, third edition, MIT Press (Cambridge, MA), 1996. 22 I Stewart, 'Self-Organization in Evolution: A Mathematical Perspective', Philosophical Transactions, The Royal Society of London, 361, 2003. 
Michael Weinstock, 'Morphogenesis and the Mathematics of Emergence', AD Emergence: Morphogenetic Design Strategies, vol 74, no 3, John Wiley & Sons Ltd (London), 2004, pp 10-17. Reproduced by permission of Michael Weinstock and John Wiley & Sons Ltd. John Wiley & Sons Ltd. 
PHILOSOPHY OF MATHEMATICS FOR COMPUTATIONAL DESIGN SPATIAL INTUITION VERSUS LOGIC 
Jane Burry 

In this text, Jane Burry explains a specific contemporary application of computation to design one in which the foci of design stems from serial definitions of dynamic spatial constructs. Burry proposes that the conception of such a design space lays critical bearing on the understanding of geometry and the mathematical means by which it is presented. While it is geometry which provides a particular depiction, it is the mathematical relationships which define the 'state space' - the range of morphological potentials. As Burry delineates, it is in the history by which the theoretical relations of space, geometry and mathematics have evolved that one can find the means where computational spatial design can be established. This also unfolds a surprisingly synchronous relationship between the mathematician's pursuit of a solution and the designer's computational deduction of a specific spatial construct. Seemingly defined by precision, it is more appropriately in the progression from abstraction and initial supposition to proven functionality where computational design can be seen as acting in the mathematician's nature. Burry utilises this intersection to succinctly interject the notion of intuition as both a fundamental and critically involved aspect of the mentality and practice of computational design. 

Over the last half century, architecture has been slowly adapting its representational practices from the conception of objects of sensory engagement to the construction of systems of formally described relationships. This shift from object description to definition of a dynamical space of design possibilities, or state space, we call 'computational design'. This is not to say that computation in its literal sense of calculation and use of algorithm was ever absent from design process. Nor that our ready access to these heaving spaces that call on 19th- and 20th-century geometry to start to comprehend them, has not been greatly expanded by access to programmable machines based on the binary logic of Turing's metaphorical machine for computation. 

COMPUTATIONAL DESIGN MODEL SPACE HIDDEN FROM OUR EYES 
Computation, in the electronically facilitated sense, has extended the geometry of architectural representation into model spaces that no longer map to perceptual space in the way that the simple static Euclidean space, or even the projective space of the model once did. The design computational model space is potentially invisible, seen only through its instances, or the manifestations of particular trajectories through the space. It is these traces that are seen, not the model itself, which must be understood through more abstract, linguistic, mathematical, diagrammatic and perhaps logical means. There may be no transcendent, objective view of the whole. In relinquishing the primacy of the object, the value of the image also fades. Whether we speak of the image as the projection of the object into two dimensions in the scientific tradition of Desargues or of Bachelard's poetic image that collects and creates resonance in imagination, the system model realised through design computation on a programmable machine confounds the ocular-centricity of the designer. The image that has been central and all-powerful in design thinking, in the field of both the outer and the inner eye, gives place to less immediate ways to know the model space in computational design. Donald Schön gives us three types of seeing for designing: literal visual apprehension, appreciative judgements of quality and apprehension of spatial gestalts.2 The first and the last, at least, are compromised as we move into model spaces that can be experienced as having as many spatial dimensions as they have variables or degrees of freedom. The second, appreciative judgements of quality are adaptable to aesthetic frameworks predicated more on 'deep' pattern recognition through more logical, intellectual and less sensorially led appreciation.3 These are neither visible nor readily visualisable spaces. With reference to Nigel Cross's warnings about failure to recognise the distinct nature of design in relation to science' we must now ponder how this computational design space is assimilated into design's own distinct 'things to know, ways of knowing them, and ways of finding out about them',5 

REPRESENTATION, PERCEPTION AND MATHEMATICS 
Cross has also written that the central concern of design is the conception and realisation of new things and at its core is the language of modelling. This language is changing, orchestrated through tentative appropriation from mathematics and the prior experiences of computer science in design computation. In being drawn into this encounter with multidimensional spaces and deeper tacit understanding of topology and the Erlangen Program, reliving in a small way the seismic shifts in geometry in the 19th century, questions are raised for computational design thinking regarding intuition, aesthetics and the impact of the altered relationship between representational and perceptual space. 

7 

These are questions inherited from mathematics and, in particular, late 19th-, early 20th-century philosophy of mathematics and the developmental psychology of mathematics and space. The changes in geometry during the 19th and early 20th century were characterised by the increasing power of analytic representations, larger, more general frames of reference for geometric space, and the rise and fall of an ambition to bring the whole of mathematics within a framework of logic. Discourse about whether (and if so, how) intuition could be dispensed with from both the foundations and the practice of mathematics was central to satellite questions about the continuing role of synthetic geometry, the value of the visual and visualisable, the cognitive nature of original ideation for mathematical problems. Although the rise of computational design modelling, and even its systems' theoretical underpinning came after the project of subsuming all of mathematics within logic had foundered, it was nevertheless born into the unresolved tussle between intuition and logic. 

GEOMETRY AND ARCHITECTURE 
Both geometry and architecture have the power to express and organise space by using representations outside the constraints of a direct mapping to the physical. The principal distinction between them lies in their levels of abstraction and generality. Geometry, as a pursuit, looks for the greatest generalities and, once established (demonstrated or proved), offers them up as truths (and for use). Architecture very selectively employs these general relationships constructively to underpin and create specific spatial relationships. It is concretely particular. During the 19th century, mathematicians arrived to a potentially infinite hierarchy of different geometries, spaces with different geometrical truths. Manuel DeLanda, in his essay, 10 describes in more detail the progression from Carl Friedrich Gauss's application of differential geometry to consistent intrinsic description of surface to the extension of such intrinsic description from the two-dimensional space of surface to n-dimensional space by Bernhard Riemann. Gauss is also credited with developing, although not publishing, parts of the mathematics of group theory." In 1872, Felix Klein used group theory to tackle the problem of categorising and characterising the multiple different geometries in his influential manifesto known as the Erlangen Program.12 A group is a set closed under a binary operation satisfying three particular axioms.13 

From the mid-1870s to the mid-1880s, Georg Cantor was also developing the basis of set theory. Up until this time the concept of 'set' had been considered a simple one pertaining to finite sets and it had been in implicit use dating back to the Greeks. Cantor moved infinity from its place as a philosophical concept into the sphere of concrete mathematical relations. In showing that some infinities are larger than others, some are bounded, the distinction between countable and uncountable sets, he established set theory as a foundational theory of modern mathematics. To appreciate the complexities of setting the criterion that determines the members of a set, we have only to dwell for a few moments on Bertrand Russell's paradox, presented in challenge to the set theory of Richard Dedekind and Gottlob Frege in 1901: the set that contains exactly the sets that are not members of themselves.14 Most of the work of the computational designer involves explicitly defining and editing the definition of sets. Each variable is a set (possibly an infinite set such as the set of all positive real numbers or a random real value between 0 and 1); a list or an array is a set. Computational designers define sets and subsets, intersections and unions of sets without necessarily being able to conceptualise the members or the full formal implications of their relationships. Rather the set definitions are the translation of linguistically framed design intentions. 

SPACE AND INTUITION 
In his Critique of Pure Reason, 1781, Immanuel Kant gave us the proposition that our minds possess, independently of experience, the forms of space and time. He called these forms intuition, suggesting that the intellect does not so much discover laws from nature as impose its laws on nature. 16 His analysis of knowledge from sensible intuition led to a distinction between what he termed the matter and the form of sense-intuition. Objects affecting our senses give us the matter of sensible intuition but these objects also appear to be ordered in definite relationships: the form of sense-intuition. This form is not grounded in sensation; it is what he calls 'pure intuition'.17 Space and time are not actual things, but are what make existence possible for spacio-temporal things. Things' presuppose space and time, nor are space and time mere relationships between things, for spatial and temporal relationships exist only where space and time are presupposed,1 

Kant overturns both Newton's theory of absolute space (the space of the objective observer that exists independently of the things in it) and Leibniz's theory of space and time based on the relationships between things. But while space and time are, in Kant's construction, the foundations which make it possible for a world of the senses to be, mathematics (based on 'pure intuition') remains the basis for its assessment. Moving smoothly from space to the propositions of geometry as though there were little distinction between the two, he wrote: 'Geometry is a science which determines the properties of space synthetically19 yet a priori20,21 

Objects and systems, from the Kantian viewpoint, are not so much constructed by use of geometry; geometry is the necessary intuitive context in which objects and systems can be conceived. 

But Kant also wrote that: 'geometrical principles are always apodictic, that is, united with the consciousness of their necessity, as, "Space has only three dimensions" 22 Were such truths empirically derived it would only be possibly to say, 'so far as hitherto observed, no space has been found which has more than three dimensions'.23 This implies Kant's adherence to the three dimensions of space as necessary truth, for, in order for other types of space to exist, geometrical knowledge would have to be a posteriori, or, in other words, empirical. 

Whether Kant would have seen the truths of hyperbolic and Riemannian geometry as no less self-apparent and inherently true than other geometrical understandings were they offered to him, and whether higher dimensional spaces would belong, for him, to the analytic rather than synthetic are questions for speculation or philosophical interpretation. Design, whether of architecture, graphics or political systems, unlike pure mathematics, must remain concretely grounded and maintain a direct mapping between its models and their imagined realisation in the world. In the barely mediated leap back and forth between the imagined 'real' (signified?) and model context, it is difficult to imagine the absence of intuition. The computational design model may perform synthetically (provide design solutions) and analytically (performance of evaluations). The underlying logic of the model comes from the analysis of the context but its implementation seems wholly synthetic and largely intuitive. However, computational design is not a quest for truth. 

PURE INTUITION AND 19TH- AND 20TH-CENTURY GEOMETRY 
At the beginning of the 20th century the renowned mathematician Henri Poincaré turned to the issues raised above with interpreting Kant's synthetic a priori intuition in its application to geometry, to which geometry and how. In Science and Hypothesis he writes that were the geometrical axioms synthetic a priori intuitions, as Kant affirmed, there would be no non-Euclidean geometry.24 He wrote that the geometrical axioms were neither synthetic a priori intuitions nor experimental facts. They were conventions. One geometry could not be more true than another, it could only be more convenient.? 

25 

However, Poincaré sat on the fence in the intuitionism-logicism debate. Subsequently, in Science and Method, he dismantles the logicists' assertion that all mathematical truths can be demonstrated using the principles of logic without making a fresh appeal to intuition 26 The logicists were those who worked to reconcile the whole of mathematics with logic, notably Dedekind and Frege, in the case of Poincaré's argument specifically Richard Courant and Giuseppe Peano. They were concerned with the primacy of deductive reasoning. 

So were the logical positivists who followed much later in the early application of computational design to architecture in the 1960s. By his own subsequent analysis, Christopher Alexander's Notes on the Synthesis of Form, 1964, while it raised the value of systematic contextual analysis, failed ultimately to proffer any real connection to formal design solutions from the analysis advocated in his design method.27 

Bertrand Russell famously thought that his own logicism conflicted with Kant's philosophy of mathematics although none of his own writing, at least up until 1912, refuted the claim that mathematics is synthetic (the apparently obvious conflict, as strict logicism would imply that it is analytic). Russell held two doctrines simultaneously known as standard and conditional logicism. Mathematical theories for which there appeared to be no alternative (ie, arithmetic) were to be reduced to logic in the standard sense; those for which there were several legitimate alternatives (eg, geometr(ies)) were to be reduced to logic only in the conditional sense.28 This last sometimes known as 'If-thenism' focuses not on whether one or other conflicting set of axioms is true but on the truth of the implications from a particular set of axioms. In other words, the mathematician's job is to study inference of certain given truths. 

So the conflict between Russell and Kant turns on a more subtle point of interpretation than whether mathematics is synthetic or analytic. One interpretation is that the role of intuition for Kant would be restricted to the mathematical context (the basic axioms of an internally consistent geometry, for example). Once chosen, theorems could result from the axioms by formal logical deduction. But if the role of intuition is not limited in this way and intuition, not formal logic, directs the whole of geometric reasoning, then even the result of a geometric construction for a proof would be determined not by the definition of the context but by spatial intuition.29 It is this aspect of the Kantian view that mathematical reasoning is not strictly formal, but always uses intuition, the a priori knowledge of space and time, that Russell refutes unequivocally, based on progress in symbolic logic.30 

A very concrete objection to a priori intuitions providing methods of reasoning that could not be substituted by formal logic, was the necessity of the figure, real or imagined to all geometrical proofs.31 It can be argued that the same question has hung over computational design modelling. To what extent can a real or imagined figurative engagement with the computational design model be sacrificed in order to embrace the greater sophistication of relationships within the representation? The figurative sampling of the model remains at some level central to its value in communicating but is it equally central to designers' own designerly ways of knowing the space in which they work? 

While Russell for a while endorsed the idealist tradition in which sensibility had a dominating role, the foundational research of Karl Weierstrass, Dedekind, Cantor and their followers led in Russell's writing to 'the collapse of the idea that analysis must rely on the imagination in order to deal with the basic space-time notions of infinity and continuity.32 

Poincaré also picked up on the distinction between intuition in the choice of axiomatic context and intuition in the process of inference. For him there are fresh appeals to intuition in the mathematician's ongoing work. 

In the case of mathematical discovery, we shall see how closely mathematical and design thought processes can be seen to align. Computational design uses 'off the shelf' geometrical relationships but can make use of, and reconfigure them, only by fresh recourse to intuition. How does such spatial intuition operate except through visualisation? 

MINDS OF MATHEMATICIANS AND COMPUTATIONAL DESIGNERS 
In Jacques Hadamard's book The Mathematician's Mind, Poincaré appears once more as a protagonist. He provides an example of a problem that had occupied him consciously for. a fortnight without success before one night drinking black coffee, 'Ideas rose in crowds; I felt them collide until pairs interlocked, so to speak, making a stable combination.' He then went away on a geological expedition for some days and in a moment of certainty in the instant he boarded a bus, 'the idea came to me, without anything in my former thoughts seeming to have paved the way for it, that the transformations I had used to define Fuchsian functions were identical with those of non-Euclidean geometry... It seems, in such cases, that one is present at his own unconscious work, made partially perceptible to the over-excited consciousness 33 This process, most associated in Hadamard's book with those of rare talent - Mozart, Gauss, Helmholtz, for example - is a form of unconscious thought in which many combinations of ideas are shaken together (the etymological root of cognito) until a chance fit occurs and the result is thrown close to the surface of 'fringe' consciousness. 

This idea of mathematical clarity arising out of crowded, murky and disorderly space is a familiar one. John Stillwell wrote that: 'Rigor and precision are necessary for communication of mathematics to the public but they are only the last stage in the mathematician's own thought. New ideas generally emerge from confusion and obscurity, so they cannot be grasped precisely until they have first been grasped vaguely and even inconsistently.' 34 

This intuitive search within a very ill-defined space-the outcomes only acquiring their logical analytical structure in the proofs, which come much later - makes mathematical discovery appear indistinguishable from design process in all but its aims. Thus although mathematics is ultimately a search for truth or at least consistency, and design a search for form (whether of space or object), the cognitive journeys are not necessarily very distinct. In computational design, the searching within obscurity may be for the form of the computational design model system. Directed mechanised computation then underpins the search for the form of the designed space itself. 

Jane Burry, Lorenz Attractor created using 'Chaoscope' (www.chaoscope.org), RMIT, 2010. 3D strange attractors rendering software offers an experience of the volatility of dynamic system spaces; spaces similar to those first identified by Poincaré in the 1880s as nonperiodic but not increasing or approaching a fixed point. These images based on a simple equation created by Edward Lorenz reveal local regions of intensity that exhibit briefly only in multiple short, parameter variable trajectories from these intensities and vast voids of near inactivity for much of the space. The sense of Deleuze's multiplicities becomes more palpable in the interaction and continuous transition between states. Few computational design modelling experiences offer the same intuitive immediacy in sensing the form of the model or state space - nor could they, given the heterogeneity and potentially vast numbers of variables defining the space - but their meaningful operational states or useful areas of intensity may be equally localised. Jane Burry. 
GEOMETRICAL, VISUAL AND TACTILE SPACE 
Strangely Poincaré segregated and defined for us the contrasting nature of first, geometrical space, 35 second, visual, and third, tactile space, giving very physical, embodied but mechanistic descriptions of the latter two, while Ernst Mach, who devoted a whole book to quite a similar exercise wrote that: 'No sharp line can be drawn between the instinctive, the technical, and the scientific acquisition of geometric notions'.36 Jean Piaget, in critique of both Kant and Poincaré, and while stating the evidence for a child's early topological rather than metrical perception of space, raises an intriguing aspect of the relationship between representational and perceptual space. During 'the development of representational space, representational activity is, in a manner of speaking, reflected or projected back on to perceptual activity." In other words the perceptual space is not the space of untouched sensory inputs but shaped progressively also by representational input. Once the Cartesian axial representation of space has been assimilated and superimposed on the space we perceive, our spatial perception is changed ever after. This 'discovery' in experimental psychology is paradoxically close to Kant's own statement of our imposition of our laws on nature. In design for realisation in physical space, the designer strives to recreate in imagination and in representation the full sensory as well as abstract ordering of the space they are crafting. There is,a relationship between the visual and more abstract geometrical understanding in oscillating between the space of constructing the representation, a potentially turbulent ocean of different states, and the imagined experience of possible realisations in physical space. But we can hardly expect it to be a continuous or consistent one. 

Logicism, while it won many battles, lost the war in mathematics. Perhaps it can be said to have had lived until finding its nadir in Gödel's incompleteness theorems in 1937. But it took other blows, in particular Brouwer's 1923 essay 'On the Significance of the Principle of the Excluded Middle in Mathematics, Especially in Function Theory', that questioned the bilateral assumption inherent in proofs based on the impossibility or absurdity of the impossibility of a property of a system, while perhaps neglecting a 'third way'. On the other hand, unconditional adherents to Kant's doctrine of a priori synthetic intuition in the aesthetic must also now be rare. 

In more recent decades, computational designers have followed their own logicist quests, in the belief that the well-structured logical model will bring consistency, overcome the vagaries of geometrical and computational surprise within the model space, or resolve the numerous interrelated conditions bearing on the design. 

CONCLUSION 
Computational design has sponsored a shift in design representation and thinking from object models to dynamic system models, from the visible to abstract mappings. Dynamic system computational design model spaces exhibit geometrical phenomena that require the geometrical developments of the 19th and 20th centuries to understand them. The 19th-century developments in geometry and mathematics - non-Euclidean geometries, the classification of geometries using group theory, new set-theoretical basis for infinities, topology as the larger geometrical context - led to philosophical debate on the subjects of intuition and logic with regard to space. The nonvisual geometrical and spatial character of computational design models raises similar questions about the status of intuition and the figurative versus deductive logical reasoning within these spaces, in particular for designerly ways of knowing. 

Are the arguments from philosophy of mathematics applicable to computational design? Mathematics and design are two different things. One is concerned with generalities and constructing proofs, the other with particulars and constructing solutions, though both are driven by aesthetics. Mathematical proofs are constructed logically applying deductive reasoning but often rest at base on axioms with an intuitive foundation. Computational design models also use logic and deductive reasoning but the particular form the design description takes among infinite possibilities is intuitively based. Design, itself, and mathematical discovery, however, are portrayed as similarly obscure and largely subconscious combinatorial processes. 

While the figure has been relinquished from contemporary mathematical spaces, seemingly far removed from possible visualisation, computational spatial design practice leans heavily on the figurative throughout even its most abstract modelling processes, always maintaining its pathways back into the physical world of appearances, at least partially through imagination. The intuitionism-logicism debate in space and mathematics sheds at least a pencil of torchlight into the foundational nature of the perceptual and geometrical challenges of bridging between logical structure and concrete mappings within computational design space. 

NOTES 
1 Gaston Bachelard, The Poetics of Space, translated by Maria Jolis, 1969 edition, Beacon Press (Boston), 1964 (trans), p xxx. 

2 Donald A Schön and Glenn Wiggins, Department of Urban Studies and Planning, Cambridge, MA, 'Kinds of Seeing and their Functions in Designing', in Design Studies, 1992, pp 135-56. 

3 In the early days of the Joint Center for Urban Studies at Harvard and MIT, and the Centre for Land Use and Built Form studies (LUBFs) at Cambridge, UK, despite a culture of scientific analytical methodology in bringing computation into design problem-solving that had no time for the consideration of 'appearances' in architectural design, Lionel March has written that his primary interest and motivation was aesthetic. (Lionel March, 'Modern Movement to Vitruvius: Themes of Education and Research', Royal Institute of British Architects Journal 81, 1972, pp 101-9) (Sean Blair Keller, 'Systems Aesthetics: Architectural Theory at the University of Cambridge, 1960-75', in Architecture, Landscape Architecture, and Urban Planning, Harvard University (Cambridge, MA), 2005, p 172.) For one reflection on the nature of aesthetics in mathematics, see Godfrey Harold Hardy, A Mathematician's Apology, Cambridge University Press (London), 1940. 

4 Nigel Cross, 'Designerly Ways of Knowing', in Design Studies, vol 3 no 4, 1982, p 221-227. 

5 Ibid. 

6 Ibid. 

7 Felix Klein's 1872 research programme and manifesto under the title 'Vergleichende Betrachtungen über neuere geometrische Forschungen' proposing a coherent comparative system to categorise all different geometries on the basis of their degree of symmetry using group theory. It was written while he was at Erlangen. 

8 It is generally considered poor mathematical practice to prove a theorem for one class when the same applies to a much broader category, for instance a property of certain types of quadratic equation when the same truth applies to all quadratic equations. 

9 Lionel March, 'Mathematics and Architecture since 1960', in Nexus IV, Kim Williams (ed), Kim Williams Books (Italy), 2002, p 9. 

10 Manuel DeLanda, "The Mathematics of the Virtual: Manifolds, Vectors Fields and Transformation Groups' in Intensive Science and Virtual Philosophy, Continuum (New York; London), 2002, pp 11-12. 

11 Group theory, sometimes also called the theory of symmetry, is attributed to the published papers of the young mathematician Évariste Galois as a student before his untimely death at the age of 20 in 1831. 

12 So named because it was delivered at Erlangen. 

13 To unpack this: closure in this case, means that the result of a particular operation on two members of the set (this two being the 'binary') gives a result that is also a member of the set. The three axioms relate to associativity (the order in which the operation is applied to the elements does not matter), the identity element (there is one element in the set which operated on with any other element results in no change to the other element) and the inverse element (for each element there is an inverse element which combined with the first results in the identity element). For instance, the group with the set whose elements are all translations in the plane, rotations about a point and reflections in a line (transformations that leave the dimensions and angles in a rigid figure unaltered) with the operation meaning 'followed by' defines Euclidean geometry; it is the Euclidean group. This group has subgroups-the first including direct isometries that preserve orientation (translations, rotations) and the second, indirect isometries that reverse orientation (reflections). The identity element is the translation (0,0). Every element has an inverse element, for instance, the inverse of the rotation 90° about a point is the rotation -90" about the same point, which is also an element of the group. DeLanda provides a fuller explanation of the relationship to geometries in hierarchy according to their symmetry in "The Mathematics of the Virtual: Manifolds, Vectors Fields and Transformation Groups', Intensive Science and Virtual Philosophy, Continuum (New York; London), 2002, pp 11-12. 

14 Also identified although not published by Emst Zermelo in 1900. 

15 'Intuition' here is the English translation of Anschauung, which in contemporary dictionary translation gives 'opinion', or even according to one's own 'experience'. Yet Leonard Nelson affirms that intuition in this context means 'not from thought'. These are ideas that we believe, opinions that we hold, views that we have, that, according to Richard Robinson, we cannot help but hold. They come not from thought but from some more fundamental source. (WH Walsh, Kant's Criticism of Metaphysics, p 11.) 

16 Morris Kline, Mathematics and the Search for Knowledge, Oxford University Press (New York; Oxford), 1985, p 16. 

17 Leonard Nelson, Julius Kraft (eds), trans Humphrey Palmer, Progress and Regress in Philosophy, vol 1, Basil Blackwell (Oxford), 1971, p 118. 

18 Ibid, pp 119-23. 

19 Synthetic statements are those that introduce new information in the predicate. In their antonym, analytic statements, the predicate can be derived from the concept alone. For instance, 'the model is morphogenetic' is synthetic as the idea of morphogenetic cannot be derived analytically from the idea model but 'the shortest distance between two points is a straight line' is only synthetic as long as a straight line is in fact distinct from the shortest distance between two points. If 'is a straight line' can be deduced from 'the shortest distance', it is analytic. That the geometrical understanding of space is synthetic, rather than analytic, is significant in separating the foundations of mathematics from the process of logical deduction. Analytic judgements can be formed from concepts alone; synthetic ones cannot. 

20 A posteriori knowledge comes from experience and is based on the perception of some object or phenomenon. A prior knowledge is independent of experience, and so is not based on perception at all. Thus synthetic a priori intuition, the nature, for Kant, of spatial knowledge and the foundations of geometry, is knowledge which cannot be logically deduced, is not derived from empirical experience, and is not from thought. 

21 Immanuel Kant, trans Norman Kemp Smith, Critique of Pure Reason, Macmillan (London; Basingstoke), 1970, first edition 1929, first published (German) 1781, p 70. 

22 Kant, trans JMD Meiklejohn, Critique of Pure Reason, Barnes and Noble Books (New York), 2004 (1781), p 5. 

23 Ibid, p 69. 

24 Henri Poincaré, Science and Hypothesis, Dover Publications (New York), 1952 (1905 first English translation), p 48. 

25 Henri Poincaré, trans Francis Maitland, Science et méthode: Science and Method, Dover Publications (New York), 1952 (1908) (1914 in English), p 50. 

26 Ibid, p 149. 

27 Christopher Alexander, Notes on the Synthesis of Form, Harvard University Press (Cambridge, MA), 1964. Alexander's preface to the 1971 paperback edition already refutes his own mathematical methods and calls for a more natural method based on designer experience. 

28 Alberto Coffa, 'Russell and Kant', in Synthese (1981), p 252. 

29 Evert Willem Beth. The Foundations of Mathematics, North Holland (Amsterdam), 1959, p 57. 

30 Bertrand Russell, The Principles of Mathematics, WW Norton (New York), 1937 (1903), p 4. 

31 Ibid, pp 456-7. 

32 Coffa, 'Russell and Kant', p 254. 

33 Jacques Hadamard, The Mathematician's Mind: the Psychology of Invention in the Mathematical Field, Princeton University Press (Princeton, NJ), 1996 (1945), p 14. 

34 John Stillwell, Numbers and Geometry, S Axler, FW Gehring, and KA Ribet (eds), Undergraduate Texts in Mathematics, Springer (New York), 1998, p 65. 

35 He defined it as continuous, infinite, of three dimensions, homogeneous and isotropic. In this exercise he constrained himself to homogeneous three-dimensional space although he is no adherent of Kant's necessity of three dimensions in space. Poincaré is the father of chaos theory. He found, in the 1880s, while studying the three-body problem, that there are orbits which are nonperiodic and yet not ever increasing or approaching a fixed point. 

36 Ernst Mach, Space and Geometry, Open Court Publishing (Chicago), 1960 (1906), p 69. 

37 Jean Piaget and Bärbel Inhelder, trans FJ Langdon and JL Lunzer, The Child's Conception of Space, 

Routledge & Kegan Paul (London; New York), 1956, p 4. 

©2011 John Wiley & Sons Ltd. 

ASSOCIATIVE DESIGN: FROM TYPE TO POPULATION 
Peter Trummer 

Computation, specified by parametric means, defines processes by their degrees of variation, or freedom, and the associations between the parameters. In this text, Peter Trummer elaborates upon this computational principle, arguing for materiality as the fundamental premise for variational and relational processes. Trummer succinctly traces the possibility for such an approach through a historical trajectory, in both science and architecture, from type-based thinking to a population-based approach. This shift restates the character of a system from a presumptive description based upon a single type to a characterisation born of a multitude of variants. This functions, as Trummer states, in the same fashion as a morphogenetic process, where the 'possibilities' inherent in the genotype are given specificity through internal reaction induced by interaction with environment. While variation is a key component in this, it is only realised as a necessitated repercussion to the dynamic nature of context (environments). Trummer presents, through this argument, a series of concepts by which information, elaborated as constraints, can be instituted within a computational process, and executed as a strategy for defining the whole of a functioning system. 

The assumptions of population thinking are diametrically opposed to those of the typologist. The populationist stresses the uniqueness of everything in the organic world. What is true for the human species, that no two individuals are alike, is equally true for all other species of animals and plants... all organisms and organic phenomena are composed of unique features and can be described collectively only in statistical terms. Individuals, or any kind of organic entities, form populations of which we can determine the arithmetic mean and the statistics of variation. Averages are merely statistical abstractions, only the individuals of which the population are composed have reality. The ultimate conclusions of the population thinker and of the typologist are precisely the opposite. For the typologist, the type (eidos) is real and the variation an illusion, while for the populationist the type (average) is an abstraction and only the variation is real. No two ways of looking at nature could be more different. 

(Ernst Mayr) 

If we were to replace the essences as the explanation of the identity of material objects and natural kinds we need to specify the way in which multiplicities relate to the physical processes which generate those material objects and kinds. 

(Manuel DeLanda) 

The Encyclopedia of Computers and Computer History defines computer-aided design as programs that are based on objects. Such objects 'can be straight geometric constructs, polygons, b-splines, cubes or they can be complex high level objects such as bathtubs, semiconductor circuits, and car engines. Today we call them geometrical or architectural primitives. The Oxford Dictionary of Computing adds to the above that there is an individual; an author needed who, equipped with technical knowledge, puts all these primitives together so that they make sense as a digital design system. 

This definition, being based on geometrical objects on the one side and the requirement for an author on the other, provoked the discipline of architecture to develop a series of theoretical frameworks that have become essential for today's architectural practice. 

From William Mitchell we have learned that these geometrical or architectural primitives can be defined as parametric shape grammars, a grammar of geometrical forms based on metrical proportions. Applied to the aesthetic rules of Palladio's villas, he demonstrated through a set of algorithms the emergence of completely new forms of variations.2 Bernhard Cache made us understand the importance of Euclidean geometry and demonstrated that the discipline of architecture has to become aware of the fact that all computational techniques define variations of geometrical invariants. With his revisit to geometry as a disciplinary knowledge, he reminded us that every geometrical primitive has to be understood, as Felix Klein formulated it in his Erlanger Program in 1872, as a member of a group defined by its invariants while being transformed. Greg Lynn introduced to us the understanding of form by means of animation, In his most explicit example, the embryological house, he came up with topological bodies that are based on geometrical primitives that become transformed through a field of forces.4 

The intention that of all these examples basically shared is not only to extend the pure technical definition of what is stated under CAD in the dictionary, but to search for the 'new' or for novelty through means of computational techniques. But this search for the new was, at the same time, an attempt to overcome the problem of the author. It therefore lays in continuation with Peter Eisenman's critique on Modernism and its anthropocentric world. 

While all of these attempts were concerned with processing new forms of variation, they were still based on the notion of essentialism. What I mean is that while using computational techniques to overcome the uniqueness of the object as a masterpiece with its particular author, they still defined an essence for all its variations. In the case of Mitchell, it is his analogy to typological thinking in language and architecture of the 18th century. In the case of Bernard Cache, it is the truth of the deductive description of Greek geometry. And in the case of Greg Lynn, who probably was one of the first to argue for the emergence of new forms by means of a morphogenetic process, still rejected the introduction of material constraints and therefore idealised the process of the architectural object itself. 

In all the above examples essentialism is formulated through a hylomorphic model; that is a model that has a clear directionality of how matter moves into form. The term 'hylemorph' indicates what is needed to design an object. It derives from hyle, meaning matter, on the one side, and morph, meaning form, on the other. So when we design a table by means of the hylomorphic model, we take a form (morph) - the image of the table we would like to design - and press it into the wood (hyle) - the material by which the image should be realised. The result is a copy, a representation of what we imagined the table should look like. 

What I will argue in this essay is that the real potential of computational techniques is to overcome any idea of typological thinking and to come up with a thesis of a design practice based on morphogenetic processes. Such morphogenetic processes would require an understanding of the architectural object as population rather than as a type. They would move away from a hylomorphic schema towards what Deleuze called a machinic phylum. To develop this argument I want to introduce the Deleuzian idea of the multiplicity presented as an alternative to an idea of essentialism. 

In this sense, I will argue in the essay that architectural thinking is affected by a much wider production of knowledge. Thus I will show how ideas of other disciplines could become originated in architecture. In this way, the essay can be called a transdisciplinary approach focusing on the question of how the concept of population thinking could become reoriginated within computational design in architecture and, in doing so, what effect this would have on our design practices. 

ASSOCIATIVE DESIGN TECHNIQUES 
To develop an architectural practice based on the notion of a machinic phylum, we have to move away from understanding architectural formations as a problem of geometry. We have to shift our attention to the question of how, as DeLanda stated above, physical processes can start to generate the form of inanimate objects. In this essay I would like to show how this could be done and thought of through means of associative design techniques. 

The contemporary computational discourse is contaminated by the term parametric design. Parametric design techniques define geometrical or architectural primitives as objects that are variable by their metrical or dimensional parameters. This suggests that an object has endless variations due to its geometric parameters. But the right description of these computational applications would be one that defines all these parametric techniques as associative design techniques. The word 'associative focuses not only on the variation of objects, but rather on their assemblage: the way these geometric primitives are linked. This linkage becomes suddenly relevant through the way an architectural assemblage can change. If architectural objects are thought of as assemblies we can start to understand them as physical systems, whereby the addition of parts defines a space of possible change. Manuel DeLanda has given a name to that. He called it the degrees of freedom an object has. The degree of freedom defines the possibilities inherent in an object's transformation. As a consequence, the architectural object is no longer to be understood as a geometrical construct but as a physical entity. 

So if we take the example of an architectural object with all its geometrical or architectural primitives, we can say that every primitive defines a particular degree of freedom depending on its material or physical constraints. All assemblies together would form a whole described by its space of possible changes. These changes are no longer defined by the identity or essence of an object, but rather would allow us to understand the object as possible forms of appearances through the interaction of the population of its parts. Thus the architectural object would be understood by means of its morphogenetic process and defined as a multiplicity rather than a type. 
Han Ju Chen, Sebastiano Manservisi, Alessandro Martinelli, Metrical Properties, Berlage Institute (Peter Trummer), Rotterdam, 2008. The matrix shows prototypical conditions of urban block formations and the specific diversities of land subdivision. This subdivision of land and its block sizes can be potentially found within the urban fabric of Mexico City. They reach from perimeter blocks, similar to the 113/113 metre block in Barcelona (on the right comer below) to block sizes of 200/40 metres (upper right corner). Each node inside the blocks represents the structural starting point of the vertical agglomerations and defines the centre point of the 'conic frames'. © Associative Design Program/Berlage Institute, Rotterdam/Peter Trummer. 
Han Ju Chen, Sebastiano Manservisi, Alessandro Martinelli, Non-Metrical Properties, Berlage Institute (Peter Trummer), Rotterdam, 2008. The matrix shows the topological organisation of vertical joins within each block configuration. The diversity of joins is based on the variation of earthquake intensity, increasing from top to bottom (which means that the conic frames increase). Each line between two nodes inside the blocks represents an angled assembly of towers. Each of these assemblies is based on a generative process of finding after each interaction neighbouring joins that give the best structural performance. © Associative Design Program/Berlage Institute, Rotterdam/ Peter Trummer. 
MULTIPLICITIES 
To understand what is meant by the idea of population thinking and its origination in architecture, it seems to be important to define the idea of multiplicity in this particular context. An often used example to define a multiplicity is the definition of a geometrical manifold. Such a manifold is, for example, a sphere, as demonstrated by the sphere of the globe. Every region of the earth can be described as a planar map, or as it is called in geometry, a Euclidean space, in which on the edge of one map, the other map begins. So the earth can be described as a manifold defined by a set - a many of maps. These maps define a metrical space on the one hand and its assemblies define a manifold, like the earth, that is non-metrical. It cannot be defined as a whole, but only as an entity that is assembled by a certain amount of measurable planes or maps. It can only be understood as a topological sphere which defines one particular orientation of an enclosed space but is able to take on various forms that are embedded within the assemblies of its parts. 

If we relate this definition of a geometrical manifold, or more generally speaking, the idea of a multiplicity, to the above description of an architectural object based on associative design techniques, then each of the architectural primitives defines the metrical constraint components of an architectural object. These components are for the architectural objects what the maps are for the skin of the earth, each being constrained to very precise measurements. The way these architectural primitives are assembled define it as a manifold. Now the way these assemblies are connected and how they are linked affect their appearance. The processes which inform their appearance define the morphogenesis of the population of architectural objects. 

Deleuze's concept of multiplicities is particularly radical as he actually applies it to all our material realities, the animate ones as well as the inanimate ones. For him all 'ideas are multiplicities: every idea is a multiplicity or a variety'. The consequence of this thought can be best visualised when it comes to defining a group of things or objects. A morphogenetic process accounts that a 'multiplicity must not designate a combination of the many and the one, but rather an organization belonging to the many as such, which has no need whatsoever of unity in order to form a system'.1o 

10 

This is in stark contrast to typological thinking, where the essence of an object defines its identity; here: 

a species is not defined by essential traits but rather by the morphogenetic process that gave rise to it. [...] While an essentialist account of species is basically static, a morphogenetic account is inherently dynamic. And while an essentialist account may rely on factors that transcend the realm of matter and energy (eternal archetypes, for instance), a morphogenetic account gets rid of all transcendental factors using exclusively form-generating resources which are immanent to the material world. (Manuel DeLanda)" 
Han Ju Chen, Sebastiano Manservisi, Alessandro Martinelli, Population of Vertical Assemblies, Berlage Institute (Peter Trummer), Rotterdam, 2008. The image shows the emerging population of vertical building agglomerations based on the relationship between their extensive properties such as the geometrical constraints of structural stability and the metrical properties of block size and their intensive property of earthquake energy based on the relationship of acceleration (g) and wave periods (s). The population of towers is based on the component of angled structures that vary in terms of their metrical properties in length and angle, and are assembled due to the topological organisation of neighbouring joins. Associative Design Program/Berlage Institute, Rotterdam/Peter Trummer. Berlage Institute, Rotterdam - Peter Trummer: 
Han Ju Chen, Sebastiano Manservisi, Alessandro Martinelli, Actualised Population, Berlage Institute (Peter Trummer), Rotterdam, 2008. Emerging population of vertical assemblies due to the interrelationship of the intensive and extensive properties of space. Associative Design Program/ Berlage Institute, Rotterdam/Peter Trummer. © Berlage Institute, Rotterdam - Peter Trummer. 
For Deleuze, any material reality is based on ideas that do not transcend ever-counting properties. In opposition to the typological thought, in which the type resembles all other members of a species, a multiplicity is based on the effects produced by the accumulation of the individual members of the species in relationship to each other. In this sense, we have to understand Deleuzian ontology, as he states in his book Difference and Repetition, that 'everything is a multiplicity in so far as it incarnates an idea ... instead of the enormous opposition between the one and the many, there is only the variety of multiplicities - or in other words, difference","2 

The important reason for explaining the difference between the idea of the one and the many is the fact that the typological variant was thus far the only accepted theory to explain the genesis of architectural forms. The most influential and constantly recurring example of this typological thought is the one defined by Quatremère de Quincy, given at the end of the 18th and beginning of the 19th century, which defines the one to many relationship as one that is based on the distinction between the type as the idea of an object and its model as its application. This essential type definition means that every architectural design or urban plan is the application of a model based on an ever-counting essence called the idea or type. We have to keep in mind that this definition of the type versus the model served as the argument for Aldo Rossi's work on The Architecture of the City, which was and still is today the most dominating architectural theory governing practice for the last 50 years. 

POPULATION THINKING IN BIOLOGY 
Ernst Mayr introduced the idea of population thinking in biology based on Darwin's On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. When he writes that, 'The assumptions of population thinking are diametrically opposed to those of the typologist, 14 and continues that, 'For the typologist, the type (eidos) is real and the variation an illusion, while for the populationist the type (average) is an abstraction and only the variation is real," he basically replaces one species concept by another. A species concept is used to designate a class or a family of similar objects or things to which a name has been given. Most of the time, the term species is applied to describe living organisms since it originated in biology, but it has also been used to describe inanimate objects as in the world of physics, and even to describe human artefacts.16 

Population thinking became the only accepted species concept within biology. Two aspects are tremendously relevant for the argument presented here. One is the difference in defining a group of objects or individuals as a species by the typologist or the populationist; the second, perhaps more important, aspect is how the differences between species are understood in terms of their morphogenesis. 

Before Mayr's groundbreaking concept was introduced, two kinds of typological thoughts had emerged in biology, which described the genesis of individuals. One was called transmutationism, the other transformationism." Both of these concepts were based on a typological approach, but had slightly different explanations on how types change throughout history. The transmutationist believed, as Mayr tells us, that change could only occur by the origin of new types. 'Since a type (essence) cannot evolve gradually (types are considered to be constant), a new type can originate only through an instantaneous "mutation" or saltation of an existing type, which thereby gives rise to a new class or type. While the transmutationist rejected any forms of gradualism or better gradual transformation, the transformationist believed in the idea of evolution and formed the concept of gradual evolution. 

18 

The discipline of biology distinguishes between two forms of transformationism. Each of them had a different understanding of what influences changes within a species. One movement of transformationism'postulated that types (essences) were steadily improved by an intrinsic drive, and that evolution was believed to take place not by the origin of new types, but by the transformation of existing types".19 It became known in biology as finalism. This kind of transformation due to a strive for perfection sounds very familiar to us as architects, reminding us of Quatremère de Quincy, who understood the evolution of primitive huts advancing into complicated construction in the same way as primitive society advanced to high forms of civilisation. 

The second group of transformationists are known as followers of the Lamarckian theory. Evolution is understood as the gradual change of organisms, in which the change is due to the direct influence of the environment on the genetic material. We all know the example of giraffes reaching to the highest accessible tree branches for food, and giving this information to lengthen their neck further to be inherited by the next generation. 

Typological thinking therefore defines its genesis by transcended essences that classify a species by its morphological characteristics. In contrast, the populationist defines the identity of a species through its morphogenetic process. That's why it is radically different to those views based on typologies. Population thinking not only replaced typological thinking, it basically erased its roots. 

To understand what population thinking is or how it distinguishes itself from other species concepts, we can look at the way it defines the relationship between the individual and its related species. It does it by two forms of differences. One states that each individual has to be different from the other, but at the same time these differences among individuals sustain its difference as a species. The diversity of the individuals is critical to the entire species. And this is the reason why it is called population thinking. It needs a critical mass of different individuals in order to diversify its gene pool. 

In that sense, a population refers to a multiplicity as defined by Deleuze. It does not define itself as the relationship between the one and the many, it rather defines the organisation of the many itself. 

To introduce population thinking into architecture, we have to understand how biology explains the morphogenetic process of this species concept, particularly in regards to the difference between genotype and phenotype. The terminological distinction between the phenotype and the genotype is that the: 

genetic material itself is the genome (haploid) or genotype (diploid), which controls the production of the body of an organism and all of its attributes, the phenotype. This phenotype is the result of the interaction of the genotype with the environment during development. The amplitude of variation of the phenotype produced by a given genotype under different environmental conditions is called its norms of reaction. (Ernst Mayr)20 
Monroe W Strickberger, Skeletal Structures of Forelimbs, 2000. Skeletal structures of the forelimbs in various representative tetrapods (literally animals with four pods or feet) to show the homology among bones at different proximodistal levels in the limbs (where proximal is near the body and distal furthest from the body) - homology that is preserved despite changes in size, shape and function of the forelimbs during the course of evolution. Compare, for example, the single upper arm bone (the humerus in white) in each limb, or the distal digits in green. Even when digit number varies, homology of the digits remains (Hall 2008). As Darwin noted, 'What can be more curious than that the hand of man formed for grasping, that of a mole for digging, the leg of a horse, the paddle of a porpoise and the wing of a bat, should all be constructed on the same pattern and should include similar bones and in the same relative positions?' The image and its subtitle is taken from BK Hall, B Hallgrimssom, Strickberger's Evolution, fourth edition, 2008, Jones & Bartlett Learning (Sudbury, MA), www.jblearning.com. Reprinted with permission. © Jones & Bartlett Learning. 
Ernst Haeckel, Embryos, 1870. Haeckel's classic illustration of different vertebrate embryos at comparable stages of development. Although Haeckel took some liberties in drawing these figures, the earlier stages are more similar to one another than later stages are to one another. Each embryo begins with a similar number of pharyngeal arches, paired eyes and internal structures such as a single midline heart (not shown). Some biologists call this a phylotypic stage representing the distinctive developmental substructure of the body plan shared by individuals in a particular phylum. In later stages of development, these and other structures are modified to yield the characteristic form of each subgroup. The embryos in the different groups have been scaled to the same approximate size so that comparisons can be made among them. The image and its subtitle is taken from BK Hall, B Hallgrimssom, Strickberger's Evolution, fourth edition, 2008, Jones & Bartlett Learning (Sudbury, MA), www.jbleaming.com. Reprinted with permission. © Jones & Bartlett Learning. 
It is the phenotype which is exposed to natural selection, never the genotype. This distinction between a mortal body and an immortal transmitter of hereditary instructions is what revolutionised biology.21 

This is what biologists call morphogenetic process. Within the creation of mankind and other vertebrates this is called embryology. It is not just a metaphor of fertilised eggs with all the information given for the development of an organism. It defines the genesis of its form, the process of an organism unfolding under 'differentiated tissues and organs'.22 This unfolding is called 'progressive differentiation'.23 It is this process of differentiation, which defines the physical processes of its emerging form, that allows us to think of it as a transdisciplinary concept. 

POPULATION THINKING IN ARCHITECTURE 
To understand the idea of population thinking in relation to associative design techniques, two important definitions are described above. One is that population thinking does not define the relationship between the one (type) and the many (model), but that it belongs to the organisation of the many as such. The many of an architectural object is therefore the assembly of its components or geometrical primitives. The second important statement is that a morphogenetic process is a process of differentiation. This is needed in order to understand an assembly of architectural primitives as a dynamical system. 

Various fields of knowledge are dealing with the dynamical potential of interacting systems like population thinking. From economics, sociology, physics and politics, it occurred in geography through the work of the Chicago School's Burgess, Park and McKenzie24 and their study of human ecologies in the city of Chicago. It entered urbanism by the central place theory of Walter Christaller, for whom the formation of densities of cities emerged as the accumulation of individual behaviour based on economic forces. 

All these examples have in common that, within their discipline, they have replaced the hylomorphic model as the dominating model that defined the relationship between matter and form.25 

The hylomorphic model denies the interrelation between morph (form) on one side and hyle (matter) on the other side.26 It is Gilbert Simondon, in his book L'individu et sa genèse physico-biologique, who introduces the common notion of the hylomorphic model to a wider field of disciplines when he says: 'It is not only the clay and the brick, the marble and the statue which can be thought according to the hylemorphic model, but also a great number of formal, genetic and compositional actualities, in the living world and the psychic domain. In the chapter on 'Form on Matter, Simondon criticises the hylomorphic schema as it 'sees matter as homogenous and form as fixed'. As mentioned above, the conceptualisation of homogenous matter on the one side, and fixed form on the other, defines the direction of how ideas move into form. But what actually happens, Simondon continues, is that matter and form define ‘a zone of medium and intermediary dimension, of energetic and molecular dimensions between them." This zone can be referred to as a morphogenetic space, in which both matter (hyle) and ideas (form) are in dynamic relations. Based on Simondon, Deleuze gave this dynamic condition a name: machinic phylum.28 

In his article The Machinic Phylum 29 Manuel DeLanda unfolds this philosophical concept and examines its significance in what he calls 'the emergence of novelty", where matter is seen as an active material with 'morphogenetic capabilities' to generate different structures through the constraints of its material properties. To demonstrate what he means by this, let us take an example he uses in his article. In physics, he explains, 'a population of interacting physical entities, such as molecules, can be constrained energetically to force it to display organised collective behaviour.30 The example he often used to explain this behaviour is the formation of soap bubbles. He explains that the molecules of the soap collectively minimise the tension of the surface. Each soap bubble differs according to the way the molecules organise themselves to perform the most minimal surface tension. Another example can be given by the formation of crystal. 'If you hold a crystal, made out of carbon, it could take the shape of a diamond with its beautifully regular tetrahedral form. But it could be graphite, whose hexagonal sheets sheer off as it is rubbed over paper. 31 One substance can occur in many forms. 

Both examples show that such processes of progressive differentiation embedded within a machinic phylum can be defined by its physicality: the interrelationship between its metrical and non-metrical properties. The metrical properties of the soap bubble or the crystals are given form by the non-metrical properties of the minimal surface tension in the case of the soap or the collectively minimised bonding energy to form crystals. All of which occurred as the collective behaviour of a population of molecules. 

If we relate this knowledge to our computational design techniques and the organisation of the architectural object as an assembly of its parts, we can say that the process of differentiation occurs between the constraints of the architectural primitives, and the external forces that effect the negotiation between them. What the molecules are in physical processes, the geometrical primitives are to our assembled objects. Each of their metrical properties of space such as lengths, widths, heights, surfaces and volumes called extensive properties are constrained by their materiality. But to produce a collective behaviour, they have to be confronted or tested with the non-metrical properties of our build environment. These properties are defined by degrees of temperature, pressure, tension and capacities called intensive properties.32 These non-metrical properties affect the way one assembly relates to the other. The metrical material constraint of the architectural primitive defines the degrees of freedom of the system. 

The negotiation between the non-metrical properties of architectural components and the effects originating from their assembly driven by the non-metrical properties, can be called the morphogenetic process from which a form emerges. 

Mika Watanabe and Lin Chia-Ying, Intensive Property, Berlage Institute (Peter Trummer), Rotterdam, 2008. The image shows the radiation effect of the soil condition of the desert section of Maricopa County, Phoenix. Due to the vegetation density the soil is either more or less resistant to heat-up. This technique is used to indicate heat island effects within the urban layout of Phoenix, Arizona. Associative Design Program/Berlage Institute, Rotterdam/ Peter Trummer. Berlage Institute, Rotterdam - Peter Trummer. 
Mika Watanabe and Lin Chia-Ying, Interrelationship between Extensive Properties and Intensive Properties, Berlage Institute (Peter Trummer), Rotterdam, 2008. The diagram shows the metrical effect of the intensive property of radiation. Depending on the amount of curvature of the build structure, the radiation of a building envelope can be reduced due to the metrical changes made possible by the chosen material constraints of the building structure. Associative Design Program/Berlage Institute, Rotterdam/ Peter Trummer. © Berlage Institute, Rotterdam-Peter Trummer. 
Even though it was not designed by computational techniques but definitely is based on thinking associatively, an interesting example of this is the repetitive beams in the delivery zone of Utzon's Opera House in Sydney.33 These repetitive beams first occur as strangely curved surfaces. They seem to be a merely aesthetically driven construction without any particular purpose or constraints. But upon closer examination, we see how these beams perform a kind of geometrical transformation through a metrical system: the curvature of the beam is defined by the position of a point moving along a diagonal line. In one region, the section of the beam takes the form of a square, while in another it becomes a T-shape. The diversity of these sectional shapes of the beam follows the distribution of structural forces, or, in other words, the intensive property of space. In this way, the form of the beam is differentiated by the metrical property of the constraints of its plywood formwork in relation to the non-metrical properties of the force distribution of the structural diagram. The beam structure behaves as a population whereby every element is different in order to generate a whole. 

CONCLUSION 
What I have tried to argue in this essay is the way population thinking can become re- originated in architecture. I have done that by demonstrating that population thinking performs as a multiplicity, which redefines the relationship between the one and many as an organisation of the many. I have further tried to outline what this could mean to our understanding of computational techniques and how the potential of associative design techniques supports the idea of thinking in morphogenetic processes rather than in types. In this instance, the morphogenetic process is driven by the constraints and influences of the metrical properties of architectural components and non-metrical properties of spaces. The aim of this article is to support a materialist viewpoint in architecture. It is a critique on the disciplinary knowledge of architecture with its bias towards typological thinking, which needs to exclude true complexity in order to work. But, as Robert Venturi in Complexity and Contradiction34 stated: 'More is not less. 

More is not a necessity; it is material. 

Jørn Utzon. The sculptural concourse beams of the Sydney Opera House viewed from below, Sydney. 1962. Max Dupain Associates, photograph by Max Dupain, Oct. 1962. Max Dupain. 
Jørn Utzon, geometry of a concourse beam, Sydney Opera House, Sydney, 1973. The image (a) shows cross-sections of various concourse beam schemes. The image (b) shows the geometrical construction of the material constraints on the plywood framework. The geometry demonstrates the interrelationship between the extensive properties of the beam defined by the constraints of the plywood and the intensive property, in this case the momentum line, of the beam. The image defines the variation of the section of the beam by moving the point g along a circle defined by point c and £. If g meets f the beam transforms into a T-shape and if point g is concurrent with points o the section turns into a U-shape. The speed by which point h moves along the line of depends on the material constraints of the bending capacity of the plywood and its interrelationship to the diversity of momentum in each point of the beam. The image (c) shows a section of the grand staircase isometric view of a concourse beam describing a prototypical transformation of the section of the beam. The beam is subdivided by plates of plywood. Redrawn by Sean Ahlquist.
NOTES 
1 Encyclopedia of Computers and Computer History, vol 1 A-L, Raúl Rojas (ed), Fitzroy Dearbor (Chicago; London), 2001, p 179. 

2 William Mitchell, 'A Grammar of Palladian Villa Plans', in The Logic of Architecture - Design, Computation, and Cognition, MIT Press (Cambridge, MA), second edition, 1990, p 152. 

3 Architecture of Geometry - Geometry of Architecture, lecture by Bernard Cache, at the Berlage Institute in Rotterdam 2003/04. 

4 Greg Lynn, Animate Form, Princeton Architectural Press (New York), 1998, p 30. 

5 'Misreading Peter Eisenman', in Peter Eisenman Aura und Exzeß: Zur Überwindung der Metaphysik der Architektur, Ullrich Schwarz (ed), Passagen Verlag (Vienna), 1995, p 109. 

6 The definition of the hylomorph is taken from 'Simondon and the Physico-Biological Genesis of the Individual', Chapter One: 'Form and Matter, Section I- Foundations of the Hylemorphic Model: Technology of the Capture of Form', by Gilbert Simondon. Original in French: L'individu et sa genèse physico-biologique, Presses Universitaires de France (Paris), 1964, pp 27-39, original translation by Taylor Adkins. 

7 Patrik Schumacher, 'Parametricism - A New Global Style for Architecture and Urban Design', Architectural Design, vol 79, no 4, July/August 2009, pp 14-23 and 'The Parametricist Epoch: Let the Style Wars Begin', The Architects' Journal, no 16, vol 231, 6 May 2010. 

8 Manuel DeLanda, Intensive Science and Virtual Philosophy, Continuum (New York), 2002, p 13. 

9 Gilles Deleuze, Difference and Repetition, Athlone Press (London), 1997, p 182. 

10 Ibid, p 182. 

11 DeLanda, Intensive Science and Virtual Philosophy, p 9. 

12 Deleuze, Difference and Repetition, p 182. 

13 See Sylvia Lavin, Quatremère de Quincy and the Invention of a Modern Language of Architecture, MIT Press (Cambridge, MA; London), 1992. 

14 Ernst Mayr, Populations, Species, and Evolution, Belknap Press of Harvard University Press (Cambridge, MA; London), 1963, p 4. 

15 Ibid, p 4. 

16 Ibid, 'Species Concepts and Their Application', p 10. 

17 Emst Mayr, What Evolution Is, Phoenix (London), 2002, p 83. 

18 Ibid, p 85. 

19 Ibid, p 89. 

20 Mayr, What Evolution Is, p 98. 

21 See Brian Goodwin, How the Leopard Changed Its Spots, Phoenix (London), 1995, p 27. 

22 DeLanda, Intensive Science and Virtual Philosophy, p 16. 

23 Ibid, p 17. 

24 Robert Park, Ernest W Burgess and Roderick D McKenzie, The City, University of Chicago Press (Chicago), 1925. 

25 See Gilles Deleuze and Felix Guattari, A Thousand Plateaus - Capitalism and Schizophrenia, Continuum (New York) and Athlone Press (London), 1987, pp 408-9. 

26 'Simondon and the Physico-Biological Genesis of the Individual', pp 27-39. 

27 Deleuze and Guattari, A Thousand Plateaus, p 409. 

28 Ibid. 

29 Manuel DeLanda, "The Machinic Phylum', in TechnoMorphica, V2_Publisher (Amsterdam), 1997. 30 Ibid. 

31 Goodwin, How the Leopard Changed its Spots, p 9. 

32. The definition of extensive and intensive properties of space is taken from Deleuze, Difference and Repetition, p 223. 

33 See Françoise Fromonot, Christopher Thompson, Jørn Utzon - The Sydney Opera House, Electra/ Gingko Press (Corte Madera, CA), 1998. 

34 Robert Venturi, Complexity and Contradiction in Architecture, Museum of Modern Art (New York), 1966. 

©2011 John Wiley & Sons Ltd. 
INTEGRAL FORMATION AND MATERIALISATION COMPUTATIONAL FORM AND MATERIAL GESTALT 
Achim Menges 

In this excerpt from the book Manufacturing Material Effects: Rethinking Design and Making in Architecture (2008), Achim Menges clearly establishes a system-centric approach to both design and computation - a thread which binds both the mechanisms of a virtual process with the behaviours of physical material processes. In doing so, he pinpoints both an opportunity and a necessity to capture materialisation within the realm of computation. Computation is a system of functions which operate through interaction and variation of information. Natural systems, as Menges describes, operate in a similar fashion reconciling in material structures that result from both internal physical constraints and external influences and forces. Performative morphologies, in computational design, can be derived similarly. As Menges describes in relation to representational techniques, form of a static nature emerges from processes concerned primarily with shape. Form as a performative, dynamic system emerges from principles based on the behaviour of material, methods of manipulation and assembly, and interaction with environment. In computing material as a property in the generation of form, it becomes the critical nexus at which performance in spatiality, environmental mediation and structural capacity become issues of novel design, well beyond the mere generation of shape. 

- 

- 

Architecture, as a material practice, attains social, cultural and ecological relevance through the articulation of material arrangements and structures. Thus, the way we conceptualise these material interventions and particularly the technology that enables their construction presents a fundamental aspect in how we (re)think architecture. 

In many ways, the progress over decades of computer-aided design and manufacturing (CAD/CAM), or rather the greater availability and affordability of these technologies, can be seen in the lineage of other technical advancements. In the history of architecture and construction, groundbreaking technologies have often been employed initially to facilitate projects that were conceived - and indeed embraced through well-established design concepts and construction logics. There is ample evidence of this inertia in design thinking in the context of technological progress. For example, the design of the structure and connection details of the first cast-iron bridges of late 18th-century England were modelled on timber constructions. Similarly, the early reinforced-concrete structures of the late 19th century mimicked previous iron and steel frame buildings. In fact, almost half a century had to pass between the first patent for reinforced concrete and its significant influence on design through the conceptualisation of its innate material capacities as manifested in Robert Maillart's bridges and the shell structures of various 20th-century pioneers such as Franz Dischinger. 

While these examples of deferred impact refer mainly to advances in material technology, one can still trace an interesting parallel with the current employment of computer-aided design and manufacturing technologies. The by-now-ubiquitous use of CAD/CAM technologies in architecture serves, more often than not, as the facilitative and affordable means to indulge in so-called free-form architecture as conceived at the end of the last century. Although this may lead occasionally to innovative structures and spatial qualities, it is important to recognise that the technology used in this way provides a mere extension of well-rehearsed and established design processes. 

Particularly emblematic is the one-dimensional reference to the notion of digital morphogenesis. By now almost a cliché, this term refers to various processes of form generation resulting in shapes that remain elusive to material and construction logics. In foregrounding the geometry of the eventual outcome as the key feature, these techniques are quintessentially not dissimilar to more conventional and long-established representational techniques for explicit, scalar geometric descriptions. As these notational systems cannot integrate means of materialisation, production and construction, these crucial aspects need to be subsequently pursued as top-down engineered material solutions. Being essentially about appearance, digital morphogenesis dismisses both the capacity of computational morphogenesis to encode logic, Structure and behaviour, as well as the underlying principles of natural morphogenesis, 

INTEGRATING FORMATION AND MATERIALISATION 
Natural morphogenesis, the process of growth and evolutionary development, generates systems that derive complex articulation, specific gestalt and performative capacity through the interaction of system-intrinsic material characteristics, as well as external stimuli of environmental forces and influences. Thus, formation and materialisation are always inherently and inseparably related in natural morphogenesis. Such integral processes of unfolding material gestalt are particularly striking when one considers architecture which, as a material practice, is (by contrast) still mainly based on design approaches characterised by a hierarchical relationship that prioritises the definition and generation of form over its subsequent materialisation. This suggests the latent potential of the technology at stake may unfold from an alternative approach to design, one that derives morphological complexity and performative capacity without differentiating between form generation and materialisation processes. 

The underlying logic of computation strongly suggests such an alternative, in which the geometric rigour and simulation capability of computational modelling can be deployed to integrate manufacturing constraints, assembly logics and material characteristics in the definition of material and construction systems. Furthermore, the development of versatile analysis tools for structure, thermodynamics, light and acoustics provides for integrating feedback loops of evaluating the system's behaviour in interaction with a simulated environment as generative drivers in the design process. Far beyond the aptitude of representational digital models, which mainly focus on geometry, such computational models describe behaviour rather than shape. This enables the designer to conceive of material and construction systems as the synergetic result of computationally mediating and instrumentalising the system's intrinsic logics and constraints of making, the system's behaviour and interaction with external forces and environmental influences, as well as the performative effects resulting from these interactions. Thus, the understanding of material effects [...] extends far beyond the visible effect towards the thermodynamic, acoustic and luminous modulation of the (built) environment. As these modulations, in relation to the material interventions and their construction process, can now be anticipated as actual behaviour rather than textbook principles, the design of space, structure and climate becomes inseparable. 
Nico Reinhardt, 3D Gewirke-Verbund, University of Art and Design Offenbach (Achim Menges), Offenbach, Germany, 2006-7. The '3D Gewirke-Verbund' project investigated ways of utilising local form-finding processes to differentiate a larger, continuous material system. The specific manipulation-component (Figure 1) defines the vectors and distances of gathering particular points on a three-dimensional spacer textile. Numerous computational and physical experiments were conducted exploring the behaviour of local manipulation areas, interdependent manipulation arrays and the resulting overall morphology (Figure 2. Figure 3). Based on the resultant computationally derived protocols, a number of full-scale prototypes were constructed (Figure 4). © Achim Menges. 
Crossing a number of disciplinary boundaries, the design approach presented here demands that structural and environmental engineering, which has tended to be a question of post-design optimisation, becomes an essential factor in the setup of the design process itself. Therefore realising the potential of computational design and computer controlled fabrication is twofold: first, it enables (re)establishing a far more immediate relation to the processes of making and constructing by unfolding innate material capacity and behaviour, and, second, understanding this behaviour as a means of creating not only space and structure but also micro-climatic conditions. While the latter may have a profound impact on our conception of spatial organisation, which can now be thought of as differentiated macro- and micro-climatic conditions, providing a heterogeneous habitat for human activities, the former will be the main focus, especially as the research on integral processes of computational morphogenesis and performance evaluation is a substantial field of its own. This basic research entails developing and exploring new modes of integrating design techniques, production technologies and system performance. These modes are by no means similar when developed for different systems, but rather differentiate into a wide range of possible material articulations and computational methods. So, while sharing a common objective, a rich palette of different approaches has been explored over the past five years through various research projects. What follows is a cross-section of these approaches. 

DIFFERENTIATED MATERIAL SYSTEMS 
The research projects presented here seek to develop and deploy computational techniques and digital fabrication technologies to unfold innate material capacity and specific latent gestalt. They commence from extensive investigations and tests of what we define as material systems. Material systems are considered, not so much as derivatives of standardised building systems and elements, but rather as generative drivers in the design process. 

Extending the concept of material systems by embedding their material characteristics, geometric behaviour, manufacturing constraints and assembly logics within an integral computational model promotes an understanding of form, material, structure and behaviour not as separate elements, but rather as complex interrelations. This initially requires disentangling a number of aspects that later form part of the integral computational setup in which the system evolves. 

First of all, the geometric description of material systems, or rather the notation of particular features of the system's morphology, needs to be established. The geometric definition of the system has to overcome the primacy of shape and related metric, descriptive characteristics. Therefore, the designer has to facilitate the setup of a computational model, not as a particular gestalt specified through a number of coordinates and dimensions, but as a framework for possible formations affording further differentiation that remains coherent with the behaviour observed and extracted from physical experiments and explorations of the relevant system. This computational framework, which essentially constitutes an open model (referred to as 'framework' here due to the ambiguous meaning of 'model' in a design context), is then step-by-step informed by a series of additional parameters, restrictions and characteristics inferred from material, fabrication, and assembly logics and constraints. Principally, this includes the specific material and geometric behaviour in formative processes, the size and shape constraints of involved machinery, the procedural logistics of assembly, and the sequences of construction. As these aspects vary greatly depending on the setup and construction of the material system, more detailed explanations follow describing specific projects. However, it is interesting to note the significant shift in the way computer-aided manufacturing (CAM) processes are employed in this context. 

Whereas the nature of CAM enables difference to be achieved, it is currently used mainly as a means of increasing speed and precision in the production of variation. Symptomatic for preserving the facilitative character of manufacturing and its related protocols is the term 'mass customisation'. Flourishing due to the reintroduction of affordable variation, 'mass customisation' nevertheless remains an extension of well- known and long-established design processes embracing the still dominant hierarchy of prioritised shape-definition and subsequent, purely facilitative manufacturing. One needs to be aware that the accomplishment of economically feasible variation through computer-controlled production and fabrication, by manufacturers and designers alike, does not by itself lead to strategies of instrumentalising the versatility of differentiated material systems. Nonetheless, the far-reaching potential of CAM technologies is evident once they turn into one of the defining factors of a design approach seeking the synthesis of form-generation and materialisation processes. At this point, the highly specific restrictions and possibilities of manufacturing hardware and controlling software can become generative drivers embedded in the setup and development of the computational framework. 

COMPUTATIONAL MORPHOGENESIS 
Generally, it can be said that the inclusion of what may be referred to as system-intrinsic characteristics and constraints comprises the first crucial constituent of the computational setup defined through a series of parameters. The definition of the range in which these parameters can be operated upon, and yet remain coherent with the material, fabrication and construction constraints, is the critical task for the designer at this stage. 

Steffen Reichert, Responsive Surface Structure, University of Art and Design Offenbach (Achim Menges), Offenbach, Germany, 2006-7. The 'Responsive Surface Structure' project utilises wood's inherent hygroscopic properties to develop a skin structure capable of adapting its porosity in response to changes in ambient humidity. The component consists of a moisture responsive veneer composite element (Figure 1). Once exposed to a higher level of humidity, the veneer composite components autonomously open a gap between the substructure and the veneer scales resulting in different degrees of porosity (Figure 2). The local component shape and orientation, as well as the mathematically defined surface undulation (Figure 3), evolve in continuous feedback with structural evaluation and thermodynamic analysis. As the logics of fabrication and assembly had also been encoded in the initial computational setup, the evolved morphology could be directly constructed as a fully functional prototype (Figure 4). © Achim Menges. 
The second crucial constituents of the generative computational framework are recurring evaluation cycles that expose the system to embedded analysis tools. Analysis plays a critical role during the entire morphogenetic process, not only in establishing and assessing fitness criteria related to structural and environmental capacity, but also in revealing the system's material and geometric behavioural tendencies. The conditioning relation between constraint and capacity, in concert with the feedback between stimuli and response, is consequently an operative element within the computational framework. In this way, evaluation protocols serve to track both the coherency of the generative process with the aforementioned system-intrinsic constraints, as well as the system's interaction with a simulated environment. Depending on the system's intended environmental modulation capacity, the morphogenetic development process needs to recurrently interface with appropriate analysis applications, such as multiphysics computational fluid dynamics (CFD) for the investigation of thermodynamic relations, or light and acoustic analyses. It seems important to mention that a CFD analysis provides only a partial insight, as the thermodynamic complexity of the actual environment is far greater than any computational model can handle at this moment in time. Nonetheless, as the main objective here lies not solely in the prediction of precise data, but mainly in the recognition of behavioural tendencies and patterns, the instrumental contributions of such tools are significant. 

In parallel with the environmental factors, continual structural evaluation informs the development process, or even directly interacts with the generation of the system's morphology through processes of evolutionary structural optimisation. Yet, in general, the notion of single-criteria optimisation is opposed to the underlying principles of morphogenesis. It is imperative to recognise that computational morphogenesis does not at all reproduce a technocratic attitude towards an understanding of efficiency based on a minimal material weight to structural capacity ratio. Nor does it embrace the rationale of what 20th-century engineers called 'building correctly'. Structural behaviour in this sense becomes one agent within the multifaceted integration process. Overall, this necessitates a shift in conceptualising multicriteria evaluation rather than an efficiency model. Biologists, for example, refer to effectiveness as the result of a developmental process comprising of a wide range of criteria. Accordingly, the robustness of the resulting systems is as much due to the persistent negotiation of divergent and conflicting requirements as their consequential redundancies. 

As of yet, two essential elements of a computational framework for morphogenetic processes have been introduced: the parametric setup based on the material system's intrinsic constraints, and the evaluation cycles through which the interaction of individual system instances with external influences and forces are frequently analysed. In other words, the possibility of manipulating the system's articulation in direct relation to understanding the consequential modulation of structural or environmental effects has been established. Therefore, the processes that trigger and drive the advancing development of the system are the third critical constituents of the computational framework. The framework through these processes is able to operate, as they provide the variable input to the defining parameters. This input generates a specific output - one individual instance of the system-leading to the registration and analysis of instance specific structural and environmental effects. Through these effects - basically the way the system modulates the environment - the system's performative capacity unfolds from feedback cycles of manipulation and evaluation. 

These processes of driving the development of the system through continual differentiation of its instances can be envisioned in different ways. The most immediate possibility is the direct, top-down intervention of the designer in the parametric manipulation and related assessment cycle. More coherent with the overall concept though are processes based on similar principles as natural morphogenesis. In this respect, two kinds of development processes are of interest here: the growth of the individual instance and the evolution of the system across generations of populations of individual instances. In order to facilitate the former, there are different computational growth models that can be implemented, which are all based on two critical factors: on the one hand, the internal dataset or growth rules - the genotype - and on the other, the variable gestalt that results from the interaction of the genotype with the environment - the phenotype. The critical task for the designer is defining the genotype through the aforementioned system-intrinsic,constraints. The generation of phenotypic system instances, enabled through seed configurations and repeatedly applied genotypic rewriting rules, happens in direct interaction with the environment. One critical aspect to be considered here, and captured in the computational process, is the profound influence of goal-oriented physiological regulation mechanisms, such as, for example, homeostasis on the growth process. 

Each derived instance then forms part of a population and is evaluated with the aforementioned analytical tools. Driven by fitness criteria, evolutionary computation (through the implementation of genetic algorithms, for example) can then be employed to evolve various generations based on the confluent dynamics of mutation, variation, selection and inheritance. 

A continuous mediation of the stochastic evolutionary processes and goal-oriented physiological developments at play, or more generally the skilful negotiation between bottom-up and top-down processes, is a central task for the designer. Furthermore, in order to enable genuine morphological differentiation (that is, changes in kind and not just in degree), it is of critical importance that the initially established fitness criteria, as well as the defining parameter ranges - in fact, the entire computational framework – is capable of evolving alongside the system's development. 

[...] Two trite, yet common misconceptions may need to be addressed. First, employing such a computational framework challenges the nature of currently established design approaches, yet it does not invoke the retirement of the architect in favour of computation. On the contrary, it highlights the importance of the designer in an alternative role, one that is central to enabling, moderating and influencing truly integral design processes and requires novel abilities and sensitivities. Second, despite the fact that the presented design approach requires a serious engagement with technology, as may have become clear from the above description of the involved computational framework, its use is not limited to exotic materials and manufacturing processes. [...] In effect, as the main expenditure consists of the intellectual investment in an alternative conceptualisation of material systems and related computational processes, this design approach flourishes in contexts of limited resources. Here, complexity and related performative capacity unfolds from the continuous evolution and differentiation of initially simple material elements and fabrication procedures. 

EmTech, Component Membrane, AA 
Emergent Technologies and Design Master Programme (Michael Hensel, Achim Menges, Michael Weinstock), Architectural Association, London, UK, 2007. The 'AA Component Membrane' project is based on the definition of a component that deploys a hyperparabolic membrane as a load-bearing tensile element within a framework of steel members. The proliferation of the component was evolved in feedback with structural evaluation, as well as environmental analysis of precipitation (Figure 1), wind and sunlight (Figure 2). The resulting overlapping membrane articulation protects from rain, while at the same time remaining porous enough to avoid excessive wind pressure or blocking views across London's roofscape (Figure 3). Furthermore, the membranes contribute considerably to the stiffness of the overall structure, which acts as a cantilever resting on just three points (Figure 4). AA EmTech -Achim Menges. 
From Achim Menges, 2008, 'Integral Formation and Materialisation: Computational Form and Material Gestalt', in B Kolarevic and K Klinger (eds), Manufacturing Material Effects: Rethinking Design and Making in Architecture, Routledge, Taylor & Francis Books (New York), pp 195-210. Reproduced by permission of Achim Menges. Achim Menges. 
THE COMPUTATIONAL FALLACY 
Sanford Kwinter 

Considering the increasing pervasiveness of computation in architecture, Sanford Kwinter, one of today's leading architectural thinkers and theoreticians, questions in this article, first published in the architectural journal Thresholds (2003), the prevailing use of computerised methods in design, as they are characterised by processes that are reductive in nature. Computing, at its most basic, is the execution of a series of simplified linear, binary operations. As a mechanistic process, Kwinter explains, the numerical processes become over-abstractions of physical processes, be it social, material or cultural. The consequence is an imposed order whose reality can be realised only through means of translating the representational to the physical. Computation does not, itself, operate in the same manner as natural material systems. Models for computation emerge when material (matter) intelligence is equated to computer (matter) intelligence. Kwinter establishes a crucial distinction between computation as a wilful engine for calculation, and the results that it can derive in unfolding the logic and elegance of self- organising, emergent systems and developmental patterns at extraordinary and unprecedented depth. This opportunity lies with the ability to allow computation to investigate the elemental conditions of material intelligence, searching for intricate nonlinear order and complex material formation. 

The 'mechanical' and the 'electronic' are by themselves not paradigms and do not represent distinct, successive, agonistic 'ages' or irreducible worlds in collision. To continue to think of these in such worn and sterile ways can have no other effect than to hide from ourselves their political dimensions. The mechanical and the electronic (and most of what is denoted by these terms in present usage) are in fact expressions of two continuous, interdependent historical-ontological modalities: those of Matter (substance) and Intelligence (order, shape). 

Every unit of intelligible matter in our technical or cultural world, regardless how simple, is refined or organised to a degree sufficient at least to distinguish it from the random and disordered background flux or noise of the natural world. (Of course, natural objects may possess this same property of refinement in proportion to how closely they are formed and organised by the processes of life, processes now commonly understood to extend beyond the merely organic.) In this sense such matter may be said to possess a greater or lesser amount of 'embedded intelligence'. One can understand by this a set of instructions accumulated over the ages (through the application of tools and controlled processes) and incorporated into this matter as a kind of permanent and continually reactivated 'memory' (either through shape, rhythm, or disposition as in a tool, or through purity or precise proportion as in the relationships of metals in an alloy and the properties derived therefrom). 

All matter, even totally disorganised matter, possesses some degree of active intelligence (what Diderot called 'sensitivity'), and the refinement of matter is always the refinement of the intelligence embedded within it. When different types of matter, or different orientations of intelligence in matter, are brought together in a proper or sympathetic rhythm or proportion, an entirely new level of intelligence is created. As the complexity of added and engaged elements increases, one approaches, then arrives at, the 'mechanical'. 

Now the mechanical is thought to be primitive in perhaps two senses. First, its relationships of intelligence are based on rudimentary, visible associations of isolated elements that interact at a very reduced level of information exchange. What this means is that in mechanical devices, most qualitative information tends to be reduced or eliminated in favour of a very controlled, exclusive extraction of quantitative flows (rates of movement, measures, etc). Most elements in a mechanical complex have a single function and a single set of relationships, so that most of the embedded material intelligence is suppressed in favour of a single quality or dimension of expression. (It is here that the commonly confused terms 'mechanical' and 'mechanistic' find a justifiable convergence. The first term means having to do with machines, the second means deterministic and reductionistic.) 

Second, the mechanical is said to be primitive in relation to electronic processes, because the latter appear to manifest the same magical qualities of material intelligence found in fundamental, free and unprocessed matter, a set of qualities that can be summed up in a single word, self-control. 'Control' here means simply the sustained application of intelligent - or organising force over time. While the mechanical complex seems over- designed and therefore limited to very rigid and predetermined pathways of development (ie, no development at all), archaic and electronic matter-complexes are thought to be able to move and evolve in coherent yet unpredetermined ways. Their manifest intelligence is both multispectral and free-form or "complex". 

The movement of all (advanced) technological societies has been one from archaic matter intelligence (empirical, qualitative, multispectral) to mechanical matter intelligence (numerical, dissociated), but only incompletely and each in its own way. 

In the West, mechanical matter intelligence took on an almost religious status (as electronics is certainly achieving today) to the point of annihilating archaic matter intelligence from public and social memory. Now the way in which a society organises its systems of intuition - its science, its philosophy and its technics is in every manner a political one. The real and possible arrangements of intelligence and matter in nature are one thing (and likely unlimited), yet how we represent these possibilities to ourselves is another. To speak of a mechanical paradigm of material qualities and perceptible functions and to oppose this to an electronic one of immaterial processes and pure intelligence is absurd and dangerous. Absurd, because despite what cyberspace glad-handers may think, there can clearly be no shape or order (Intelligence) without matter, even if this matter is comprised of nothing more than pure photons (cinema, retinal laser projection) or molecular acoustic resonance (music), etc, and dangerous, because such clichés do little more than render us stupid and docile in the face of disfiguring yet well-camouflaged social and historical processes. 

What is at stake today has nothing to do with the eclipse of a material or mechanical world by an increasingly electronic one, but rather the emergence of a new regime of 'subjection' that uses the undeniable allure of an archaic revival (a return to matter, complexity and free development) to facilitate a repressive reorganisation of social space as well as a mastery of the very conceptual lexicon with which this reorganisation will be thought through. More bluntly: what is taking place today under the guise of such 'rational' historical process is the systematic formation of a new subjectivity - a new type of man, to use Nietzsche's expression whose matter/intelligence variables are being re-engineered and finely calibrated to fit those of a new machinic workplace-society into which s/he is to be seamlessly integrated. 

It is therefore disconcerting to observe the direction of much discussion in the design world today around the advent of new telecommunications technologies, computerisation and software-driven milieus. These developments are either extolled as 'exciting', 'new', and 'full of new freedoms and possibilities' (by those blissfully unconcerned that much. of what is being so celebrated is but an extension of all that is oldest and most repressive in our political and corporeal history), or else are seen as posing an unavoidable or even welcome challenge to an already weakened or near-obsolete domain of cultural practice, namely the slow, grave, viscous world of matter. 

The routine disdain heaped on matter by both these points of view is in fact focused ideologically on an officially distorted notion of the mechanical - made now to mean anything that is concrete and available to intuition. Our task today I would argue, is to resist these pathways of thought, and wherever possible to expand the concept of the concrete and to extend the play of intuition into new domains. 

To do this effectively it must remain within our power (conceptual and political) to refuse the advent, not so much of the specific machines and techniques of contemporary development, but of the broader systems of rationality in which they come packaged or for which they serve as Trojan horses. 

Communications networks, computers, microprocessor control systems are socially toxic entities, I would argue, primarily when used correctly, that is, in their capacity to routinise interactions with people and processes in increasingly engineered, confined and deterministic spaces. It is our duty and mandate to refuse this new, pseudo-material space entirely, and to follow the 'minor', archaic path through the microchip; that is, to make the electronic world work for us to reimpart the rich indeterminacy and magic of matter out of the arid, cruel and numericalised world of the reductionist-mechanical and the disciplinary-electronic. 

No computer on earth can match the processing power of even the most simple natural system, be it of water molecules on a warm rock, a rudimentary enzyme system, or the movement of leaves in the wind. The most powerful and challenging use of the computer (aside from the obvious benefits of automated number crunching in purely numerical domains such as accounting) is in learning how to make a simple organisation (the computer) model what is intrinsic about a more complex, infinitely entailed organisation (the natural or real system). 

Implicit here is the idea of learning how to make matter model matter, or how to study natural or 'wild' Intelligence in a contained but active, refining domain. In this use the computer becomes metallurgical substance, it extends the exploratory evolutionary process of differentiation and refinement by inventing new levels of order and shape. The computer and its software together can form a Matter/Intelligence unit of a very primitive but useful kind. But to do this, the computer, in the triad Nature-Mind-Computer, must play only the appropriate intermediary role of interface between Nature and Mind. This would be in clear contradistinction to what is more often the case today, where computational environments provide a customary but imperceptible experiential envelope from which Nature (and all nondeterministic unfolding) is excluded and within which the activity horizon of Mind is insidiously confined. We must not believe the narcotising hype that an emerging electronic world is poised to liberate us from a mechanical one, nor even that there exists an electronosphere fundamentally discontinuous from the mechanosphere that has formed us till now. 

It is true that an important transition is taking place: mechanical relations are being dramatically transferred to new and different levels - like the little ball in a scam artist's shell game - but they are certainly not disappearing. What is more, this transition state is an unstable one, and one of the possible arms on history's bifurcation diagram (the one that does not lead smoothly to the total routinisation and economic subsumption of the human organism) leads at once to the possibility of multiple new ecologies of human existence as well as to the dark, possibly unfathomable mysteries of nature itself. 

What we need today is twofold. On the one hand, resistance we need to direct our theoretical activity away from simple-minded clichés in order to conceptualise the proper materiality of the electronic with its brutal effect on both human energy and the physical environment; and on the other hand, productive affirmation to actively press computation toward its deep rootedness in the archaic world of natural intelligence, which means at the very least to use computation just as the early moderns used the telescope and the microscope, to engage aspects of nature whose logic and pattern had previously remained ungraspable because they were lodged at too great a remove from the modalities of human sense and intuition. 

During the Renaissance, specific movements and structures of astronomical and microscopic scale were for the first time brought into the purview of human thought and perception, and in the process certain forms of historical tyranny became forever impossible. Today the computer offers the possibility of apprehending developmental patterns of extraordinary and unprecedented depth and abstraction, offering tantalising glimpses of the very free-form structure of time itself (chaos, complexity, self-organisation). 

Just as Lucretius's hydraulic hypothesis in his ancient Treatise On Nature once proposed to free humans from the capriciousness and prejudices of the gods, so this new tool among all the horrors to which it is already giving place may well bear the potential to unlock the door on the universal laws that govern the appearance and destruction of form, and in so doing to free us from the multiple tyranny of determinism and from the poverty of a linear, numerical world. Yet there should be no illusions: the possibilities for such a scenario are almost already foreclosed, and it will certainly not come to pass with anything short of a colossal, sustained and collective act of human will, It is we, the engineers of human environment and activity, who bear the burden to ensure a properly human pleading in this struggle for our fate. 

Sanford Kwinter, 'The Computational Fallacy', Thresholds – Denatured, no 26, 2003, pp 90–1. Thresholds is the biannual journal of the Department of Architecture at the Massachusetts Institute for Technology. Reproduced by permission of Sanford Kwinter and Thresholds. © Thresholds. 


SELECT BIBLIOGRAPHY 
Abel, C. 'Evolutionary Planning', Architectural Design, December issue No. 7/6, John Wiley & Sons, London, 1968. 

Allen, S. 'Terminal Velocities: The Computer in the Design Studio', Practice: Architecture Technique + Representation, Routledge, Abingdon/New York, 2009. 

Aish, R. 'From Intuition to Precision', AÄ Files 52, The Architectural Association, London, 2005, 

Alexander, C. 'Systems Generating Systems', Architectural Design, December issue No. 7/6, John Wiley & Sons, London, 1968. 

Bentley, P and D Come. Creative Evolutionary Systems, Academic Press, San Diego, CA, 2002. Bentley, P and D Come. Evolutionary Design by Computers, Morgan Kaufmann Publishers, San Francisco, 1999. 

Bertalanffy, L v. General System Theory: Foundations, Development, Applications, George Braziller, New York, 1969. 

Chu, K. 'Genetic Space: Hourglass of the Demiurge', AD Architects in Cyberspace II, Profile No. 136, John Wiley & Sons, London, 1998. 

Crutchfield, J. 'Is Anything Ever New? Considering Emergence', G Cowan, D Pines and D Melzner (eds) Integrative Themes, Addison-Wesley, Reading, 1994. 

Thompson, DW. On Growth and Form: The Complete Revised Edition, Dover Publications, New York, 1992. DeLanda, M. "The Mathematics of the Virtual', Intensive Science and Virtual Philoshopy, Continuum, New York, 2002. 

DeLanda, M. Deleuze and the Use of the Genetic Algorithm in Architecture', Neil Leach (ed), Designing for a Digital World, Wiley-Academy, Chichester, 2002. 

De Wolf T and T Holvoet. 'Emergence versus Self-organisation: Different Concepts but Promising When Combined', $ Brueckner, G Di Marzo Serugendo, A Karageorgos, R Nagpal (eds), Engineering self- organising systems: methodologies and applications, Springer, Berlin, 2005. 

Frazer, J. An Evolutionary Architecture, AA Publications, London, 1995. 

Frazer, J and M Rastogi. 'A New Canvas', AD Architects in Cyberspace II, Profile No. 136, John Wiley & Sons, London, 1998. 

Gero, J. 'Creativity, emergence and evolution in design: Concepts and framework', Knowledge-Based Systems 9(7), 1996. 

Goethe JW. trans by Bertha Mueller. 'Formation and Transformation', Goethe's Botanical Writings, OX BOW Press, Woodbridge, 1989. 

Goodwin, B. 'Developmental Complexity and Evolutionary Order', G Cowan, D Pines, D Meltzer (eds), Complexity: Metaphors, Models and Reality, Westview Press, Boulder, CO, 1994. 

Holland, J. Emergence: from Chaos to Order, Oxford University Press, Oxford, 1998. 

Howard, R. Computing in Construction: Pioneers and the Future, Butterworth-Heinemann, Oxford/ Woburn, 1998. 

Kwinter, S. 'The Computational Fallacy', Thresholds - Denatured, No. 26, MIT Department of Architecture, Cambridge, MA, 2003. 

Kwinter, S. 'Who is Afraid of Formalism', C Davidson (ed), Far From Equilibírum: Essays on Technology and Design Culture, Actar, New York, 2008. 

Loleit, S. "The Mere Digital Process of Turning Over Leaves". Zur Wort- und Begriffsgeschichte von "Digital"", j Schröter and A Böhnke (eds), Analog/Digital - Opposition oder Kontinuum?: Zur Theorie 

und Geschichte einer Unterscheidung, Transcript Verlag, Bielefeld, 2004. 

Mayr, E. "Typological versus Population Thinking', Evolution and the Diversity of Life, First Harvard University Press, 1997. 

Mayr, E. 'Variational Evolution', What Evolution Is, Basic Books, New York, 2001. 

Menges, A. 'Integral Formation and Materialization: Computational Form and Material Gestalt', B Kolarevic and K Klinger (eds), Manufacturing Material Effects: Rethinking Design and Making in Architecture, Routledge, New York, 2008. 

Marvin M. 'Why People Think Computers Can't', Technology Review, Nov/Dec, Technology Review Inc, Cambridge, MA, 1983. 

Mitchell, W. 'A New Agenda for Computer-Aided Design', M McCullough, W Mitchell and P Purcell (eds), The Electronic Design Studio: Architectural Knowledge and Media in the Computer Era, MIT Press, Cambridge, MA, 1990. 

Mitchell, W. 'Reasoning about Designs', The Logic of Architecture: Design, Computation, and Cognition, MIT Press, Cambridge, MA, 1990. 

Mitchell, W. 'The Uses of Inconsistency in Design', Y Kalay (ed), Principles of Computer-Aided Design: Evaluating and Predicting Design Performance, Wiley-Interscience, New York, 1992. 

Negroponte, N. "Towards a Humanism Through Machines', Architectural Design, September issue No. 7/6, John Wiley & Sons, London, 1969. 

Negroponte, N. Computer Aids to Design and Architecture, Mason/Charter Publishers, London, 1975. Novak, M. "Transarchitectures and Hypersurfaces: Operations of Transmodernity', AD Hypersurface Architectures, Profile No. 133, John Wiley & Sons, London, 1998. 

Pask, G. "The Architectural Relevance of Cybernetics', Architectural Design, September issue No. 7/6, John Wiley & Sons, London, 1969. 

Schröter, J. 'Analog/Digital - Opposition oder Kontinuum?', J Schröter and A Böhnke (eds), Analog/ Digital - Opposition oder Kontinuum?: Zur Theorie und Geschichte einer Unterscheidung, Transcript Verlag, Bielefeld, 2004. 

Spiller, N. 'Vacillating Objects', AD Architects in Cyberspace II, Profile No. 136, John Wiley & Sons, London, 1998. 

Steadman, P. 'Evolutionary Design by Computer', The Evolution of Designs: Biological Analogy in Architecture and the Applied Arts. Rev. ed., Routledge, London/New York, 2008. 

Terzidis, K. 'Algorithmic Form', Expressive Form: A Conceptual Approach to Computational Design, Spon Press, New York, 2003. 

Turing, A. 'Computing Machinery and Intelligence', Mind, New Series, vol. 59, no. 236, Oxford University Press, 1950. 

Weibel, P. 'Algorithmus und Kreativität, W Berka, E Brix and C Smekal (eds), Woher kommt das Neue? Kreativität in Wissenschaft und Kunst, Böhlau, 2003. 

Weinstock, M. 'Morphogenesis and the Mathematics of Emergence', M Hensel, A Menges and M Weinstock (eds), AD Emergence: Morphogenetic Design Strategies, Profile No. 169, John Wiley & Sons, London, 2004. 

Wolfram, S. 'How Do Simple Programs Behave', M Silver (ed), AD Programming Cultures: Art and Architecture in the Age of Software, Profile No. 182, John Wiley & Sons, London, 2006. 

Weiner, N. Cybernetics: or Control and Communication in the Animal and the Machine, MIT Press, Cambridge, MA, 1948. 

1

